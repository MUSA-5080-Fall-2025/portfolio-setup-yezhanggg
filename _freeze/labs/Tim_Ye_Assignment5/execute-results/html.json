{
  "hash": "23eedf9ce3e5e24a39f7ddae14b2d97f",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Space-Time Prediction of Bike Share Demand: Philadelphia Indego\"\nauthor: \"Tim Wen & Ye Zhang\"\ndate: 2025/12/1\nformat: \n  html:\n    code-fold: true\n    toc: true\n    toc-location: left\n    theme: cosmo\nexecute:\n  warning: false\n  message: false\n---\n\n## Part 1: Replicate with Quarter 4 2024\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Core tidyverse\nlibrary(tidyverse)\nlibrary(lubridate)\n\n# Spatial data\nlibrary(sf)\nlibrary(tigris)\n\n# Census data\nlibrary(tidycensus)\n\n# Weather data\nlibrary(riem)  # For Philadelphia weather from ASOS stations\n\n# Visualization\nlibrary(viridis)\nlibrary(gridExtra)\nlibrary(knitr)\nlibrary(kableExtra)\n\n# here!\nlibrary(here)\n\n#Knearest distance\nlibrary(FNN)\n# Get rid of scientific notation. We gotta look good!\noptions(scipen = 999)\nSys.setlocale(\"LC_TIME\", \"English_United States\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] \"\"\n```\n\n\n:::\n:::\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nplotTheme <- theme(\n  plot.title = element_text(size = 14, face = \"bold\"),\n  plot.subtitle = element_text(size = 10),\n  plot.caption = element_text(size = 8),\n  axis.text.x = element_text(size = 10, angle = 45, hjust = 1),\n  axis.text.y = element_text(size = 10),\n  axis.title = element_text(size = 11, face = \"bold\"),\n  panel.background = element_blank(),\n  panel.grid.major = element_line(colour = \"#D0D0D0\", size = 0.2),\n  panel.grid.minor = element_blank(),\n  axis.ticks = element_blank(),\n  legend.position = \"right\"\n)\n\nmapTheme <- theme(\n  plot.title = element_text(size = 14, face = \"bold\"),\n  plot.subtitle = element_text(size = 10),\n  plot.caption = element_text(size = 8),\n  axis.line = element_blank(),\n  axis.text = element_blank(),\n  axis.ticks = element_blank(),\n  axis.title = element_blank(),\n  panel.background = element_blank(),\n  panel.border = element_blank(),\n  panel.grid.major = element_line(colour = 'transparent'),\n  panel.grid.minor = element_blank(),\n  legend.position = \"right\",\n  plot.margin = margin(1, 1, 1, 1, 'cm'),\n  legend.key.height = unit(1, \"cm\"),\n  legend.key.width = unit(0.2, \"cm\")\n)\n\npalette5 <- c(\"#eff3ff\", \"#bdd7e7\", \"#6baed6\", \"#3182bd\", \"#08519c\")\n```\n:::\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Read Q4 2024 data\nindego <- read_csv(here(\"data/indego-trips-2024-q4.csv\"))\n\n# Quick look at the data\nglimpse(indego)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nRows: 299,121\nColumns: 15\n$ trip_id             <dbl> 1050296434, 1050293063, 1050296229, 1050276817, 10…\n$ duration            <dbl> 22, 8, 12, 1, 5, 9, 5, 6, 16, 16, 16, 39, 6, 40, 1…\n$ start_time          <chr> \"10/1/2024 0:00\", \"10/1/2024 0:06\", \"10/1/2024 0:0…\n$ end_time            <chr> \"10/1/2024 0:22\", \"10/1/2024 0:14\", \"10/1/2024 0:1…\n$ start_station       <dbl> 3322, 3166, 3007, 3166, 3075, 3166, 3030, 3010, 33…\n$ start_lat           <dbl> 39.93638, 39.97195, 39.94517, 39.97195, 39.96718, …\n$ start_lon           <dbl> -75.15526, -75.13445, -75.15993, -75.13445, -75.16…\n$ end_station         <dbl> 3375, 3017, 3244, 3166, 3102, 3008, 3203, 3163, 33…\n$ end_lat             <dbl> 39.96036, 39.98003, 39.93865, 39.97195, 39.96759, …\n$ end_lon             <dbl> -75.14020, -75.14371, -75.16674, -75.13445, -75.17…\n$ bike_id             <chr> \"03580\", \"05386\", \"18082\", \"02729\", \"18519\", \"0272…\n$ plan_duration       <dbl> 365, 365, 365, 1, 30, 1, 365, 365, 30, 30, 30, 30,…\n$ trip_route_category <chr> \"One Way\", \"One Way\", \"One Way\", \"Round Trip\", \"On…\n$ passholder_type     <chr> \"Indego365\", \"Indego365\", \"Indego365\", \"Walk-up\", …\n$ bike_type           <chr> \"standard\", \"standard\", \"electric\", \"standard\", \"e…\n```\n\n\n:::\n:::\n\n\n::: {.cell}\n\n```{.r .cell-code}\nindego <- indego %>%\n  mutate(\n    # Parse datetime\n    start_datetime = mdy_hm(start_time),\n    end_datetime = mdy_hm(end_time),\n    \n    # Create hourly bins\n    interval60 = floor_date(start_datetime, unit = \"hour\"),\n    \n    # Extract time features\n    week = week(interval60),\n    month = month(interval60, label = TRUE),\n    dotw = wday(interval60, label = TRUE),\n    hour = hour(interval60),\n    date = as.Date(interval60),\n    \n    # Create useful indicators\n    weekend = ifelse(dotw %in% c(\"Sat\", \"Sun\"), 1, 0),\n    rush_hour = ifelse(hour %in% c(7, 8, 9, 16, 17, 18), 1, 0)\n  )\n\n# Look at temporal features\nhead(indego %>% select(start_datetime, interval60, week, dotw, hour, weekend))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 6 × 6\n  start_datetime      interval60           week dotw   hour weekend\n  <dttm>              <dttm>              <dbl> <ord> <int>   <dbl>\n1 2024-10-01 00:00:00 2024-10-01 00:00:00    40 Tue       0       0\n2 2024-10-01 00:06:00 2024-10-01 00:00:00    40 Tue       0       0\n3 2024-10-01 00:06:00 2024-10-01 00:00:00    40 Tue       0       0\n4 2024-10-01 00:06:00 2024-10-01 00:00:00    40 Tue       0       0\n5 2024-10-01 00:07:00 2024-10-01 00:00:00    40 Tue       0       0\n6 2024-10-01 00:08:00 2024-10-01 00:00:00    40 Tue       0       0\n```\n\n\n:::\n:::\n\n\nGet Weather Data\n\n::: {.cell}\n\n```{.r .cell-code}\n# Get weather from Philadelphia International Airport (KPHL)\n# This covers Q4 2024: October 1 - December 31\nweather_data <- riem_measures(\n  station = \"PHL\",  # Philadelphia International Airport\n  date_start = \"2024-10-01\",\n  date_end = \"2024-12-31\"\n)\n\n# Process weather data\nweather_processed <- weather_data %>%\n  mutate(\n    interval60 = floor_date(valid, unit = \"hour\"),\n    Temperature = tmpf,  # Temperature in Fahrenheit\n    Precipitation = ifelse(is.na(p01i), 0, p01i),  # Hourly precip in inches\n    Wind_Speed = sknt  # Wind speed in knots\n  ) %>%\n  select(interval60, Temperature, Precipitation, Wind_Speed) %>%\n  distinct()\n\n# Check for missing hours and interpolate if needed\nweather_complete <- weather_processed %>%\n  complete(interval60 = seq(min(interval60), max(interval60), by = \"hour\")) %>%\n  fill(Temperature, Precipitation, Wind_Speed, .direction = \"down\")\n\n# Look at the weather\nsummary(weather_complete %>% select(Temperature, Precipitation, Wind_Speed))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n  Temperature    Precipitation        Wind_Speed    \n Min.   :12.00   Min.   :0.000000   Min.   : 0.000  \n 1st Qu.:41.00   1st Qu.:0.000000   1st Qu.: 4.000  \n Median :51.00   Median :0.000000   Median : 7.000  \n Mean   :50.80   Mean   :0.004177   Mean   : 7.663  \n 3rd Qu.:60.95   3rd Qu.:0.000000   3rd Qu.:11.000  \n Max.   :83.00   Max.   :0.520000   Max.   :30.000  \n```\n\n\n:::\n:::\n\nCensus data download\n\n::: {.cell}\n\n```{.r .cell-code}\n# Get Philadelphia census tracts\nphilly_census <- get_acs(\n  geography = \"tract\",\n  variables = c(\n    \"B01003_001\",  # Total population\n    \"B19013_001\",  # Median household income\n    \"B08301_001\",  # Total commuters\n    \"B08301_010\",  # Commute by transit\n    \"B02001_002\",  # White alone\n    \"B25077_001\"   # Median home value\n  ),\n  state = \"PA\",\n  county = \"Philadelphia\",\n  year = 2022,\n  geometry = TRUE,\n  output = \"wide\"\n) %>%\n  rename(\n    Total_Pop = B01003_001E,\n    Med_Inc = B19013_001E,\n    Total_Commuters = B08301_001E,\n    Transit_Commuters = B08301_010E,\n    White_Pop = B02001_002E,\n    Med_Home_Value = B25077_001E\n  ) %>%\n  mutate(\n    Percent_Taking_Transit = (Transit_Commuters / Total_Commuters) * 100,\n    Percent_White = (White_Pop / Total_Pop) * 100\n  ) %>%\n  st_transform(crs = 4326)  # WGS84 for lat/lon matching\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\n  |                                                                            \n  |                                                                      |   0%\n  |                                                                            \n  |=                                                                     |   1%\n  |                                                                            \n  |=                                                                     |   2%\n  |                                                                            \n  |==                                                                    |   2%\n  |                                                                            \n  |==                                                                    |   3%\n  |                                                                            \n  |===                                                                   |   4%\n  |                                                                            \n  |===                                                                   |   5%\n  |                                                                            \n  |====                                                                  |   6%\n  |                                                                            \n  |=====                                                                 |   6%\n  |                                                                            \n  |=====                                                                 |   7%\n  |                                                                            \n  |======                                                                |   8%\n  |                                                                            \n  |======                                                                |   9%\n  |                                                                            \n  |=======                                                               |   9%\n  |                                                                            \n  |=======                                                               |  11%\n  |                                                                            \n  |========                                                              |  12%\n  |                                                                            \n  |=========                                                             |  12%\n  |                                                                            \n  |=========                                                             |  13%\n  |                                                                            \n  |==========                                                            |  14%\n  |                                                                            \n  |===========                                                           |  15%\n  |                                                                            \n  |===========                                                           |  16%\n  |                                                                            \n  |============                                                          |  17%\n  |                                                                            \n  |=============                                                         |  18%\n  |                                                                            \n  |=============                                                         |  19%\n  |                                                                            \n  |==============                                                        |  20%\n  |                                                                            \n  |===============                                                       |  21%\n  |                                                                            \n  |===============                                                       |  22%\n  |                                                                            \n  |================                                                      |  23%\n  |                                                                            \n  |=================                                                     |  24%\n  |                                                                            \n  |==================                                                    |  25%\n  |                                                                            \n  |==================                                                    |  26%\n  |                                                                            \n  |===================                                                   |  27%\n  |                                                                            \n  |====================                                                  |  28%\n  |                                                                            \n  |====================                                                  |  29%\n  |                                                                            \n  |=====================                                                 |  30%\n  |                                                                            \n  |======================                                                |  31%\n  |                                                                            \n  |======================                                                |  32%\n  |                                                                            \n  |=======================                                               |  33%\n  |                                                                            \n  |========================                                              |  34%\n  |                                                                            \n  |========================                                              |  35%\n  |                                                                            \n  |=========================                                             |  36%\n  |                                                                            \n  |==========================                                            |  37%\n  |                                                                            \n  |==========================                                            |  38%\n  |                                                                            \n  |===========================                                           |  39%\n  |                                                                            \n  |============================                                          |  40%\n  |                                                                            \n  |=============================                                         |  41%\n  |                                                                            \n  |=============================                                         |  42%\n  |                                                                            \n  |==============================                                        |  43%\n  |                                                                            \n  |===============================                                       |  44%\n  |                                                                            \n  |===============================                                       |  45%\n  |                                                                            \n  |================================                                      |  46%\n  |                                                                            \n  |=================================                                     |  47%\n  |                                                                            \n  |=================================                                     |  48%\n  |                                                                            \n  |==================================                                    |  49%\n  |                                                                            \n  |===================================                                   |  50%\n  |                                                                            \n  |===================================                                   |  51%\n  |                                                                            \n  |====================================                                  |  52%\n  |                                                                            \n  |=====================================                                 |  53%\n  |                                                                            \n  |=====================================                                 |  54%\n  |                                                                            \n  |======================================                                |  54%\n  |                                                                            \n  |=======================================                               |  55%\n  |                                                                            \n  |========================================                              |  56%\n  |                                                                            \n  |========================================                              |  57%\n  |                                                                            \n  |=========================================                             |  58%\n  |                                                                            \n  |==========================================                            |  59%\n  |                                                                            \n  |==========================================                            |  60%\n  |                                                                            \n  |===========================================                           |  61%\n  |                                                                            \n  |============================================                          |  62%\n  |                                                                            \n  |============================================                          |  63%\n  |                                                                            \n  |=============================================                         |  64%\n  |                                                                            \n  |==============================================                        |  65%\n  |                                                                            \n  |==============================================                        |  66%\n  |                                                                            \n  |===============================================                       |  67%\n  |                                                                            \n  |================================================                      |  68%\n  |                                                                            \n  |================================================                      |  69%\n  |                                                                            \n  |=================================================                     |  70%\n  |                                                                            \n  |==================================================                    |  71%\n  |                                                                            \n  |===================================================                   |  72%\n  |                                                                            \n  |===================================================                   |  73%\n  |                                                                            \n  |====================================================                  |  74%\n  |                                                                            \n  |=====================================================                 |  75%\n  |                                                                            \n  |=====================================================                 |  76%\n  |                                                                            \n  |======================================================                |  77%\n  |                                                                            \n  |=======================================================               |  78%\n  |                                                                            \n  |=======================================================               |  79%\n  |                                                                            \n  |========================================================              |  80%\n  |                                                                            \n  |=========================================================             |  81%\n  |                                                                            \n  |=========================================================             |  82%\n  |                                                                            \n  |==========================================================            |  83%\n  |                                                                            \n  |===========================================================           |  84%\n  |                                                                            \n  |===========================================================           |  85%\n  |                                                                            \n  |============================================================          |  86%\n  |                                                                            \n  |=============================================================         |  87%\n  |                                                                            \n  |=============================================================         |  88%\n  |                                                                            \n  |==============================================================        |  89%\n  |                                                                            \n  |===============================================================       |  90%\n  |                                                                            \n  |================================================================      |  91%\n  |                                                                            \n  |================================================================      |  92%\n  |                                                                            \n  |=================================================================     |  93%\n  |                                                                            \n  |==================================================================    |  94%\n  |                                                                            \n  |==================================================================    |  95%\n  |                                                                            \n  |===================================================================   |  96%\n  |                                                                            \n  |====================================================================  |  97%\n  |                                                                            \n  |====================================================================  |  98%\n  |                                                                            \n  |===================================================================== |  99%\n  |                                                                            \n  |======================================================================| 100%\n```\n\n\n:::\n\n```{.r .cell-code}\n# Check the data\nglimpse(philly_census)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nRows: 408\nColumns: 17\n$ GEOID                  <chr> \"42101001500\", \"42101001800\", \"42101002802\", \"4…\n$ NAME                   <chr> \"Census Tract 15; Philadelphia County; Pennsylv…\n$ Total_Pop              <dbl> 3251, 3300, 5720, 4029, 4415, 1815, 3374, 2729,…\n$ B01003_001M            <dbl> 677, 369, 796, 437, 853, 210, 480, 734, 763, 11…\n$ Med_Inc                <dbl> 110859, 114063, 78871, 61583, 32347, 48581, 597…\n$ B19013_001M            <dbl> 24975, 30714, 20396, 22293, 4840, 13812, 6278, …\n$ Total_Commuters        <dbl> 2073, 2255, 3032, 2326, 1980, 969, 2427, 708, 2…\n$ B08301_001M            <dbl> 387, 308, 478, 383, 456, 189, 380, 281, 456, 68…\n$ Transit_Commuters      <dbl> 429, 123, 685, 506, 534, 192, 658, 218, 438, 51…\n$ B08301_010M            <dbl> 188, 66, 219, 144, 285, 71, 278, 184, 176, 235,…\n$ White_Pop              <dbl> 2185, 2494, 3691, 3223, 182, 984, 2111, 231, 35…\n$ B02001_002M            <dbl> 268, 381, 592, 380, 88, 190, 463, 112, 238, 778…\n$ Med_Home_Value         <dbl> 568300, 605000, 350600, 296400, 76600, 289700, …\n$ B25077_001M            <dbl> 58894, 34876, 12572, 22333, 10843, 118720, 1506…\n$ geometry               <MULTIPOLYGON [°]> MULTIPOLYGON (((-75.16558 3..., MU…\n$ Percent_Taking_Transit <dbl> 20.694645, 5.454545, 22.592348, 21.754084, 26.9…\n$ Percent_White          <dbl> 67.2100892, 75.5757576, 64.5279720, 79.9950360,…\n```\n\n\n:::\n:::\n\nData Structure Check\n\n::: {.cell}\n\n```{.r .cell-code}\n# How many trips?\ncat(\"Total trips in Q4 2024:\", nrow(indego), \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nTotal trips in Q4 2024: 299121 \n```\n\n\n:::\n\n```{.r .cell-code}\n# Date range\ncat(\"Date range:\", \n    min(mdy_hm(indego$start_time)), \"to\", \n    max(mdy_hm(indego$start_time)), \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nDate range: 1727740800 to 1735689360 \n```\n\n\n:::\n\n```{.r .cell-code}\n# How many unique stations?\ncat(\"Unique start stations:\", length(unique(indego$start_station)), \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nUnique start stations: 256 \n```\n\n\n:::\n\n```{.r .cell-code}\n# Trip types\ntable(indego$trip_route_category)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\n   One Way Round Trip \n    282675      16446 \n```\n\n\n:::\n\n```{.r .cell-code}\n# Passholder types\ntable(indego$passholder_type)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\n  Day Pass   Indego30  Indego365 IndegoFlex       NULL    Walk-up \n     10711     159895     112690          1       1165      14659 \n```\n\n\n:::\n\n```{.r .cell-code}\n# Bike types\ntable(indego$bike_type)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nelectric standard \n  175503   123618 \n```\n\n\n:::\n:::\n\nCreate Space-Time Panel\n\n::: {.cell}\n\n```{.r .cell-code}\n# Create sf object for stations\nstations_sf <- indego %>%\n  distinct(start_station, start_lat, start_lon) %>%\n  filter(!is.na(start_lat), !is.na(start_lon)) %>%\n  st_as_sf(coords = c(\"start_lon\", \"start_lat\"), crs = 4326)\n\n# Spatial join to get census tract for each station\nstations_census <- st_join(stations_sf, philly_census, left = TRUE) %>%\n  st_drop_geometry()\n\n\nstations_for_map <- indego %>%\n  distinct(start_station, start_lat, start_lon) %>%\n  filter(!is.na(start_lat), !is.na(start_lon)) %>%\n  left_join(\n    stations_census %>% select(start_station, Med_Inc),\n    by = \"start_station\"\n  ) %>%\n  mutate(has_census = !is.na(Med_Inc))\n\n# Add back to trip data\nindego_census <- indego %>%\n  left_join(\n    stations_census %>% \n      select(start_station, Med_Inc, Percent_Taking_Transit, \n             Percent_White, Total_Pop),\n    by = \"start_station\"\n  )\n# Identify which stations to keep\nvalid_stations <- stations_census %>%\n  filter(!is.na(Med_Inc)) %>%\n  pull(start_station)\n\n# Filter trip data to valid stations only\nindego_census <- indego %>%\n  filter(start_station %in% valid_stations) %>%\n  left_join(\n    stations_census %>% \n      select(start_station, Med_Inc, Percent_Taking_Transit, \n             Percent_White, Total_Pop),\n    by = \"start_station\"\n  )\n```\n:::\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Count trips by station-hour\ntrips_panel <- indego_census %>%\n  group_by(interval60, start_station, start_lat, start_lon,\n           Med_Inc, Percent_Taking_Transit, Percent_White, Total_Pop) %>%\n  summarize(Trip_Count = n()) %>%\n  ungroup()\n\n# Calculate expected panel size\nn_stations <- length(unique(trips_panel$start_station))\nn_hours <- length(unique(trips_panel$interval60))\nexpected_rows <- n_stations * n_hours\n\ncat(\"Expected panel rows:\", format(expected_rows, big.mark = \",\"), \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nExpected panel rows: 523,296 \n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"Current rows:\", format(nrow(trips_panel), big.mark = \",\"), \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nCurrent rows: 150,972 \n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"Missing rows:\", format(expected_rows - nrow(trips_panel), big.mark = \",\"), \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nMissing rows: 372,324 \n```\n\n\n:::\n\n```{.r .cell-code}\n# Create complete panel\nstudy_panel <- expand.grid(\n  interval60 = unique(trips_panel$interval60),\n  start_station = unique(trips_panel$start_station)\n) %>%\n  # Join trip counts\n  left_join(trips_panel, by = c(\"interval60\", \"start_station\")) %>%\n  # Replace NA trip counts with 0\n  mutate(Trip_Count = replace_na(Trip_Count, 0))\n\n# Fill in station attributes (they're the same for all hours)\nstation_attributes <- trips_panel %>%\n  group_by(start_station) %>%\n  summarize(\n    start_lat = first(start_lat),\n    start_lon = first(start_lon),\n    Med_Inc = first(Med_Inc),\n    Percent_Taking_Transit = first(Percent_Taking_Transit),\n    Percent_White = first(Percent_White),\n    Total_Pop = first(Total_Pop)\n  )\n\nstudy_panel <- study_panel %>%\n  left_join(station_attributes, by = \"start_station\")\n```\n:::\n\n\n::: {.cell}\n\n```{.r .cell-code}\nstudy_panel <- study_panel %>%\n  mutate(\n    week = week(interval60),\n    month = month(interval60, label = TRUE),\n    dotw = wday(interval60, label = TRUE),\n    hour = hour(interval60),\n    date = as.Date(interval60),\n    weekend = ifelse(dotw %in% c(\"Sat\", \"Sun\"), 1, 0),\n    rush_hour = ifelse(hour %in% c(7, 8, 9, 16, 17, 18), 1, 0)\n  )\nstudy_panel <- study_panel %>%\n  left_join(weather_complete, by = \"interval60\")\n```\n:::\n\n\nCreate Temporal Lag\n\n::: {.cell}\n\n```{.r .cell-code}\n# Sort by station and time\nstudy_panel <- study_panel %>%\n  arrange(start_station, interval60)\n\n# Create lag variables WITHIN each station\nstudy_panel <- study_panel %>%\n  group_by(start_station) %>%\n  mutate(\n    lag1Hour = lag(Trip_Count, 1),\n    lag2Hours = lag(Trip_Count, 2),\n    lag3Hours = lag(Trip_Count, 3),\n    lag12Hours = lag(Trip_Count, 12),\n    lag1day = lag(Trip_Count, 24)\n  ) %>%\n  ungroup()\n\n# Remove rows with NA lags (first 24 hours for each station)\nstudy_panel_complete <- study_panel %>%\n  filter(!is.na(lag1day))\n```\n:::\n\n\nTemporal Train/Test Split   \n\n::: {.cell}\n\n```{.r .cell-code}\n# Split by week\n\n# Which stations have trips in BOTH early and late periods?\nearly_stations <- study_panel_complete %>%\n  filter(week < 50) %>%\n  filter(Trip_Count > 0) %>%\n  distinct(start_station) %>%\n  pull(start_station)\n\nlate_stations <- study_panel_complete %>%\n  filter(week >= 50) %>%\n  filter(Trip_Count > 0) %>%\n  distinct(start_station) %>%\n  pull(start_station)\n\n# Keep only stations that appear in BOTH periods\ncommon_stations <- intersect(early_stations, late_stations)\n\n\n# Filter panel to only common stations\nstudy_panel_complete <- study_panel_complete %>%\n  filter(start_station %in% common_stations)\n\n# NOW create train/test split\ntrain <- study_panel_complete %>%\n  filter(week < 50)\n\ntest <- study_panel_complete %>%\n  filter(week >= 50)\n\ncat(\"Training observations:\", format(nrow(train), big.mark = \",\"), \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nTraining observations: 410,628 \n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"Testing observations:\", format(nrow(test), big.mark = \",\"), \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nTesting observations: 171,684 \n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"Training date range:\", min(train$date), \"to\", max(train$date), \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nTraining date range: 19997 to 20065 \n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"Testing date range:\", min(test$date), \"to\", max(test$date), \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nTesting date range: 20066 to 20088 \n```\n\n\n:::\n:::\n\n\nHourly Patterns\n\n::: {.cell}\n\n```{.r .cell-code}\n# Average trips by hour and day type\nhourly_patterns <- indego %>%\n  group_by(hour, weekend) %>%\n  summarize(avg_trips = n() / n_distinct(date)) %>%\n  mutate(day_type = ifelse(weekend == 1, \"Weekend\", \"Weekday\"))\n\nggplot(hourly_patterns, aes(x = hour, y = avg_trips, color = day_type)) +\n  geom_line(linewidth = 1.2) +\n  scale_color_manual(values = c(\"Weekday\" = \"#08519c\", \"Weekend\" = \"#6baed6\")) +\n  labs(\n    title = \"Average Hourly Ridership Patterns - Q4 2024\",\n    subtitle = \"Clear commute patterns on weekdays\",\n    x = \"Hour of Day\",\n    y = \"Average Trips per Hour\",\n    color = \"Day Type\"\n  ) +\n  plotTheme\n```\n\n::: {.cell-output-display}\n![](Tim_Ye_Assignment5_files/figure-html/unnamed-chunk-9-1.png){width=672}\n:::\n:::\n\n\nTrips over time\n\n::: {.cell}\n\n```{.r .cell-code}\n# Daily trip counts\ndaily_trips <- indego %>%\n  group_by(date) %>%\n  summarize(trips = n())\n\nggplot(daily_trips, aes(x = date, y = trips)) +\n  geom_line(color = \"#3182bd\", linewidth = 1) +\n  geom_smooth(se = FALSE, color = \"red\", linetype = \"dashed\") +\n  labs(\n    title = \"Indego Daily Ridership - Q4 2024\",\n    subtitle = \"Winter demand patterns in Philadelphia\",\n    x = \"Date\",\n    y = \"Daily Trips\",\n    caption = \"Source: Indego bike share\"\n  ) +\n  plotTheme\n```\n\n::: {.cell-output-display}\n![](Tim_Ye_Assignment5_files/figure-html/unnamed-chunk-10-1.png){width=672}\n:::\n:::\n\n   \nModel 1   \n\n::: {.cell}\n\n```{.r .cell-code}\n# Create day of week factor with treatment (dummy) coding\n\ntrain <- train %>%\n  mutate(dotw_simple = factor(dotw, levels = c(\"Mon\", \"Tue\", \"Wed\", \"Thu\", \"Fri\", \"Sat\", \"Sun\")))\n\n# Set contrasts to treatment coding (dummy variables)\ncontrasts(train$dotw_simple) <- contr.treatment(7)\n\n# Now run the model\nmodel1 <- lm(\n  Trip_Count ~ as.factor(hour) + dotw_simple + Temperature + Precipitation,\n  data = train\n)\nsummary(model1)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nCall:\nlm(formula = Trip_Count ~ as.factor(hour) + dotw_simple + Temperature + \n    Precipitation, data = train)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-1.7089 -0.6783 -0.2159  0.1882 27.2727 \n\nCoefficients:\n                   Estimate Std. Error t value             Pr(>|t|)    \n(Intercept)       -0.489156   0.013731 -35.624 < 0.0000000000000002 ***\nas.factor(hour)1  -0.055267   0.012906  -4.282   0.0000184994686084 ***\nas.factor(hour)2  -0.078435   0.012690  -6.181   0.0000000006378151 ***\nas.factor(hour)3  -0.110234   0.012655  -8.711 < 0.0000000000000002 ***\nas.factor(hour)4  -0.074204   0.012566  -5.905   0.0000000035263476 ***\nas.factor(hour)5   0.038930   0.012623   3.084             0.002042 ** \nas.factor(hour)6   0.296063   0.012639  23.424 < 0.0000000000000002 ***\nas.factor(hour)7   0.562184   0.012853  43.738 < 0.0000000000000002 ***\nas.factor(hour)8   0.940524   0.012603  74.629 < 0.0000000000000002 ***\nas.factor(hour)9   0.681527   0.012691  53.700 < 0.0000000000000002 ***\nas.factor(hour)10  0.562441   0.012526  44.900 < 0.0000000000000002 ***\nas.factor(hour)11  0.602993   0.012538  48.094 < 0.0000000000000002 ***\nas.factor(hour)12  0.698148   0.012435  56.145 < 0.0000000000000002 ***\nas.factor(hour)13  0.667308   0.012338  54.085 < 0.0000000000000002 ***\nas.factor(hour)14  0.685610   0.012386  55.355 < 0.0000000000000002 ***\nas.factor(hour)15  0.791576   0.012780  61.940 < 0.0000000000000002 ***\nas.factor(hour)16  0.906925   0.012549  72.273 < 0.0000000000000002 ***\nas.factor(hour)17  1.110205   0.012639  87.842 < 0.0000000000000002 ***\nas.factor(hour)18  0.831297   0.012728  65.314 < 0.0000000000000002 ***\nas.factor(hour)19  0.541516   0.012768  42.412 < 0.0000000000000002 ***\nas.factor(hour)20  0.341348   0.012858  26.547 < 0.0000000000000002 ***\nas.factor(hour)21  0.233603   0.012881  18.135 < 0.0000000000000002 ***\nas.factor(hour)22  0.175194   0.012820  13.666 < 0.0000000000000002 ***\nas.factor(hour)23  0.076033   0.012906   5.891   0.0000000038337890 ***\ndotw_simple2       0.065588   0.006869   9.549 < 0.0000000000000002 ***\ndotw_simple3       0.062920   0.006857   9.177 < 0.0000000000000002 ***\ndotw_simple4      -0.050061   0.006571  -7.619   0.0000000000000257 ***\ndotw_simple5      -0.012223   0.006833  -1.789             0.073612 .  \ndotw_simple6      -0.024444   0.006806  -3.591             0.000329 ***\ndotw_simple7      -0.050983   0.006924  -7.363   0.0000000000001797 ***\nTemperature        0.012467   0.000161  77.432 < 0.0000000000000002 ***\nPrecipitation     -1.109233   0.081793 -13.562 < 0.0000000000000002 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.144 on 410596 degrees of freedom\nMultiple R-squared:  0.1141,\tAdjusted R-squared:  0.114 \nF-statistic:  1705 on 31 and 410596 DF,  p-value: < 0.00000000000000022\n```\n\n\n:::\n:::\n\n\nModel 2\n\n::: {.cell}\n\n```{.r .cell-code}\nmodel2 <- lm(\n  Trip_Count ~ as.factor(hour) + dotw_simple + Temperature + Precipitation +\n    lag1Hour + lag3Hours + lag1day,\n  data = train\n)\n\nsummary(model2)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nCall:\nlm(formula = Trip_Count ~ as.factor(hour) + dotw_simple + Temperature + \n    Precipitation + lag1Hour + lag3Hours + lag1day, data = train)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-12.8428  -0.4697  -0.1265   0.1225  25.5313 \n\nCoefficients:\n                    Estimate Std. Error t value             Pr(>|t|)    \n(Intercept)       -0.1971724  0.0120171 -16.408 < 0.0000000000000002 ***\nas.factor(hour)1  -0.0175731  0.0112639  -1.560              0.11873    \nas.factor(hour)2  -0.0093400  0.0110785  -0.843              0.39918    \nas.factor(hour)3  -0.0323163  0.0110523  -2.924              0.00346 ** \nas.factor(hour)4  -0.0064036  0.0109793  -0.583              0.55973    \nas.factor(hour)5   0.0625618  0.0110386   5.668  0.00000001449553616 ***\nas.factor(hour)6   0.2581886  0.0110639  23.336 < 0.0000000000000002 ***\nas.factor(hour)7   0.4087297  0.0112659  36.280 < 0.0000000000000002 ***\nas.factor(hour)8   0.6475387  0.0110725  58.482 < 0.0000000000000002 ***\nas.factor(hour)9   0.2687374  0.0111533  24.095 < 0.0000000000000002 ***\nas.factor(hour)10  0.2122365  0.0109778  19.333 < 0.0000000000000002 ***\nas.factor(hour)11  0.2554385  0.0109944  23.234 < 0.0000000000000002 ***\nas.factor(hour)12  0.3583675  0.0108988  32.881 < 0.0000000000000002 ***\nas.factor(hour)13  0.3239304  0.0108163  29.948 < 0.0000000000000002 ***\nas.factor(hour)14  0.3531805  0.0108536  32.540 < 0.0000000000000002 ***\nas.factor(hour)15  0.4322898  0.0112024  38.589 < 0.0000000000000002 ***\nas.factor(hour)16  0.5127136  0.0110123  46.558 < 0.0000000000000002 ***\nas.factor(hour)17  0.6404994  0.0111130  57.635 < 0.0000000000000002 ***\nas.factor(hour)18  0.3266069  0.0112005  29.160 < 0.0000000000000002 ***\nas.factor(hour)19  0.1612724  0.0111995  14.400 < 0.0000000000000002 ***\nas.factor(hour)20  0.0449273  0.0112804   3.983  0.00006812693842296 ***\nas.factor(hour)21  0.0488508  0.0112677   4.335  0.00001454699054146 ***\nas.factor(hour)22  0.0606778  0.0111954   5.420  0.00000005967221309 ***\nas.factor(hour)23  0.0183651  0.0112646   1.630              0.10303    \ndotw_simple2       0.0070021  0.0059975   1.167              0.24301    \ndotw_simple3      -0.0128484  0.0059925  -2.144              0.03203 *  \ndotw_simple4      -0.0448243  0.0057357  -7.815  0.00000000000000551 ***\ndotw_simple5      -0.0422563  0.0059684  -7.080  0.00000000000144379 ***\ndotw_simple6      -0.0367596  0.0059417  -6.187  0.00000000061504630 ***\ndotw_simple7      -0.0593011  0.0060464  -9.808 < 0.0000000000000002 ***\nTemperature        0.0040641  0.0001426  28.494 < 0.0000000000000002 ***\nPrecipitation     -0.9676346  0.0714492 -13.543 < 0.0000000000000002 ***\nlag1Hour           0.3238000  0.0014825 218.412 < 0.0000000000000002 ***\nlag3Hours          0.1180469  0.0014504  81.387 < 0.0000000000000002 ***\nlag1day            0.2027487  0.0013895 145.912 < 0.0000000000000002 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.9988 on 410593 degrees of freedom\nMultiple R-squared:  0.3252,\tAdjusted R-squared:  0.3252 \nF-statistic:  5821 on 34 and 410593 DF,  p-value: < 0.00000000000000022\n```\n\n\n:::\n:::\n\n\nModel 3\n\n::: {.cell}\n\n```{.r .cell-code}\nmodel3 <- lm(\n  Trip_Count ~ as.factor(hour) + dotw_simple + Temperature + Precipitation +\n    lag1Hour + lag3Hours + lag1day +\n    Med_Inc.x + Percent_Taking_Transit.y + Percent_White.y,\n  data = train\n)\n\nsummary(model3)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nCall:\nlm(formula = Trip_Count ~ as.factor(hour) + dotw_simple + Temperature + \n    Precipitation + lag1Hour + lag3Hours + lag1day + Med_Inc.x + \n    Percent_Taking_Transit.y + Percent_White.y, data = train)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-10.2603  -0.7198  -0.2719   0.4456  24.6278 \n\nCoefficients:\n                              Estimate    Std. Error t value\n(Intercept)               0.7009115282  0.0386608126  18.130\nas.factor(hour)1         -0.0339064631  0.0443755122  -0.764\nas.factor(hour)2         -0.0733390073  0.0472207425  -1.553\nas.factor(hour)3         -0.1736198797  0.0563179188  -3.083\nas.factor(hour)4         -0.1602016516  0.0526556216  -3.042\nas.factor(hour)5         -0.0842198175  0.0392042217  -2.148\nas.factor(hour)6          0.1812725910  0.0339389718   5.341\nas.factor(hour)7          0.3659932453  0.0327798040  11.165\nas.factor(hour)8          0.6419351722  0.0315874296  20.322\nas.factor(hour)9          0.0689540443  0.0318174207   2.167\nas.factor(hour)10         0.0455849373  0.0318601406   1.431\nas.factor(hour)11         0.0951277607  0.0317831322   2.993\nas.factor(hour)12         0.1718233329  0.0312986651   5.490\nas.factor(hour)13         0.1501299648  0.0313008986   4.796\nas.factor(hour)14         0.1546199385  0.0311814820   4.959\nas.factor(hour)15         0.2738677333  0.0314129060   8.718\nas.factor(hour)16         0.3825663042  0.0310863390  12.307\nas.factor(hour)17         0.5958195674  0.0311303049  19.140\nas.factor(hour)18         0.1871034280  0.0314626011   5.947\nas.factor(hour)19         0.0105635560  0.0320040115   0.330\nas.factor(hour)20        -0.0963042374  0.0327876103  -2.937\nas.factor(hour)21        -0.0718694380  0.0336061149  -2.139\nas.factor(hour)22        -0.0356940800  0.0343008260  -1.041\nas.factor(hour)23        -0.0793554621  0.0360569574  -2.201\ndotw_simple2              0.0015399442  0.0129720488   0.119\ndotw_simple3             -0.0058302798  0.0131356919  -0.444\ndotw_simple4             -0.0584003672  0.0128820191  -4.533\ndotw_simple5             -0.0902616975  0.0132777688  -6.798\ndotw_simple6             -0.0296928615  0.0133240066  -2.229\ndotw_simple7             -0.0404284310  0.0137730955  -2.935\nTemperature               0.0060129110  0.0003255823  18.468\nPrecipitation            -2.6993476761  0.2537854011 -10.636\nlag1Hour                  0.2432506608  0.0023823455 102.106\nlag3Hours                 0.0728128784  0.0024806873  29.352\nlag1day                   0.1565602280  0.0022935086  68.262\nMed_Inc.x                -0.0000001894  0.0000001237  -1.531\nPercent_Taking_Transit.y -0.0038935558  0.0004528281  -8.598\nPercent_White.y           0.0037864819  0.0002282348  16.590\n                                     Pr(>|t|)    \n(Intercept)              < 0.0000000000000002 ***\nas.factor(hour)1                      0.44482    \nas.factor(hour)2                      0.12040    \nas.factor(hour)3                      0.00205 ** \nas.factor(hour)4                      0.00235 ** \nas.factor(hour)5                      0.03170 *  \nas.factor(hour)6              0.0000000925173 ***\nas.factor(hour)7         < 0.0000000000000002 ***\nas.factor(hour)8         < 0.0000000000000002 ***\nas.factor(hour)9                      0.03022 *  \nas.factor(hour)10                     0.15249    \nas.factor(hour)11                     0.00276 ** \nas.factor(hour)12             0.0000000403123 ***\nas.factor(hour)13             0.0000016175893 ***\nas.factor(hour)14             0.0000007104928 ***\nas.factor(hour)15        < 0.0000000000000002 ***\nas.factor(hour)16        < 0.0000000000000002 ***\nas.factor(hour)17        < 0.0000000000000002 ***\nas.factor(hour)18             0.0000000027402 ***\nas.factor(hour)19                     0.74135    \nas.factor(hour)20                     0.00331 ** \nas.factor(hour)21                     0.03247 *  \nas.factor(hour)22                     0.29805    \nas.factor(hour)23                     0.02775 *  \ndotw_simple2                          0.90550    \ndotw_simple3                          0.65715    \ndotw_simple4                  0.0000058070122 ***\ndotw_simple5                  0.0000000000107 ***\ndotw_simple6                          0.02585 *  \ndotw_simple7                          0.00333 ** \nTemperature              < 0.0000000000000002 ***\nPrecipitation            < 0.0000000000000002 ***\nlag1Hour                 < 0.0000000000000002 ***\nlag3Hours                < 0.0000000000000002 ***\nlag1day                  < 0.0000000000000002 ***\nMed_Inc.x                             0.12579    \nPercent_Taking_Transit.y < 0.0000000000000002 ***\nPercent_White.y          < 0.0000000000000002 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.274 on 133822 degrees of freedom\n  (276768 observations deleted due to missingness)\nMultiple R-squared:  0.2192,\tAdjusted R-squared:  0.219 \nF-statistic:  1015 on 37 and 133822 DF,  p-value: < 0.00000000000000022\n```\n\n\n:::\n:::\n\n\nModel 4\n\n::: {.cell}\n\n```{.r .cell-code}\nmodel4 <- lm(\n  Trip_Count ~ as.factor(hour) + dotw_simple + Temperature + Precipitation +\n    lag1Hour + lag3Hours + lag1day +\n    Med_Inc.x + Percent_Taking_Transit.y + Percent_White.y +\n    as.factor(start_station),\n  data = train\n)\n\n# Summary too long with all station dummies, just show key metrics\ncat(\"Model 4 R-squared:\", summary(model4)$r.squared, \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nModel 4 R-squared: 0.2469249 \n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"Model 4 Adj R-squared:\", summary(model4)$adj.r.squared, \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nModel 4 Adj R-squared: 0.2454536 \n```\n\n\n:::\n:::\n\n\nModel 5\n\n::: {.cell}\n\n```{.r .cell-code}\nmodel5 <- lm(\n  Trip_Count ~ as.factor(hour) + dotw_simple + Temperature + Precipitation +\n    lag1Hour + lag3Hours + lag1day + rush_hour + as.factor(month) +\n    Med_Inc.x + Percent_Taking_Transit.y + Percent_White.y +\n    as.factor(start_station) +\n    rush_hour * weekend,  # Rush hour effects different on weekends\n  data = train\n)\n\ncat(\"Model 5 R-squared:\", summary(model5)$r.squared, \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nModel 5 R-squared: 0.2526638 \n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"Model 5 Adj R-squared:\", summary(model5)$adj.r.squared, \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nModel 5 Adj R-squared: 0.251187 \n```\n\n\n:::\n:::\n\n\nMAE Calculation\n\n::: {.cell}\n\n```{.r .cell-code}\n# Create day of week factor with treatment (dummy) coding\ntest <- test %>%\n  mutate(dotw_simple = factor(dotw, levels = c(\"Mon\", \"Tue\", \"Wed\", \"Thu\", \"Fri\", \"Sat\", \"Sun\")))\n\n# Set contrasts to treatment coding (dummy variables)\ncontrasts(test$dotw_simple) <- contr.treatment(7)\n\ntest <- test %>%\n  mutate(\n    pred1 = predict(model1, newdata = test),\n    pred2 = predict(model2, newdata = test),\n    pred3 = predict(model3, newdata = test),\n    pred4 = predict(model4, newdata = test),\n    pred5 = predict(model5, newdata = test)\n  )\n\n# Calculate MAE for each model\nmae_results <- data.frame(\n  Model = c(\n    \"1. Time + Weather\",\n    \"2. + Temporal Lags\",\n    \"3. + Demographics\",\n    \"4. + Station FE\",\n    \"5. + Rush Hour Interaction\"\n  ),\n  MAE = c(\n    mean(abs(test$Trip_Count - test$pred1), na.rm = TRUE),\n    mean(abs(test$Trip_Count - test$pred2), na.rm = TRUE),\n    mean(abs(test$Trip_Count - test$pred3), na.rm = TRUE),\n    mean(abs(test$Trip_Count - test$pred4), na.rm = TRUE),\n    mean(abs(test$Trip_Count - test$pred5), na.rm = TRUE)\n  )\n)\n\nkable(mae_results, \n      digits = 2,\n      caption = \"Mean Absolute Error by Model (Test Set)\",\n      col.names = c(\"Model\", \"MAE (trips)\")) %>%\n  kable_styling(bootstrap_options = c(\"striped\", \"hover\"))\n```\n\n::: {.cell-output-display}\n`````{=html}\n<table class=\"table table-striped table-hover\" style=\"margin-left: auto; margin-right: auto;\">\n<caption>Mean Absolute Error by Model (Test Set)</caption>\n <thead>\n  <tr>\n   <th style=\"text-align:left;\"> Model </th>\n   <th style=\"text-align:right;\"> MAE (trips) </th>\n  </tr>\n </thead>\n<tbody>\n  <tr>\n   <td style=\"text-align:left;\"> 1. Time + Weather </td>\n   <td style=\"text-align:right;\"> 0.54 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> 2. + Temporal Lags </td>\n   <td style=\"text-align:right;\"> 0.40 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> 3. + Demographics </td>\n   <td style=\"text-align:right;\"> 0.64 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> 4. + Station FE </td>\n   <td style=\"text-align:right;\"> 0.66 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> 5. + Rush Hour Interaction </td>\n   <td style=\"text-align:right;\"> 0.65 </td>\n  </tr>\n</tbody>\n</table>\n\n`````\n:::\n:::\n\n\n**Compare results** to Q1 2025:\n\nThe MAE values are lower for Q4 2024 for all of the models compared to Q1 2025, and the difference is between 0.06 and 0.10. Model 2 with Temporal Lags has the lowest MAE for both Q4 2024 and Q1 2025. Some of the reasons that could explain the Q1 2025 has a higher MAE include Q1 typically has more unpredictable ridership due to variable weather—snow days, temperature swings, and rain can make trips more unstable compared to Q4, when weather patterns tend to be more stable in the fall. There could be more snow days that heavily impact trips. Also, during the spring, there are New Year holiday travel and spring break, which make the commute pattern less regular and make the prediction harder. With fewer trips in Q1, relative prediction errors can appear larger. Q4 2024 has more than 299,121 observations, and Q1 2025 only has 201,588 observations. Both Q4 2024 and Q1 2025's model 3-5 increase MAE after adding more factors. This could suggest that demographics, station fixed effects, and rush hour improve the in-sample fit but don't generalize well to the test. \n   \nThe daily use of bike share looks almost the same between Q4 2024 and Q1 2025. Both of them have the same morning and evening rush hour peaks on weekdays and then more in the midday on the weekends. What is different is the overall seasonal trend. Q4 starts strong with 5,000 trips per day in October and steadily drops to below 1,000 in late December. Q1 is the opposite, where it starts around 1,500 trips in January and reaches 3,500 trips by the end of March, where the weather is much warmer. This matters in the prediction because Q4's model is trained on more data and in the fall days. It also helps explain why Q4 has slightly better MAE: the consistent high-ridership days in October give the lag variables a stronger, cleaner signal to learn from.\n\nBased on the MAE result, it is clear that adding temporal lags is the most important factor, as it reduced the MAE from 0.54 to 0.4, representing a 26% improvement. The baseline Time+Weather is also important because the baseline model performs the second-best among all the models. Hour-of-day and day-of-week dummies establish the fundamental rhythm of commuting. Factors like Demographics, Station FE, and Rush Hour Interaction are not particularly helpful because they all increased the MAE in our test, indicating that they overfit the training data. \n\n\n## Part 2: Error Analysis \n\n**Spatial patterns:**\n\nSince model 2 has the lowest error, we are going to analyze model 2's error\n\n::: {.cell}\n\n```{.r .cell-code}\ntest <- test %>%\n  mutate(\n    error = Trip_Count - pred2,\n    abs_error = abs(error),\n    time_of_day = case_when(\n      hour < 7 ~ \"Overnight\",\n      hour >= 7 & hour < 10 ~ \"AM Rush\",\n      hour >= 10 & hour < 15 ~ \"Mid-Day\",\n      hour >= 15 & hour <= 18 ~ \"PM Rush\",\n      hour > 18 ~ \"Evening\"\n    )\n  )\n\n# Calculate MAE by station\nstation_errors <- test %>%\n  group_by(start_station, start_lat.x, start_lon.y) %>%\n  summarize(\n    MAE = mean(abs_error, na.rm = TRUE),\n    avg_demand = mean(Trip_Count, na.rm = TRUE),\n    .groups = \"drop\"\n  ) %>%\n  filter(!is.na(start_lat.x), !is.na(start_lon.y))\n\n## Create Two Maps Side-by-Side with Proper Legends (sorry these maps are ugly)\n\n# Calculate station errors\nstation_errors <- test %>%\n  filter(!is.na(pred2)) %>%\n  group_by(start_station, start_lat.x, start_lon.y) %>%\n  summarize(\n    MAE = mean(abs(Trip_Count - pred2), na.rm = TRUE),\n    avg_demand = mean(Trip_Count, na.rm = TRUE),\n    .groups = \"drop\"\n  ) %>%\n  filter(!is.na(start_lat.x), !is.na(start_lon.y))\n\n# Map 1: Prediction Errors\np1 <- ggplot() +\n  geom_sf(data = philly_census, fill = \"grey95\", color = \"white\", size = 0.1) +\n  geom_point(\n    data = station_errors,\n    aes(x = start_lon.y, y = start_lat.x, color = MAE),\n    size = 3.5,\n    alpha = 0.7\n  ) +\n  scale_color_viridis(\n    option = \"plasma\",\n    name = \"MAE (trips)\",\n    direction = -1,\n    breaks = c(0.5, 1.0, 1.5),\n    labels = c(\"0.5\", \"1.0\", \"1.5\")\n  ) +\n  labs(title = \"Prediction Errors\") +\n  mapTheme +\n  theme(\n    legend.position = \"bottom\",\n    legend.title = element_text(size = 10, face = \"bold\"),\n    legend.text = element_text(size = 9),\n    plot.title = element_text(size = 14, face = \"bold\", hjust = 0.5)\n  ) +\n  guides(color = guide_colorbar(\n    barwidth = 12,\n    barheight = 1,\n    title.position = \"top\",\n    title.hjust = 0.5\n  ))\n\n# Map 2: Average Demand  \np2 <- ggplot() +\n  geom_sf(data = philly_census, fill = \"grey95\", color = \"white\", size = 0.1) +\n  geom_point(\n    data = station_errors,\n    aes(x = start_lon.y, y = start_lat.x, color = avg_demand),\n    size = 3.5,\n    alpha = 0.7\n  ) +\n  scale_color_viridis(\n    option = \"viridis\",\n    name = \"Avg Demand (trips/hour)\",\n    direction = -1,\n    breaks = c(0.5, 1.0, 1.5, 2.0, 2.5),\n    labels = c(\"0.5\", \"1.0\", \"1.5\", \"2.0\", \"2.5\")\n  ) +\n  labs(title = \"Average Demand\") +\n  mapTheme +\n  theme(\n    legend.position = \"bottom\",\n    legend.title = element_text(size = 10, face = \"bold\"),\n    legend.text = element_text(size = 9),\n    plot.title = element_text(size = 14, face = \"bold\", hjust = 0.5)\n  ) +\n  guides(color = guide_colorbar(\n    barwidth = 12,\n    barheight = 1,\n    title.position = \"top\",\n    title.hjust = 0.5\n  ))\n\n# Combine\ngrid.arrange(\n  p1, p2,\n  ncol = 2\n  )\n```\n\n::: {.cell-output-display}\n![](Tim_Ye_Assignment5_files/figure-html/unnamed-chunk-17-1.png){width=672}\n:::\n:::\n\n\n::: {.cell}\n\n```{.r .cell-code}\nerror_sf=st_as_sf(station_errors,coords=c(\"start_lon.y\",\"start_lat.x\"),crs=4326)\nerror_tract=st_join(error_sf,philly_census,join = st_within)\nerror_tract_out=error_tract%>%\n  st_drop_geometry()%>%\n  select(MAE,NAME)\ntop5 <- error_tract_out[order(error_tract_out$MAE, decreasing = TRUE)[1:5], ]\nprint(top5)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 5 × 2\n    MAE NAME                                                  \n  <dbl> <chr>                                                 \n1  1.42 Census Tract 125.02; Philadelphia County; Pennsylvania\n2  1.34 Census Tract 12.04; Philadelphia County; Pennsylvania \n3  1.27 Census Tract 19; Philadelphia County; Pennsylvania    \n4  1.21 Census Tract 13.02; Philadelphia County; Pennsylvania \n5  1.19 Census Tract 19; Philadelphia County; Pennsylvania    \n```\n\n\n:::\n:::\n\n\nAbove shows the top 5 census tracts with highest MAE, one of their common characteristics is they all have a distance to University of Penn. Such a distance is not either too close (students can walk to school) or too far that students could not bike at all. As a result, I think putting proximity to Upenn may better resolve such a high MAE. \n\n**Temporal patterns:**\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# MAE by time of day and day type\ntemporal_errors <- test %>%\n  group_by(time_of_day, weekend) %>%\n  summarize(\n    MAE = mean(abs_error, na.rm = TRUE),\n    .groups = \"drop\"\n  ) %>%\n  mutate(day_type = ifelse(weekend == 1, \"Weekend\", \"Weekday\"))\n\nggplot(temporal_errors, aes(x = time_of_day, y = MAE, fill = day_type)) +\n  geom_col(position = \"dodge\") +\n  scale_fill_manual(values = c(\"Weekday\" = \"#08519c\", \"Weekend\" = \"#6baed6\")) +\n  labs(\n    title = \"Prediction Errors by Time Period\",\n    subtitle = \"When is the model struggling most?\",\n    x = \"Time of Day\",\n    y = \"Mean Absolute Error (trips)\",\n    fill = \"Day Type\"\n  ) +\n  plotTheme +\n  theme(axis.text.x = element_text(angle = 45, hjust = 1))\n```\n\n::: {.cell-output-display}\n![](Tim_Ye_Assignment5_files/figure-html/unnamed-chunk-19-1.png){width=672}\n:::\n:::\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntrain <- train %>%\n  mutate(\n    pred2_train = predict(model2, newdata = train),\n    error_train = Trip_Count - pred2_train,\n    abs_error_train = abs(error_train),\n    time_of_day = case_when(\n      hour < 7 ~ \"Overnight\",\n      hour >= 7 & hour < 10 ~ \"AM Rush\",\n      hour >= 10 & hour < 15 ~ \"Mid-Day\",\n      hour >= 15 & hour <= 18 ~ \"PM Rush\",\n      hour > 18 ~ \"Evening\"\n    ),\n    season = \"Fall (Training)\"\n  )\n\n\ntest <- test %>%\n  mutate(season = \"Winter (Test)\")\n\n\ntemporal_errors_fall <- train %>%\n  group_by(time_of_day, weekend, season) %>%\n  summarize(\n    MAE = mean(abs_error_train, na.rm = TRUE),\n    .groups = \"drop\"\n  ) %>%\n  mutate(day_type = ifelse(weekend == 1, \"Weekend\", \"Weekday\"))\n\n\ntemporal_errors_winter <- test %>%\n  group_by(time_of_day, weekend, season) %>%\n  summarize(\n    MAE = mean(abs_error, na.rm = TRUE),\n    .groups = \"drop\"\n  ) %>%\n  mutate(day_type = ifelse(weekend == 1, \"Weekend\", \"Weekday\"))\n\n# Combine both seasons\ntemporal_errors_seasonal <- bind_rows(temporal_errors_fall, temporal_errors_winter)\n\n\nggplot(temporal_errors_seasonal, aes(x = time_of_day, y = MAE, fill = day_type)) +\n  geom_col(position = \"dodge\") +\n  facet_wrap(~season) +\n  scale_fill_manual(values = c(\"Weekday\" = \"#08519c\", \"Weekend\" = \"#6baed6\")) +\n  labs(\n    title = \"Prediction Errors by Time Period: Fall vs Winter\",\n    subtitle = \"Seasonal comparison of model performance\",\n    x = \"Time of Day\",\n    y = \"Mean Absolute Error (trips)\",\n    fill = \"Day Type\"\n  ) +\n  plotTheme +\n  theme(axis.text.x = element_text(angle = 45, hjust = 1))\n```\n\n::: {.cell-output-display}\n![](Tim_Ye_Assignment5_files/figure-html/unnamed-chunk-20-1.png){width=672}\n:::\n:::\n\n\nThe errors are the highest during the AM rush hour and PM rush hour on weekdays. On the weekend, the highest errors are during the AM rush, Mid-Day, and also PM rush. There is a significant decrease in the errors for weekend AM Rush, as it reflects on people's commuting pattern changes over the weekend. \n\nThe pattern suggests the model likely underpredicts during rush hours. The model predicts an \"average\" rush hour, but actual demand swings above and below that average more dramatically than during calmer periods like midday or overnight.\n\nThe divide for training data and test data is week 50. We assume that prior to week 50 is Fall and after week 50 is considered winter. Looking at both the Fall and Winter for the seasonal pattern. The error overall decreases as it shifts into winter. However, it is likely that the decrease in the errors is because of lower ridership, which makes the prediction easier and results in more stable patterns. Fall errors being higher isn't because the model performs worse on training data—it's because Fall has higher absolute ridership. This also means more variability in trip counts and, therefore, more room for prediction error.\n\n\n**Demographic patterns:**\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Join demographic data to station errors\nstation_errors_demo <- station_errors %>%\n  left_join(\n    station_attributes %>% select(start_station, Med_Inc, Percent_Taking_Transit, Percent_White),\n    by = \"start_station\"\n  ) %>%\n  filter(!is.na(Med_Inc))\n\n# Create plots\np1 <- ggplot(station_errors_demo, aes(x = Med_Inc, y = MAE)) +\n  geom_point(alpha = 0.5, color = \"#3182bd\") +\n  geom_smooth(method = \"lm\", se = FALSE, color = \"red\") +\n  scale_x_continuous(labels = scales::dollar) +\n  labs(title = \"Errors vs. Median Income\", x = \"Median Income\", y = \"MAE\") +\n  plotTheme\n\np2 <- ggplot(station_errors_demo, aes(x = Percent_Taking_Transit, y = MAE)) +\n  geom_point(alpha = 0.5, color = \"#3182bd\") +\n  geom_smooth(method = \"lm\", se = FALSE, color = \"red\") +\n  labs(title = \"Errors vs. Transit Usage\", x = \"% Taking Transit\", y = \"MAE\") +\n  plotTheme\n\np3 <- ggplot(station_errors_demo, aes(x = Percent_White, y = MAE)) +\n  geom_point(alpha = 0.5, color = \"#3182bd\") +\n  geom_smooth(method = \"lm\", se = FALSE, color = \"red\") +\n  labs(title = \"Errors vs. Race\", x = \"% White\", y = \"MAE\") +\n  plotTheme\n\ngrid.arrange(p1, p2, p3, ncol = 2)\n```\n\n::: {.cell-output-display}\n![](Tim_Ye_Assignment5_files/figure-html/errors_demographics-1.png){width=672}\n:::\n:::\n\n\nThe errors patterns show a relatively weak relationship with the census characteristics but there are still trends. MAE increase slightly as median income increase and MAE also increase in whiter areas. This suggest that the model struggle in affluent and white communities. In contrast, errors decline as transit usage increases, meaning neighborhoods with more transit riders are somewhat easier to predict. These patterns indicate that certain community types—particularly high-income or predominantly White tracts—may be systematically harder for the model to capture. This could reflect that the prediction performance varies in different demographic and socioeconomic groups. The equity implication for this is that model-based decisions could unevenly impact the disadvantage communities unless the biases are identified and corrected. \n\n\n## Part 3: Feature Engineering & model improvement\n\n**Potential features to consider:**\nPhilly University and College shapefile\n\n::: {.cell}\n\n```{.r .cell-code}\n#load data\ncollege=st_read(here(\"data/Universities_Colleges.geojson\"))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nReading layer `Universities_Colleges' from data source \n  `/Users/ye/Documents/GitHub/portfolio-setup-yezhanggg/data/Universities_Colleges.geojson' \n  using driver `GeoJSON'\nSimple feature collection with 629 features and 10 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: -75.2514 ymin: 39.92048 xmax: -74.97471 ymax: 40.10695\nGeodetic CRS:  WGS 84\n```\n\n\n:::\n\n```{.r .cell-code}\n#convert them into point\ncollege_point=st_centroid(college)\ncollege=st_transform(college,crs =2272)\nstations_sf_proj=st_transform(stations_sf,crs=2272)\n#calculate the nearest distance from college to each bike station\nuni_coords <- st_coordinates(college)[, 1:2]\nbike_coords <- st_coordinates(stations_sf_proj)[, 1:2]\nnearest <- get.knnx(data = uni_coords,   # search in universities\n                    query = bike_coords,  # for each bike station\n                    k = 1)                # find 1 nearest\n\n# Add results to bike stations\nstations_sf$nearest_uni_distance <- nearest$nn.dist[,1]\nstations_sf$nearest_uni_index <- nearest$nn.index[,1]\n\n# Add university name\nstations_sf$nearest_uni_name <- college$name[nearest$nn.index[,1]]\n\n#join back to panel data\nstations_with_distance <- stations_sf %>%\n  st_drop_geometry() %>%\n  select(start_station, nearest_uni_distance)  # Keep station ID and distance\n\nstudy_panel_complete <- study_panel_complete %>%\n  left_join(stations_with_distance, by = \"start_station\")\n```\n:::\n\n\n\"Perfect biking weather\" indicator (60-75°F, no rain)\n\n::: {.cell}\n\n```{.r .cell-code}\nstudy_panel_complete=study_panel_complete%>%\n  mutate(perfect_weather=ifelse(Temperature>=60&Temperature<=75&Precipitation==0,1,0))\n```\n:::\n\n\nRerun everything \nTemporal Train/Test Split   \n\n::: {.cell}\n\n```{.r .cell-code}\n# Which stations have trips in BOTH early and late periods?\nearly_stations <- study_panel_complete %>%\n  filter(week < 50) %>%\n  filter(Trip_Count > 0) %>%\n  distinct(start_station) %>%\n  pull(start_station)\n\nlate_stations <- study_panel_complete %>%\n  filter(week >= 50) %>%\n  filter(Trip_Count > 0) %>%\n  distinct(start_station) %>%\n  pull(start_station)\n\n# Keep only stations that appear in BOTH periods\ncommon_stations <- intersect(early_stations, late_stations)\n\n\n# Filter panel to only common stations\nstudy_panel_complete <- study_panel_complete %>%\n  filter(start_station %in% common_stations)\n\n# NOW create train/test split\ntrain <- study_panel_complete %>%\n  filter(week < 50)\n\ntest <- study_panel_complete %>%\n  filter(week >= 50)\n\ncat(\"Training observations:\", format(nrow(train), big.mark = \",\"), \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nTraining observations: 410,628 \n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"Testing observations:\", format(nrow(test), big.mark = \",\"), \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nTesting observations: 171,684 \n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"Training date range:\", min(train$date), \"to\", max(train$date), \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nTraining date range: 19997 to 20065 \n```\n\n\n:::\n\n```{.r .cell-code}\ncat(\"Testing date range:\", min(test$date), \"to\", max(test$date), \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nTesting date range: 20066 to 20088 \n```\n\n\n:::\n:::\n\n\nFinal Model (based on model 2 plus two new features)\n\n::: {.cell}\n\n```{.r .cell-code}\ntrain <- train %>%\n  mutate(dotw_simple = factor(dotw, levels = c(\"Mon\", \"Tue\", \"Wed\", \"Thu\", \"Fri\", \"Sat\", \"Sun\")))\n\n# Set contrasts to treatment coding (dummy variables)\ncontrasts(train$dotw_simple) <- contr.treatment(7)\nFinal_Model <- lm(\n  Trip_Count ~ as.factor(hour) + dotw_simple + Temperature + Precipitation +\n    lag1Hour + lag3Hours + lag1day + perfect_weather + nearest_uni_distance,\n  data = train\n)\n\nsummary(Final_Model)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nCall:\nlm(formula = Trip_Count ~ as.factor(hour) + dotw_simple + Temperature + \n    Precipitation + lag1Hour + lag3Hours + lag1day + perfect_weather + \n    nearest_uni_distance, data = train)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-12.6491  -0.4718  -0.1260   0.1519  25.5211 \n\nCoefficients:\n                          Estimate    Std. Error t value             Pr(>|t|)\n(Intercept)          -0.1464970415  0.0127434419 -11.496 < 0.0000000000000002\nas.factor(hour)1     -0.0182714609  0.0112486604  -1.624              0.10431\nas.factor(hour)2     -0.0107940128  0.0110643090  -0.976              0.32928\nas.factor(hour)3     -0.0344526072  0.0110459708  -3.119              0.00181\nas.factor(hour)4     -0.0083531795  0.0109717622  -0.761              0.44646\nas.factor(hour)5      0.0611848981  0.0110358703   5.544   0.0000000295508649\nas.factor(hour)6      0.2575670218  0.0110628063  23.282 < 0.0000000000000002\nas.factor(hour)7      0.4098444408  0.0112596421  36.399 < 0.0000000000000002\nas.factor(hour)8      0.6507940325  0.0110663730  58.808 < 0.0000000000000002\nas.factor(hour)9      0.2738975965  0.0111458034  24.574 < 0.0000000000000002\nas.factor(hour)10     0.2175864385  0.0109658961  19.842 < 0.0000000000000002\nas.factor(hour)11     0.2614767519  0.0109827158  23.808 < 0.0000000000000002\nas.factor(hour)12     0.3636194978  0.0108879885  33.396 < 0.0000000000000002\nas.factor(hour)13     0.3290331906  0.0108042597  30.454 < 0.0000000000000002\nas.factor(hour)14     0.3582957691  0.0108404880  33.052 < 0.0000000000000002\nas.factor(hour)15     0.4378781197  0.0111892000  39.134 < 0.0000000000000002\nas.factor(hour)16     0.5179924213  0.0110139789  47.030 < 0.0000000000000002\nas.factor(hour)17     0.6468554690  0.0111187868  58.177 < 0.0000000000000002\nas.factor(hour)18     0.3333275911  0.0112064088  29.744 < 0.0000000000000002\nas.factor(hour)19     0.1663624023  0.0112143240  14.835 < 0.0000000000000002\nas.factor(hour)20     0.0498153289  0.0112824361   4.415   0.0000100896164512\nas.factor(hour)21     0.0518462585  0.0112638217   4.603   0.0000041677035984\nas.factor(hour)22     0.0624837504  0.0111837402   5.587   0.0000000231148447\nas.factor(hour)23     0.0192416386  0.0112496277   1.710              0.08719\ndotw_simple2          0.0071928563  0.0060194123   1.195              0.23211\ndotw_simple3         -0.0119256857  0.0059921760  -1.990              0.04657\ndotw_simple4         -0.0451924239  0.0057326832  -7.883   0.0000000000000032\ndotw_simple5         -0.0418175215  0.0059612574  -7.015   0.0000000000023050\ndotw_simple6         -0.0366658624  0.0059348918  -6.178   0.0000000006497233\ndotw_simple7         -0.0602417427  0.0060890954  -9.893 < 0.0000000000000002\nTemperature           0.0043733480  0.0001797252  24.334 < 0.0000000000000002\nPrecipitation        -0.9771055955  0.0715379325 -13.659 < 0.0000000000000002\nlag1Hour              0.3203241506  0.0014841433 215.831 < 0.0000000000000002\nlag3Hours             0.1141549365  0.0014532176  78.553 < 0.0000000000000002\nlag1day               0.1989228383  0.0013923784 142.865 < 0.0000000000000002\nperfect_weather      -0.0067215626  0.0045380586  -1.481              0.13857\nnearest_uni_distance -0.0000330542  0.0000009882 -33.449 < 0.0000000000000002\n                        \n(Intercept)          ***\nas.factor(hour)1        \nas.factor(hour)2        \nas.factor(hour)3     ** \nas.factor(hour)4        \nas.factor(hour)5     ***\nas.factor(hour)6     ***\nas.factor(hour)7     ***\nas.factor(hour)8     ***\nas.factor(hour)9     ***\nas.factor(hour)10    ***\nas.factor(hour)11    ***\nas.factor(hour)12    ***\nas.factor(hour)13    ***\nas.factor(hour)14    ***\nas.factor(hour)15    ***\nas.factor(hour)16    ***\nas.factor(hour)17    ***\nas.factor(hour)18    ***\nas.factor(hour)19    ***\nas.factor(hour)20    ***\nas.factor(hour)21    ***\nas.factor(hour)22    ***\nas.factor(hour)23    .  \ndotw_simple2            \ndotw_simple3         *  \ndotw_simple4         ***\ndotw_simple5         ***\ndotw_simple6         ***\ndotw_simple7         ***\nTemperature          ***\nPrecipitation        ***\nlag1Hour             ***\nlag3Hours            ***\nlag1day              ***\nperfect_weather         \nnearest_uni_distance ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.9974 on 410591 degrees of freedom\nMultiple R-squared:  0.3271,\tAdjusted R-squared:  0.327 \nF-statistic:  5543 on 36 and 410591 DF,  p-value: < 0.00000000000000022\n```\n\n\n:::\n:::\n\n\nCalculate the MAE of the Final Model\n\n::: {.cell}\n\n```{.r .cell-code}\n# Get predictions on test set\n\n# Create day of week factor with treatment (dummy) coding\ntest <- test %>%\n  mutate(dotw_simple = factor(dotw, levels = c(\"Mon\", \"Tue\", \"Wed\", \"Thu\", \"Fri\", \"Sat\", \"Sun\")))\n\n# Set contrasts to treatment coding (dummy variables)\ncontrasts(test$dotw_simple) <- contr.treatment(7)\n\ntest <- test %>%\n  mutate(\n    pred2 = predict(model2, newdata = test),\n    pred6 = predict(Final_Model, newdata = test)\n  )\n\n# Calculate MAE for each model\nmae_results <- data.frame(\n  Model = c(\n    \"2. + Temporal Lags\",\n    \"6. + Temporal Lags + Spatial/Temporal Feature\"\n  ),\n  MAE = c(\n    mean(abs(test$Trip_Count - test$pred2), na.rm = TRUE),\n    mean(abs(test$Trip_Count - test$pred6), na.rm = TRUE)\n    \n  )\n)\n\nkable(mae_results, \n      digits = 2,\n      caption = \"Mean Absolute Error by Model (Test Set)\",\n      col.names = c(\"Model\", \"MAE (trips)\")) %>%\n  kable_styling(bootstrap_options = c(\"striped\", \"hover\"))\n```\n\n::: {.cell-output-display}\n`````{=html}\n<table class=\"table table-striped table-hover\" style=\"margin-left: auto; margin-right: auto;\">\n<caption>Mean Absolute Error by Model (Test Set)</caption>\n <thead>\n  <tr>\n   <th style=\"text-align:left;\"> Model </th>\n   <th style=\"text-align:right;\"> MAE (trips) </th>\n  </tr>\n </thead>\n<tbody>\n  <tr>\n   <td style=\"text-align:left;\"> 2. + Temporal Lags </td>\n   <td style=\"text-align:right;\"> 0.40 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> 6. + Temporal Lags + Spatial/Temporal Feature </td>\n   <td style=\"text-align:right;\"> 0.41 </td>\n  </tr>\n</tbody>\n</table>\n\n`````\n:::\n:::\n\n\nMap the error out\n\n::: {.cell}\n\n```{.r .cell-code}\ntest <- test %>%\n  mutate(\n    error = Trip_Count - pred6,\n    abs_error = abs(error),\n    time_of_day = case_when(\n      hour < 7 ~ \"Overnight\",\n      hour >= 7 & hour < 10 ~ \"AM Rush\",\n      hour >= 10 & hour < 15 ~ \"Mid-Day\",\n      hour >= 15 & hour <= 18 ~ \"PM Rush\",\n      hour > 18 ~ \"Evening\"\n    )\n  )\n\n# Calculate MAE by station\nstation_errors <- test %>%\n  group_by(start_station, start_lat.x, start_lon.y) %>%\n  summarize(\n    MAE = mean(abs_error, na.rm = TRUE),\n    avg_demand = mean(Trip_Count, na.rm = TRUE),\n    .groups = \"drop\"\n  ) %>%\n  filter(!is.na(start_lat.x), !is.na(start_lon.y))\n\n## Create Two Maps Side-by-Side with Proper Legends (sorry these maps are ugly)\n\n# Calculate station errors\nstation_errors <- test %>%\n  filter(!is.na(pred6)) %>%\n  group_by(start_station, start_lat.x, start_lon.y) %>%\n  summarize(\n    MAE = mean(abs(Trip_Count - pred6), na.rm = TRUE),\n    avg_demand = mean(Trip_Count, na.rm = TRUE),\n    .groups = \"drop\"\n  ) %>%\n  filter(!is.na(start_lat.x), !is.na(start_lon.y))\n\n# Map 1: Prediction Errors\np1 <- ggplot() +\n  geom_sf(data = philly_census, fill = \"grey95\", color = \"white\", size = 0.1) +\n  geom_point(\n    data = station_errors,\n    aes(x = start_lon.y, y = start_lat.x, color = MAE),\n    size = 3.5,\n    alpha = 0.7\n  ) +\n  scale_color_viridis(\n    option = \"plasma\",\n    name = \"MAE (trips)\",\n    direction = -1,\n    breaks = c(0.5, 1.0, 1.5),\n    labels = c(\"0.5\", \"1.0\", \"1.5\")\n  ) +\n  labs(title = \"Prediction Errors\") +\n  mapTheme +\n  theme(\n    legend.position = \"bottom\",\n    legend.title = element_text(size = 10, face = \"bold\"),\n    legend.text = element_text(size = 9),\n    plot.title = element_text(size = 14, face = \"bold\", hjust = 0.5)\n  ) +\n  guides(color = guide_colorbar(\n    barwidth = 12,\n    barheight = 1,\n    title.position = \"top\",\n    title.hjust = 0.5\n  ))\n\n# Map 2: Average Demand  \np2 <- ggplot() +\n  geom_sf(data = philly_census, fill = \"grey95\", color = \"white\", size = 0.1) +\n  geom_point(\n    data = station_errors,\n    aes(x = start_lon.y, y = start_lat.x, color = avg_demand),\n    size = 3.5,\n    alpha = 0.7\n  ) +\n  scale_color_viridis(\n    option = \"viridis\",\n    name = \"Avg Demand (trips/hour)\",\n    direction = -1,\n    breaks = c(0.5, 1.0, 1.5, 2.0, 2.5),\n    labels = c(\"0.5\", \"1.0\", \"1.5\", \"2.0\", \"2.5\")\n  ) +\n  labs(title = \"Average Demand\") +\n  mapTheme +\n  theme(\n    legend.position = \"bottom\",\n    legend.title = element_text(size = 10, face = \"bold\"),\n    legend.text = element_text(size = 9),\n    plot.title = element_text(size = 14, face = \"bold\", hjust = 0.5)\n  ) +\n  guides(color = guide_colorbar(\n    barwidth = 12,\n    barheight = 1,\n    title.position = \"top\",\n    title.hjust = 0.5\n  ))\n\n# Combine\ngrid.arrange(\n  p1, p2,\n  ncol = 2\n  )\n```\n\n::: {.cell-output-display}\n![](Tim_Ye_Assignment5_files/figure-html/unnamed-chunk-26-1.png){width=672}\n:::\n:::\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nerror_sf=st_as_sf(station_errors,coords=c(\"start_lon.y\",\"start_lat.x\"),crs=4326)\nerror_tract=st_join(error_sf,philly_census,join = st_within)\nerror_tract_out=error_tract%>%\n  st_drop_geometry()%>%\n  select(MAE,NAME)\ntop5 <- error_tract_out[order(error_tract_out$MAE, decreasing = TRUE)[1:5], ]\nprint(top5)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 5 × 2\n    MAE NAME                                                  \n  <dbl> <chr>                                                 \n1  1.42 Census Tract 125.02; Philadelphia County; Pennsylvania\n2  1.32 Census Tract 12.04; Philadelphia County; Pennsylvania \n3  1.28 Census Tract 214; Philadelphia County; Pennsylvania   \n4  1.26 Census Tract 19; Philadelphia County; Pennsylvania    \n5  1.20 Census Tract 13.02; Philadelphia County; Pennsylvania \n```\n\n\n:::\n:::\n\n\nThe two new features are perfect_weather and nearest_uni_distance. Perfect weather is defined as days where the temperature is between 60 to 75 degrees with no rain. This predictor was chosen because it allows the model to capture the threshold effect that linear weather terms miss. There are many perfect biking days in October but far fewer in November and December. University areas have unique demand patterns with different ridership based on time of day and time of year. By including distance to the nearest university, the model can adjust predictions for stations and address the spatial clustering of errors around University City.\n\nLooking at the MAE for both models, the final model has a slightly higher MAE of 0.41 compared to 0.40 for the baseline Model 2. The features helped but didn't fully solve the problem. The top 5 highest-error tracts are all located in Center City and South Philadelphia—areas that are 1–3 miles from UPenn, which falls in the \"bikeable but not walkable\" zone where university-related demand is most unpredictable. These tracts have high and variable demand driven by a mix of uses.\n\n\n## Part 4: Critical Reflection \n\nIn conclusion, it is hard to decide if the final MAE of the model good enough to use, as the mean MAE is still around 0.4, with maximum MAE of 1.42 (Model 2). Based on the mean of the count data (0.53), such a model would easily overestimate in low-demand area. However, for areas with a high demand, such as center city, the MAE seems not to be an issue. Temporally, the model did not predict well in rush hours while did a better job at night and midnight. \n\nThe prediction errors were clustered between University City and City Hall (east center city). One of the potential explanation would be that location is too far to commute on foot but too close/expensive to commute by car. The distance is perfect for bike riding for work and school and thus the model underestimate it. However, after adding distance to college, the model even performed worse a little bit. In the future, more comprehensive spatial data such as employment centers/sub centers could be brought into the model.\n\nBased on the current distribution of Bike station, it is hard to say the data we collected perfectly reflect the demand of biking in Philadelphia. As most of the bike stations are clustered in center city and university city,low-income communities at the north and the south of Philadelphia do not have access to bike station at all. I recommend placing more bike stations in those vulnerable communities to create a more equal access to indego biking.\n\n---",
    "supporting": [
      "Tim_Ye_Assignment5_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {
      "include-in-header": [
        "<script src=\"../site_libs/kePrint-0.0.1/kePrint.js\"></script>\n<link href=\"../site_libs/lightable-0.0.1/lightable.css\" rel=\"stylesheet\" />\n"
      ]
    },
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}