[
  {
    "objectID": "labs/presentation/presentation.html#project-intention",
    "href": "labs/presentation/presentation.html#project-intention",
    "title": "Predicting Housing Sales",
    "section": "Project Intention",
    "text": "Project Intention\nWhy it Matters\nPhiladelphia’s current Automated Valuation Model (AVM) is underperforming, leading to inconsistent and inequitable property assessments.\nOur Guiding Question\nWhat key predictors are missing from the legacy model, and how can we improve its accuracy and fairness?"
  },
  {
    "objectID": "labs/presentation/presentation.html#data-sources",
    "href": "labs/presentation/presentation.html#data-sources",
    "title": "Predicting Housing Sales",
    "section": "Data Sources",
    "text": "Data Sources\n\nCity of Philadelphia\n\nPhiladelphia Properties and Current Assessments (n= 21,807 Home Sales)\n\nOpenDataPhilly\n\nNeighborhood Boundaries (Polygon)\nSchool Locations, Attendance, and Withdrawals\n\nGeofabrik (OpenStreetMap)\n\nCommercial and Economic Activity Points\n\nU.S. Census (ACS 5-Year)\n\nEducation\nVacancy Rates\nFamily Household Share\nPercent White\nMedian Household Income"
  },
  {
    "objectID": "labs/presentation/presentation.html#mapped-observations",
    "href": "labs/presentation/presentation.html#mapped-observations",
    "title": "Predicting Housing Sales",
    "section": "Mapped Observations",
    "text": "Mapped Observations\n\n\n\n\n\nHome Price Across the City\n\n\n\nKey Findings\n\nPrices cluster spatially\n\nCenter City & Northwest Philadelphia show highest home values\n\nAdditional hotspots in South Philadelphia"
  },
  {
    "objectID": "labs/presentation/presentation.html#structural-price-drivers",
    "href": "labs/presentation/presentation.html#structural-price-drivers",
    "title": "Predicting Housing Sales",
    "section": "Structural Price Drivers",
    "text": "Structural Price Drivers"
  },
  {
    "objectID": "labs/presentation/presentation.html#progressive-regression-models",
    "href": "labs/presentation/presentation.html#progressive-regression-models",
    "title": "Predicting Housing Sales",
    "section": "Progressive Regression Models",
    "text": "Progressive Regression Models\nGoal: Test whether place-based factors improve predictive accuracy.\n\\[\n\\begin{aligned}\n\\log(\\text{Price}) = \\beta_0 &+ \\color{red}{\\beta_1 \\log(\\text{Livable Area}) + \\beta_2 \\text{Bedrooms} + \\beta_3 \\text{Bathrooms}} \\\\\n&+ \\color{red}{\\beta_4 \\text{Age} + \\beta_5 \\text{Interior Condition} + \\beta_6 \\text{Quality Grade}} \\\\\n&+ \\color{orange}{\\beta_7 \\text{Median Income} + \\beta_8 \\text{Family Household Ratio}}  \\\\\n&+ \\color{orange}{\\beta_9 \\text{% Bachelors}+ \\beta_{10} \\text{% Vacant} + \\beta_{11} \\text{% White}} \\\\\n&+ \\color{blue}{\\beta_{12} \\text{EconKDE} + \\beta_{13} \\text{EconKDE}^2 + \\beta_{14} \\text{TreeKDE} + \\beta_{15} \\text{Mean 3NN Dist}} \\\\\n&+ \\color{green}{\\beta_{16} \\text{MHI Quantile} + \\beta_{17} (\\text{MHI Quantile} \\times \\text{Age})} + \\varepsilon\n\\end{aligned}\n\\]\nModel Layers\n- Model 1: Structural\n- Model 2: + Demographics\n- Model 3: + Spatial\n- Model 4: + Interactions"
  },
  {
    "objectID": "labs/presentation/presentation.html#model-performance-improves-at-each-stage",
    "href": "labs/presentation/presentation.html#model-performance-improves-at-each-stage",
    "title": "Predicting Housing Sales",
    "section": "Model Performance Improves at Each Stage",
    "text": "Model Performance Improves at Each Stage\n\n\n\n\n\n\n\n\nModel\nAdj. R²\nInterpretation\n\n\n\n\n(1) Structural\n0.38\nHouse features alone are not enough\n\n\n(2) + Census\n0.60\nNeighborhood socioeconomic patterns matter\n\n\n(3) + Spatial\n0.61\nLocation and amenities add signal\n\n\n(4) + Interactions\n0.64\nWealthy areas shape how homes retain value\n\n\n\nTakeaway: Spatial + neighborhood effects are essential to explaining price variation in Philadelphia."
  },
  {
    "objectID": "labs/presentation/presentation.html#hardest-neighborhoods-to-predict",
    "href": "labs/presentation/presentation.html#hardest-neighborhoods-to-predict",
    "title": "Predicting Housing Sales",
    "section": "Hardest Neighborhoods to Predict",
    "text": "Hardest Neighborhoods to Predict\n\n\n\n\n\nProblem Neighborhoods\n\n\n\nKey Insight\n\nThe model struggles most in high-value, high-amenity neighborhoods\nUnder-predicted (red): Center City, Northwest Philly, Fishtown, South Philly\n→ Homes sell for more than the model expects\nOver-predicted (blue): Economically underserved neighborhoods\n→ Homes sell for less than predicted\nWhy? Prestige, walkability, and growing interest are not fully captured by structural or census variables"
  },
  {
    "objectID": "labs/presentation/presentation.html#key-findings-1",
    "href": "labs/presentation/presentation.html#key-findings-1",
    "title": "Predicting Housing Sales",
    "section": "Key Findings",
    "text": "Key Findings\nModel Accuracy: RMSE = 0.479 (log scale)\nModel Fit: R² = 0.64 (0.65 with K-fold CV)\nTop Predictors\n\nEconomic Density (U-shaped effect): Strongest spatial driver of price\nHome Size (β = 0.509, p &lt; 0.001): 1% increase in area → ~0.5% ↑ in price\nBathrooms (β = 0.151, p &lt; 0.001): Large value premium per added bath\nNeighborhood Composition: Higher education (+) and vacancy (–) strongly shape prices\n\nKey Insight:\nNeighborhood effects matter as much as home features — location premiums are real, measurable, and spatially clustered. —"
  },
  {
    "objectID": "labs/presentation/presentation.html#recommendations",
    "href": "labs/presentation/presentation.html#recommendations",
    "title": "Predicting Housing Sales",
    "section": "Recommendations",
    "text": "Recommendations\nImprove the City’s AVM by Incorporating:\n\nAmenity-based spatial features (EconKDE, trees, school access)\nNeighborhood context variables to capture local market conditions\n\nPolicy Implications\n\nCurrent AVM likely undervalues booming neighborhoods near economic hubs\nAffluent areas remain hardest to predict due to prestige and perception effects\nAdd monitoring for rapidly changing neighborhoods (e.g., Fishtown)\n\nBottom Line:\nTo reduce inequity and improve accuracy, the AVM must move beyond structure-only predictors and account for context, amenities, and spatial clustering."
  },
  {
    "objectID": "labs/presentation/presentation.html#limitations-next-steps",
    "href": "labs/presentation/presentation.html#limitations-next-steps",
    "title": "Predicting Housing Sales",
    "section": "Limitations & Next Steps",
    "text": "Limitations & Next Steps\nKey Limitations\n\nThe model cannot quantify word-of-mouth effects\n→ Perceived “vibe shifts” can rapidly change prices\nFuture plans are not incorporated\n→ Planned infrastructure can greatly alter market outcomes\n\nWhere Do We Go From Here?\n\nBuild an enhanced model that includes:\n→ Amenity proximity\n→ Economic development indicators\n→ Spatial clustering of sale prices\nContinue data collection to track a changing housing landscape\nConsult with local government on new projects and emerging trends"
  },
  {
    "objectID": "labs/presentation/presentation.html#questions",
    "href": "labs/presentation/presentation.html#questions",
    "title": "Predicting Housing Sales",
    "section": "Questions?",
    "text": "Questions?\n\nThank you!\n\n\nContact Information:\n\nJack Bader\n\njbader14@upenn.edu\n\nMatthew Levy\n\nmblevy@upenn.edu\n\nTim Wen\n\nsw6as@upenn.edu\n\n\n\n\nJoey Cahill\n\ncahill1@upenn.edu\n\nSam Sen\n\nsen1@upenn.edu\n\nYe Zhang\n\nyezhang1@upenn.edu"
  },
  {
    "objectID": "labs/Zhang_Ye_Assignment1.html",
    "href": "labs/Zhang_Ye_Assignment1.html",
    "title": "Assignment 1: Census Data Quality for Policy Decisions",
    "section": "",
    "text": "You are a data analyst for the [California] Department of Human Services. The department is considering implementing an algorithmic system to identify communities that should receive priority for social service funding and outreach programs. Your supervisor has asked you to evaluate the quality and reliability of available census data to inform this decision.\nDrawing on our Week 2 discussion of algorithmic bias, you need to assess not just what the data shows, but how reliable it is and what communities might be affected by data quality issues.\n\n\n\n\nApply dplyr functions to real census data for policy analysis\nEvaluate data quality using margins of error\nConnect technical analysis to algorithmic decision-making\nIdentify potential equity implications of data reliability issues\nCreate professional documentation for policy stakeholders\n\n\n\n\nSubmit by posting your updated portfolio link on Canvas. Your assignment should be accessible at your-portfolio-url/assignments/assignment_1/\nMake sure to update your _quarto.yml navigation to include this assignment under an “Assignments” menu."
  },
  {
    "objectID": "labs/Zhang_Ye_Assignment1.html#scenario",
    "href": "labs/Zhang_Ye_Assignment1.html#scenario",
    "title": "Assignment 1: Census Data Quality for Policy Decisions",
    "section": "",
    "text": "You are a data analyst for the [California] Department of Human Services. The department is considering implementing an algorithmic system to identify communities that should receive priority for social service funding and outreach programs. Your supervisor has asked you to evaluate the quality and reliability of available census data to inform this decision.\nDrawing on our Week 2 discussion of algorithmic bias, you need to assess not just what the data shows, but how reliable it is and what communities might be affected by data quality issues."
  },
  {
    "objectID": "labs/Zhang_Ye_Assignment1.html#learning-objectives",
    "href": "labs/Zhang_Ye_Assignment1.html#learning-objectives",
    "title": "Assignment 1: Census Data Quality for Policy Decisions",
    "section": "",
    "text": "Apply dplyr functions to real census data for policy analysis\nEvaluate data quality using margins of error\nConnect technical analysis to algorithmic decision-making\nIdentify potential equity implications of data reliability issues\nCreate professional documentation for policy stakeholders"
  },
  {
    "objectID": "labs/Zhang_Ye_Assignment1.html#submission-instructions",
    "href": "labs/Zhang_Ye_Assignment1.html#submission-instructions",
    "title": "Assignment 1: Census Data Quality for Policy Decisions",
    "section": "",
    "text": "Submit by posting your updated portfolio link on Canvas. Your assignment should be accessible at your-portfolio-url/assignments/assignment_1/\nMake sure to update your _quarto.yml navigation to include this assignment under an “Assignments” menu."
  },
  {
    "objectID": "labs/Zhang_Ye_Assignment1.html#data-retrieval",
    "href": "labs/Zhang_Ye_Assignment1.html#data-retrieval",
    "title": "Assignment 1: Census Data Quality for Policy Decisions",
    "section": "2.1 Data Retrieval",
    "text": "2.1 Data Retrieval\nYour Task: Use get_acs() to retrieve county-level data for your chosen state.\nRequirements: - Geography: county level - Variables: median household income (B19013_001) and total population (B01003_001)\n- Year: 2022 - Survey: acs5 - Output format: wide\nHint: Remember to give your variables descriptive names using the variables = c(name = \"code\") syntax.\n\n# Write your get_acs() code here\n\ncounty_data &lt;- get_acs(\n  geography = \"county\",\n  variables = c(\n    median_income = \"B19013_001\",  \n    total_pop = \"B01003_001\"    \n  ),\n  state = my_state,\n  year = 2022,\n  output = \"wide\"\n)\n\n# Clean the county names to remove state name and \"County\" \n# Hint: use mutate() with str_remove()\n\ncounty_data &lt;- county_data %&gt;%\n  mutate(NAME = str_remove(NAME, \" County, California\"))\n\n# Display the first few rows\nhead(county_data)\n\n# A tibble: 6 × 6\n  GEOID NAME      median_incomeE median_incomeM total_popE total_popM\n  &lt;chr&gt; &lt;chr&gt;              &lt;dbl&gt;          &lt;dbl&gt;      &lt;dbl&gt;      &lt;dbl&gt;\n1 06001 Alameda           122488           1231    1663823         NA\n2 06003 Alpine            101125          17442       1515        206\n3 06005 Amador             74853           6048      40577         NA\n4 06007 Butte              66085           2261     213605         NA\n5 06009 Calaveras          77526           3875      45674         NA\n6 06011 Colusa             69619           5745      21811         NA"
  },
  {
    "objectID": "labs/Zhang_Ye_Assignment1.html#data-quality-assessment",
    "href": "labs/Zhang_Ye_Assignment1.html#data-quality-assessment",
    "title": "Assignment 1: Census Data Quality for Policy Decisions",
    "section": "2.2 Data Quality Assessment",
    "text": "2.2 Data Quality Assessment\nYour Task: Calculate margin of error percentages and create reliability categories.\nRequirements: - Calculate MOE percentage: (margin of error / estimate) * 100 - Create reliability categories: - High Confidence: MOE &lt; 5% - Moderate Confidence: MOE 5-10%\n- Low Confidence: MOE &gt; 10% - Create a flag for unreliable estimates (MOE &gt; 10%)\nHint: Use mutate() with case_when() for the categories.\n\n# Calculate MOE percentage and reliability categories using mutate()\n\ncounty_data &lt;- county_data %&gt;%\n  mutate(\n    income_moe_pct = (median_incomeM / median_incomeE) * 100,\n     reliability_category = case_when(\n      income_moe_pct &lt; 5 ~ \"High Confidence\",\n      income_moe_pct &lt;= 10 ~ \"Moderate Confidence\",\n      TRUE ~ \"Low Confidence\"),\n        unreliable_flag = income_moe_pct &gt; 10)\n\n# Create a summary showing count of counties in each reliability category\n# Hint: use count() and mutate() to add percentages\n\nreliability_summary &lt;- county_data %&gt;%\n  count(reliability_category) %&gt;%\n  mutate(\n    percentage = round(n / sum(n) * 100, 1)\n  )"
  },
  {
    "objectID": "labs/Zhang_Ye_Assignment1.html#high-uncertainty-counties",
    "href": "labs/Zhang_Ye_Assignment1.html#high-uncertainty-counties",
    "title": "Assignment 1: Census Data Quality for Policy Decisions",
    "section": "2.3 High Uncertainty Counties",
    "text": "2.3 High Uncertainty Counties\nYour Task: Identify the 5 counties with the highest MOE percentages.\nRequirements: - Sort by MOE percentage (highest first) - Select the top 5 counties - Display: county name, median income, margin of error, MOE percentage, reliability category - Format as a professional table using kable()\nHint: Use arrange(), slice(), and select() functions.\n\n# Create table of top 5 counties by MOE percentage\n\nhigh_uncertainty_table &lt;- county_data %&gt;%\n  arrange(desc(income_moe_pct)) %&gt;%  \n  slice_head(n = 5) %&gt;% \n  select(NAME, median_incomeE, median_incomeM, income_moe_pct, reliability_category)\n\n# Format as table with kable() - include appropriate column names and caption\n\nlibrary(knitr)\nlibrary(kableExtra)\n\nhigh_uncertainty_table %&gt;%\n  kable(\n    col.names = c(\"County\", \"Median Income\", \"Margin of Error\", \"MOE (%)\", \"Reliability\"),\n    caption = \"Top 5 California Counties with Highest Income Uncertainty\",\n    format.args = list(big.mark = \",\"),\n    digits = c(0, 0, 0, 1, 0) \n  ) %&gt;%\n  kable_styling(bootstrap_options = c(\"striped\", \"hover\"))\n\n\nTop 5 California Counties with Highest Income Uncertainty\n\n\nCounty\nMedian Income\nMargin of Error\nMOE (%)\nReliability\n\n\n\n\nMono\n82,038\n15,388\n18.8\nLow Confidence\n\n\nAlpine\n101,125\n17,442\n17.2\nLow Confidence\n\n\nSierra\n61,108\n9,237\n15.1\nLow Confidence\n\n\nTrinity\n47,317\n5,890\n12.4\nLow Confidence\n\n\nPlumas\n67,885\n7,772\n11.4\nLow Confidence\n\n\n\n\n\nData Quality Commentary:\n[Counties that have higher margin of error will have unreliable income estimates and could lead to misleading policy plan and resource allocation. The high margin of error could because these are counties that have low population and the data could easily skewed.]"
  },
  {
    "objectID": "labs/Zhang_Ye_Assignment1.html#focus-area-selection",
    "href": "labs/Zhang_Ye_Assignment1.html#focus-area-selection",
    "title": "Assignment 1: Census Data Quality for Policy Decisions",
    "section": "3.1 Focus Area Selection",
    "text": "3.1 Focus Area Selection\nYour Task: Select 2-3 counties from your reliability analysis for detailed tract-level study.\nStrategy: Choose counties that represent different reliability levels (e.g., 1 high confidence, 1 moderate, 1 low confidence) to compare how data quality varies.\n\n# Use filter() to select 2-3 counties from your county_reliability data\n# Store the selected counties in a variable called selected_counties\n\nselected_counties &lt;- county_data %&gt;%\n  filter(NAME %in% c(\"Los Angeles\", \"Trinity\", \"Tehama\")) %&gt;%\n  select(NAME, median_incomeE, income_moe_pct, reliability_category)\n\n# Display the selected counties with their key characteristics\n# Show: county name, median income, MOE percentage, reliability category\n\nprint(selected_counties)\n\n# A tibble: 3 × 4\n  NAME        median_incomeE income_moe_pct reliability_category\n  &lt;chr&gt;                &lt;dbl&gt;          &lt;dbl&gt; &lt;chr&gt;               \n1 Los Angeles          83411          0.526 High Confidence     \n2 Tehama               59029          6.95  Moderate Confidence \n3 Trinity              47317         12.4   Low Confidence      \n\n\nComment on the output: [Los Angeles has a High Confidence category because of its extremely low income margin of error percent. This is because Los Angeles is one of the most populated county and that lead to low margin of error. On the another other hand, Tehama has a income margin of error percentage around 7 and Trinity is at 12. The data for Tehama and Trinity are less reliable compare to Los Angeles County.]"
  },
  {
    "objectID": "labs/Zhang_Ye_Assignment1.html#tract-level-demographics",
    "href": "labs/Zhang_Ye_Assignment1.html#tract-level-demographics",
    "title": "Assignment 1: Census Data Quality for Policy Decisions",
    "section": "3.2 Tract-Level Demographics",
    "text": "3.2 Tract-Level Demographics\nYour Task: Get demographic data for census tracts in your selected counties.\nRequirements: - Geography: tract level - Variables: white alone (B03002_003), Black/African American (B03002_004), Hispanic/Latino (B03002_012), total population (B03002_001) - Use the same state and year as before - Output format: wide - Challenge: You’ll need county codes, not names. Look at the GEOID patterns in your county data for hints.\n\n# Define your race/ethnicity variables with descriptive names\n\nrace_vars &lt;- c(\n  total_pop = \"B03002_001\",\n  white_alone = \"B03002_003\",\n  black_alone = \"B03002_004\",\n  hispanic = \"B03002_012\"\n)\n\n# Use get_acs() to retrieve tract-level data\n# Hint: You may need to specify county codes in the county parameter\n\ntract_demographics &lt;- get_acs(\n  geography = \"tract\",\n  variables = race_vars,\n  state = my_state,\n  county = c(\"037\", \"105\", \"103\"),\n  year = 2022,\n  output = \"wide\"\n)\n\n# Calculate percentage of each group using mutate()\n# Create percentages for white, Black, and Hispanic populations\n\ntract_demographics &lt;- tract_demographics %&gt;%\n  mutate(\n    white_pct = (white_aloneE / total_popE) * 100,\n    black_pct = (black_aloneE / total_popE) * 100,\n    hispanic_pct = (hispanicE / total_popE) * 100\n  )\n\n# Add readable tract and county name columns using str_extract() or similar\n\ntract_demographics &lt;- tract_demographics %&gt;%\n  mutate(\n    census_tract = str_remove(NAME, \";.*\"),\n    county = str_remove(NAME, \"Census Tract [0-9.]+; \") %&gt;%\n             str_remove(\"; California\") %&gt;%\n             str_remove(\" County\")\n  )"
  },
  {
    "objectID": "labs/Zhang_Ye_Assignment1.html#demographic-analysis",
    "href": "labs/Zhang_Ye_Assignment1.html#demographic-analysis",
    "title": "Assignment 1: Census Data Quality for Policy Decisions",
    "section": "3.3 Demographic Analysis",
    "text": "3.3 Demographic Analysis\nYour Task: Analyze the demographic patterns in your selected areas.\n\n# Find the tract with the highest percentage of Hispanic/Latino residents\n# Hint: use arrange() and slice() to get the top tract\n\nhighest_hispanic &lt;- tract_demographics %&gt;%\n  arrange(desc(hispanic_pct)) %&gt;%\n  slice_head(n = 1) %&gt;%\n  select(census_tract, county, hispanic_pct)\n\n# Calculate average demographics by county using group_by() and summarize()\n# Show: number of tracts, average percentage for each racial/ethnic group\n\ncounty_demographics &lt;- tract_demographics %&gt;%\n  group_by(county) %&gt;%\n  summarize(\n    num_tracts = n(),\n    avg_white_pct = mean(white_pct, na.rm = TRUE),\n    avg_black_pct = mean(black_pct, na.rm = TRUE),\n    avg_hispanic_pct = mean(hispanic_pct, na.rm = TRUE)\n  )\n\n# Create a nicely formatted table of your results using kable()\n\nlibrary(knitr)          \ncounty_demographics %&gt;%\n  kable(\n    col.names = c(\"County\", \"Number of Tracts\", \"Avg White %\", \"Avg Black %\", \"Avg Hispanic %\"), \n    caption = \"Average Demographics by County\",  \n    digits = 1           \n  )\n\n\nAverage Demographics by County\n\n\n\n\n\n\n\n\n\nCounty\nNumber of Tracts\nAvg White %\nAvg Black %\nAvg Hispanic %\n\n\n\n\nLos Angeles\n2498\n26.3\n7.6\n47.6\n\n\nTehama\n14\n65.9\n0.9\n26.0\n\n\nTrinity\n4\n79.2\n1.7\n7.0"
  },
  {
    "objectID": "labs/Zhang_Ye_Assignment1.html#moe-analysis-for-demographic-variables",
    "href": "labs/Zhang_Ye_Assignment1.html#moe-analysis-for-demographic-variables",
    "title": "Assignment 1: Census Data Quality for Policy Decisions",
    "section": "4.1 MOE Analysis for Demographic Variables",
    "text": "4.1 MOE Analysis for Demographic Variables\nYour Task: Examine margins of error for demographic variables to see if some communities have less reliable data.\nRequirements: - Calculate MOE percentages for each demographic variable - Flag tracts where any demographic variable has MOE &gt; 15% - Create summary statistics\n\n# Calculate MOE percentages for white, Black, and Hispanic variables\n# Hint: use the same formula as before (margin/estimate * 100)\n\ntract_demographics &lt;- tract_demographics %&gt;%\n  mutate(\n    white_moe_pct = (white_aloneM / white_aloneE) * 100,\n    black_moe_pct = (black_aloneM / black_aloneE) * 100,\n    hispanic_moe_pct = (hispanicM / hispanicE) * 100,\n    high_moe_flag = ifelse(white_moe_pct &gt; 15 | black_moe_pct &gt; 15 | hispanic_moe_pct &gt; 15, TRUE, FALSE)\n  )\n\n# Create a flag for tracts with high MOE on any demographic variable\n# Use logical operators (| for OR) in an ifelse() statement\n\n# Create summary statistics showing how many tracts have data quality issues\n\nmoe_summary &lt;- tract_demographics %&gt;%\n  summarize(\n    total_tracts = n(),\n    tracts_with_issues = sum(high_moe_flag, na.rm = TRUE),\n    pct_with_issues = round(sum(high_moe_flag, na.rm = TRUE) / n() * 100, 1)\n  )\n\nmoe_summary\n\n# A tibble: 1 × 3\n  total_tracts tracts_with_issues pct_with_issues\n         &lt;int&gt;              &lt;int&gt;           &lt;dbl&gt;\n1         2516               2515             100"
  },
  {
    "objectID": "labs/Zhang_Ye_Assignment1.html#pattern-analysis",
    "href": "labs/Zhang_Ye_Assignment1.html#pattern-analysis",
    "title": "Assignment 1: Census Data Quality for Policy Decisions",
    "section": "4.2 Pattern Analysis",
    "text": "4.2 Pattern Analysis\nYour Task: Investigate whether data quality problems are randomly distributed or concentrated in certain types of communities.\n\n# Group tracts by whether they have high MOE issues\n# Calculate average characteristics for each group:\n# - population size, demographic percentages\n\npattern_comparison &lt;- tract_demographics %&gt;%\n  group_by(high_moe_flag) %&gt;%\n  summarize(\n    num_tracts = n(),\n    avg_population = mean(total_popE, na.rm = TRUE),\n    avg_pct_white = mean(white_pct, na.rm = TRUE),\n    avg_pct_black = mean(black_pct, na.rm = TRUE),\n    avg_pct_hispanic = mean(hispanic_pct, na.rm = TRUE)\n  )\n\n# Use group_by() and summarize() to create this comparison\n# Create a professional table showing the patterns\n\nlibrary(knitr)\npattern_comparison %&gt;%\n  kable(\n    col.names = c(\"High MOE Issues\", \"Number of Tracts\", \"Avg Population\", \n                  \"Avg White% \", \"Avg Black %\", \"Avg Hispanic %\"),\n    caption = \"Comparison of Tract Characteristics by Data Quality\",\n    digits = c(0, 0, 0, 1, 1, 1)\n  )\n\n\nComparison of Tract Characteristics by Data Quality\n\n\n\n\n\n\n\n\n\n\nHigh MOE Issues\nNumber of Tracts\nAvg Population\nAvg White%\nAvg Black %\nAvg Hispanic %\n\n\n\n\nFALSE\n1\n8994\n16.8\n33.6\n41.4\n\n\nTRUE\n2515\n3980\n26.6\n7.5\n47.4\n\n\n\n\n\nPattern Analysis: [It is crazy that almost all of the data are not reliable. The estimate for racial group is not reliable on the census tract level through their high margin of error. I think this might further stress the point of how sampling population is critical in doing analysis. The average population for all the HIGH MOE Issues tracts is at 3980. The only one tract without HIGH MOE Issue has a population 0f 8994.]"
  },
  {
    "objectID": "labs/Zhang_Ye_Assignment1.html#analysis-integration-and-professional-summary",
    "href": "labs/Zhang_Ye_Assignment1.html#analysis-integration-and-professional-summary",
    "title": "Assignment 1: Census Data Quality for Policy Decisions",
    "section": "5.1 Analysis Integration and Professional Summary",
    "text": "5.1 Analysis Integration and Professional Summary\nYour Task: Write an executive summary that integrates findings from all four analyses.\nExecutive Summary Requirements: 1. Overall Pattern Identification: What are the systematic patterns across all your analyses? 2. Equity Assessment: Which communities face the greatest risk of algorithmic bias based on your findings? 3. Root Cause Analysis: What underlying factors drive both data quality issues and bias risk? 4. Strategic Recommendations: What should the Department implement to address these systematic issues?\nExecutive Summary:\n[I think the most important takeaway from this assignment is the important of sampling size. This situation has appear in multiple analysis and multple times. If a data has some reliability issue, one of the characteristic of that data is the small sampling size.\nThe community that faced the greatest risk of algorithmic bias is Trinity. Trinity only has 4 census tract and it such a small county that the data could highly likely to be skewed because of sampling error. It is also show as a “Low Confidence” in county level income data. The interpretation and result could be really not reliable due to data quality issue.\nThe root cause may be the survey design and also just how insufficient for the data to be reliable if we want to look at small geographic areas like census tracts. Rural places like Trinity and Tehama make reliable sampling really challenging.\nI think one of the most important is to apply strict MOE thresholds. Data that are not reliable especially on small sample and big geographic area it is really important to examine the data closely. Aggregate the data and looking at large scale is necessary if the public organization want to implement any policies. Careful decision making process is necessary and key for community success. ]"
  },
  {
    "objectID": "labs/Zhang_Ye_Assignment1.html#specific-recommendations",
    "href": "labs/Zhang_Ye_Assignment1.html#specific-recommendations",
    "title": "Assignment 1: Census Data Quality for Policy Decisions",
    "section": "6.3 Specific Recommendations",
    "text": "6.3 Specific Recommendations\nYour Task: Create a decision framework for algorithm implementation.\n\n# Create a summary table using your county reliability data\n# Include: county name, median income, MOE percentage, reliability category\n\ndecision_framework &lt;- county_data %&gt;%\n  select(NAME, median_incomeE, income_moe_pct, reliability_category) %&gt;%\n  mutate(\n    algorithm_recommendation = case_when(\n      reliability_category == \"High Confidence\" ~ \"Safe for algorithmic decisions\",\n      reliability_category == \"Moderate Confidence\" ~ \"Use with caution - monitor outcomes\",\n      reliability_category == \"Low Confidence\" ~ \"Requires manual review or additional data\"\n    )\n  )\n\n# Add a new column with algorithm recommendations using case_when():\n# - High Confidence: \"Safe for algorithmic decisions\"\n# - Moderate Confidence: \"Use with caution - monitor outcomes\"  \n# - Low Confidence: \"Requires manual review or additional data\"\n\n# Format as a professional table with kable()\n\nlibrary(knitr)\nlibrary(kableExtra)\n\ndecision_framework %&gt;%\n  kable(\n    col.names = c(\"County\", \"Median Income\", \"MOE %\", \"Reliability\", \"Algorithm Recommendation\"),\n    caption = \"Decision Framework for Algorithm Implementation by County\",\n    format.args = list(big.mark = \",\"),\n    digits = c(0, 0, 1, 0, 0)\n  ) %&gt;%\n  kable_styling(bootstrap_options = c(\"striped\", \"hover\"))\n\n\nDecision Framework for Algorithm Implementation by County\n\n\nCounty\nMedian Income\nMOE %\nReliability\nAlgorithm Recommendation\n\n\n\n\nAlameda\n122,488\n1.0\nHigh Confidence\nSafe for algorithmic decisions\n\n\nAlpine\n101,125\n17.2\nLow Confidence\nRequires manual review or additional data\n\n\nAmador\n74,853\n8.1\nModerate Confidence\nUse with caution - monitor outcomes\n\n\nButte\n66,085\n3.4\nHigh Confidence\nSafe for algorithmic decisions\n\n\nCalaveras\n77,526\n5.0\nHigh Confidence\nSafe for algorithmic decisions\n\n\nColusa\n69,619\n8.3\nModerate Confidence\nUse with caution - monitor outcomes\n\n\nContra Costa\n120,020\n1.2\nHigh Confidence\nSafe for algorithmic decisions\n\n\nDel Norte\n61,149\n7.2\nModerate Confidence\nUse with caution - monitor outcomes\n\n\nEl Dorado\n99,246\n3.4\nHigh Confidence\nSafe for algorithmic decisions\n\n\nFresno\n67,756\n1.4\nHigh Confidence\nSafe for algorithmic decisions\n\n\nGlenn\n64,033\n6.2\nModerate Confidence\nUse with caution - monitor outcomes\n\n\nHumboldt\n57,881\n3.7\nHigh Confidence\nSafe for algorithmic decisions\n\n\nImperial\n53,847\n4.1\nHigh Confidence\nSafe for algorithmic decisions\n\n\nInyo\n63,417\n8.6\nModerate Confidence\nUse with caution - monitor outcomes\n\n\nKern\n63,883\n2.1\nHigh Confidence\nSafe for algorithmic decisions\n\n\nKings\n68,540\n3.3\nHigh Confidence\nSafe for algorithmic decisions\n\n\nLake\n56,259\n4.3\nHigh Confidence\nSafe for algorithmic decisions\n\n\nLassen\n59,515\n6.0\nModerate Confidence\nUse with caution - monitor outcomes\n\n\nLos Angeles\n83,411\n0.5\nHigh Confidence\nSafe for algorithmic decisions\n\n\nMadera\n73,543\n3.9\nHigh Confidence\nSafe for algorithmic decisions\n\n\nMarin\n142,019\n2.9\nHigh Confidence\nSafe for algorithmic decisions\n\n\nMariposa\n60,021\n8.8\nModerate Confidence\nUse with caution - monitor outcomes\n\n\nMendocino\n61,335\n3.6\nHigh Confidence\nSafe for algorithmic decisions\n\n\nMerced\n64,772\n3.3\nHigh Confidence\nSafe for algorithmic decisions\n\n\nModoc\n54,962\n9.8\nModerate Confidence\nUse with caution - monitor outcomes\n\n\nMono\n82,038\n18.8\nLow Confidence\nRequires manual review or additional data\n\n\nMonterey\n91,043\n2.1\nHigh Confidence\nSafe for algorithmic decisions\n\n\nNapa\n105,809\n2.8\nHigh Confidence\nSafe for algorithmic decisions\n\n\nNevada\n79,395\n4.8\nHigh Confidence\nSafe for algorithmic decisions\n\n\nOrange\n109,361\n0.8\nHigh Confidence\nSafe for algorithmic decisions\n\n\nPlacer\n109,375\n1.7\nHigh Confidence\nSafe for algorithmic decisions\n\n\nPlumas\n67,885\n11.4\nLow Confidence\nRequires manual review or additional data\n\n\nRiverside\n84,505\n1.3\nHigh Confidence\nSafe for algorithmic decisions\n\n\nSacramento\n84,010\n1.0\nHigh Confidence\nSafe for algorithmic decisions\n\n\nSan Benito\n104,451\n5.2\nModerate Confidence\nUse with caution - monitor outcomes\n\n\nSan Bernardino\n77,423\n1.0\nHigh Confidence\nSafe for algorithmic decisions\n\n\nSan Diego\n96,974\n1.0\nHigh Confidence\nSafe for algorithmic decisions\n\n\nSan Francisco\n136,689\n1.4\nHigh Confidence\nSafe for algorithmic decisions\n\n\nSan Joaquin\n82,837\n1.7\nHigh Confidence\nSafe for algorithmic decisions\n\n\nSan Luis Obispo\n90,158\n2.6\nHigh Confidence\nSafe for algorithmic decisions\n\n\nSan Mateo\n149,907\n1.7\nHigh Confidence\nSafe for algorithmic decisions\n\n\nSanta Barbara\n92,332\n2.0\nHigh Confidence\nSafe for algorithmic decisions\n\n\nSanta Clara\n153,792\n1.0\nHigh Confidence\nSafe for algorithmic decisions\n\n\nSanta Cruz\n104,409\n3.0\nHigh Confidence\nSafe for algorithmic decisions\n\n\nShasta\n68,347\n3.6\nHigh Confidence\nSafe for algorithmic decisions\n\n\nSierra\n61,108\n15.1\nLow Confidence\nRequires manual review or additional data\n\n\nSiskiyou\n53,898\n4.9\nHigh Confidence\nSafe for algorithmic decisions\n\n\nSolano\n97,037\n1.8\nHigh Confidence\nSafe for algorithmic decisions\n\n\nSonoma\n99,266\n2.0\nHigh Confidence\nSafe for algorithmic decisions\n\n\nStanislaus\n74,872\n1.8\nHigh Confidence\nSafe for algorithmic decisions\n\n\nSutter\n72,654\n4.7\nHigh Confidence\nSafe for algorithmic decisions\n\n\nTehama\n59,029\n7.0\nModerate Confidence\nUse with caution - monitor outcomes\n\n\nTrinity\n47,317\n12.4\nLow Confidence\nRequires manual review or additional data\n\n\nTulare\n64,474\n2.3\nHigh Confidence\nSafe for algorithmic decisions\n\n\nTuolumne\n70,432\n6.7\nModerate Confidence\nUse with caution - monitor outcomes\n\n\nVentura\n102,141\n1.5\nHigh Confidence\nSafe for algorithmic decisions\n\n\nYolo\n85,097\n2.7\nHigh Confidence\nSafe for algorithmic decisions\n\n\nYuba\n66,693\n4.2\nHigh Confidence\nSafe for algorithmic decisions\n\n\n\n\n\nKey Recommendations:\nYour Task: Use your analysis results to provide specific guidance to the department.\n\nhigh_confidence &lt;- decision_framework %&gt;%\n  filter(reliability_category == \"High Confidence\") %&gt;%\n  select(NAME)\n\nmoderate_confidence &lt;- decision_framework %&gt;%\n  filter(reliability_category == \"Moderate Confidence\") %&gt;%\n  select(NAME)\n\nlow_confidence &lt;- decision_framework %&gt;%\n  filter(reliability_category == \"Low Confidence\") %&gt;%\n  select(NAME)\n\nprint(\"High Confidence Counties:\")\n\n[1] \"High Confidence Counties:\"\n\nprint(high_confidence)\n\n# A tibble: 42 × 1\n   NAME        \n   &lt;chr&gt;       \n 1 Alameda     \n 2 Butte       \n 3 Calaveras   \n 4 Contra Costa\n 5 El Dorado   \n 6 Fresno      \n 7 Humboldt    \n 8 Imperial    \n 9 Kern        \n10 Kings       \n# ℹ 32 more rows\n\nprint(\"Moderate Confidence Counties:\")\n\n[1] \"Moderate Confidence Counties:\"\n\nprint(moderate_confidence)\n\n# A tibble: 11 × 1\n   NAME      \n   &lt;chr&gt;     \n 1 Amador    \n 2 Colusa    \n 3 Del Norte \n 4 Glenn     \n 5 Inyo      \n 6 Lassen    \n 7 Mariposa  \n 8 Modoc     \n 9 San Benito\n10 Tehama    \n11 Tuolumne  \n\nprint(\"Low Confidence Counties:\")\n\n[1] \"Low Confidence Counties:\"\n\nprint(low_confidence)\n\n# A tibble: 5 × 1\n  NAME   \n  &lt;chr&gt;  \n1 Alpine \n2 Mono   \n3 Plumas \n4 Sierra \n5 Trinity\n\n\n\nCounties suitable for immediate algorithmic implementation: [The counties that are suitabale for immediate algorithmic implementation are Alameda, Butte, Calaveras, Contra Costa, El Dorado, Fresno, Humboldt, Imperial, Kern, Kings, Lake, Los Angeles, Madera, Marin, Mendocino, Merced, Monterey, Napa, Nevada, Orange, Placer, Riverside, Sacramento, San Bernardino, San Diego, San Francisco, San Joaquin, San Luis Obispo, San Mateo, Santa Barbara, Santa Clara, Santa Cruz, Shasta, Siskiyou, Solano, Sonoma, Stanislaus, Sutter, Tulare, Ventura, Yolo, Yuba. These county are in the high confidence category for the new decision making framework.]\nCounties requiring additional oversight: [Counties need additional oversight are Amador, Colusa, Del Norte, Glenn, Inyo, Lassen, Mariposa, Modoc, San Benito, Tehama, Tuolumne. These county are in the moderate confidence category for the new decision making framework.]\nCounties needing alternative approaches: [Counties that need alternative approaches are Alpine, Mono, Plumas, Sierra, Trinity because they are in the low confidence category for the new decision making framework.]"
  },
  {
    "objectID": "labs/Zhang_Ye_Assignment1.html#questions-for-further-investigation",
    "href": "labs/Zhang_Ye_Assignment1.html#questions-for-further-investigation",
    "title": "Assignment 1: Census Data Quality for Policy Decisions",
    "section": "Questions for Further Investigation",
    "text": "Questions for Further Investigation\n[- How has the reliability of ACS estimates changed over time for small counties like Trinity? - Do neighboring census tracts with high MOEs cluster together spatially?]"
  },
  {
    "objectID": "labs/Zhang_Ye_Assignment1.html#submission-checklist",
    "href": "labs/Zhang_Ye_Assignment1.html#submission-checklist",
    "title": "Assignment 1: Census Data Quality for Policy Decisions",
    "section": "Submission Checklist",
    "text": "Submission Checklist\nBefore submitting your portfolio link on Canvas:\n\nAll code chunks run without errors\nAll “[Fill this in]” prompts have been completed\nTables are properly formatted and readable\nExecutive summary addresses all four required components\nPortfolio navigation includes this assignment\nCensus API key is properly set\nDocument renders correctly to HTML\n\nRemember: Submit your portfolio URL on Canvas, not the file itself. Your assignment should be accessible at your-portfolio-url/assignments/assignment_1/your_file_name.html"
  },
  {
    "objectID": "weekly-notes/week-09-notes.html",
    "href": "weekly-notes/week-09-notes.html",
    "title": "Week 9 Notes",
    "section": "",
    "text": "##Key Concepts Learned\n\nWe learned about “dirty data” - data derived from or influenced by corrupt, biased, and unlawful practices including fabricated data, systematic bias, and missing data, not just traditional data quality issues\nPredictive policing creates feedback loops where algorithms learn from biased historical policing patterns, then direct police to the same neighborhoods, generating more arrests that “confirm” the algorithm’s predictions\nThe key distinction is that crime data doesn’t reveal actual crime - it reveals policing patterns, because crime data is always socially constructed, selectively enforced, and organizationally filtered\n\n##Coding Techniques\n\nCount-based crime models use Poisson regression initially, but we check for overdispersion using dispersion statistics and switch to Negative Binomial regression when variance exceeds the mean\nKernel Density Estimation (KDE) with density.ppp(sigma = 1000) creates a baseline prediction using only past crime locations, which we compare against feature-based models to assess whether complexity adds value\nThe validation workflow is: train on 2017 data → create risk predictions → test on 2018 hold-out data → calculate hit rates by risk quintile → compare model performance to KDE baseline\n\n##Questions & Challenges\n\nThe biggest challenge is the impossibility of “neutral” crime data - even if we exclude racially biased drug arrest data, aren’t property crime and assault enforcement also biased? Where do we draw the line?\nAnother challenge is distinguishing between “cleaning” data (removing technical errors) versus addressing fundamental problems where the data itself reflects systemic injustice and cannot be “fixed”\nUnderstanding when a statistically “good” model (that beats KDE and performs well on hold-out data) can still be socially harmful requires considering who benefits, who is harmed, and what feedback loops are created\n\n##Connections to Policy\n\nCase studies from Baltimore, NYPD, and other departments revealed extensive stat manipulation including 14,000+ serious assaults misrecorded as minor offenses and officers planting evidence to meet arrest quotas\nPredictive policing systems exclude white-collar crime, wage theft ($300B+ annually), and corporate fraud from predictions, focusing enforcement only on street crime despite its much lower economic impact\nConsent decrees attempt to address police misconduct but don’t prevent historically biased data from training current algorithms, meaning past injustice becomes embedded in “objective” predictions\n\n##Reflection\n\nThe most important lesson is asking “SHOULD we build this?” before “HOW do we build this?” - technical accuracy doesn’t equal social justice, and a statistically superior model can still be ethically problematic\nSame technical methods could be redirected toward predictive models for justice (predicting eviction risk, health crises, food insecurity) where predictions lead to help instead of punishment"
  },
  {
    "objectID": "weekly-notes/week-06-notes.html",
    "href": "weekly-notes/week-06-notes.html",
    "title": "Week 6 Notes",
    "section": "",
    "text": "###Key Concepts Learned\n\nWe learned how to expand regression models beyond simple continuous variables by incorporating categorical variables (like neighborhoods), interaction terms, and polynomial terms to capture complex relationships\nThe hedonic model framework was introduced, showing how housing prices are determined by structural features, spatial characteristics, and neighborhood effects bundled together\nWe explored Tobler’s First Law and spatial autocorrelation, understanding that nearby houses influence each other’s values, which requires us to add spatial features like buffer aggregations, k-nearest neighbors, and distance to amenities\n\n###Coding Techniques\n\nst_as_sf(coords = c(“Longitude”, “Latitude”), crs = 4326) converts dataframe to spatial object, and st_transform(‘ESRI:102286’) projects to appropriate coordinate system for distance calculations\nst_join(nhoods, join = st_intersects) performs spatial join to assign each house to its neighborhood polygon\nadd_count(name) followed by mutate(name_cv = if_else(n &lt; 10, “Small_Neighborhoods”, as.character(name))) groups sparse categories to prevent cross-validation errors\nst_buffer(dist = 500) creates circular buffers around points for spatial aggregation, and st_nn(k = 3) finds k-nearest neighbors for spatial lag calculations\ntrain(SalePrice ~ LivingArea + as.factor(name_cv), method = “lm”, trControl = trainControl(method = “cv”, number = 10)) runs 10-fold cross-validation with categorical fixed effects\n\n###Questions & Challenges\n\nThe biggest challenge is handling sparse categories in cross-validation - when neighborhoods have fewer than 10 observations, random splits can put all instances in one fold, causing the model to fail on unseen categories\nCreating meaningful spatial features requires domain knowledge about what distances matter (500ft vs 1 mile buffers for crime) and which amenities actually affect prices\nDeciding between grouping small neighborhoods into “Other” versus dropping them entirely involves trade-offs between model stability and potentially excluding marginalized communities from analysis\n\n###Connections to Policy\n\nFixed effects models can reveal systematic price disparities across neighborhoods that aren’t explained by structural features alone, potentially indicating discrimination or historical disinvestment patterns\nUnderstanding which spatial features (crime, transit access, parks) have the strongest effects on property values helps prioritize infrastructure investments and target interventions where they’ll have maximum impact\nThe sparse category problem highlights data equity issues - neighborhoods with few sales are often lower-income areas, and excluding them from analysis risks making policy recommendations that ignore communities most in need\n\nReflection\n\nThe progression from structural-only models (R² = 0.13) to models with spatial features and fixed effects (much lower RMSE) demonstrates how critical location is to housing values beyond just physical characteristics\nI need to always check category counts before running cross-validation with categorical variables to avoid the “factor has new levels” error\nThe insight that fixed effects bundle many unmeasured factors (schools, prestige, amenities) explains why they often provide the biggest prediction improvement, even though they’re less interpretable than specific spatial features"
  },
  {
    "objectID": "weekly-notes/week-04-notes.html",
    "href": "weekly-notes/week-04-notes.html",
    "title": "Week 4 Notes",
    "section": "",
    "text": "We learned about spatial data in R. We talked about vector data model and different type of spatial data to use in R. We also talked about some spatial data operation of how to filter and select the data that we need. We also mentioned some corrdinate reference system and how each is created."
  },
  {
    "objectID": "weekly-notes/week-04-notes.html#key-concepts-learned",
    "href": "weekly-notes/week-04-notes.html#key-concepts-learned",
    "title": "Week 4 Notes",
    "section": "",
    "text": "We learned about spatial data in R. We talked about vector data model and different type of spatial data to use in R. We also talked about some spatial data operation of how to filter and select the data that we need. We also mentioned some corrdinate reference system and how each is created."
  },
  {
    "objectID": "weekly-notes/week-04-notes.html#coding-techniques",
    "href": "weekly-notes/week-04-notes.html#coding-techniques",
    "title": "Week 4 Notes",
    "section": "Coding Techniques",
    "text": "Coding Techniques\n\nBelow are some of the core techniques that we covered in class and used in class. | st_filter() | Select features by spatial relationship | Find neighboring counties | | st_buffer() | Create zones around features | Hospital service areas | | st_intersects() | Test spatial overlap | Check access to services | | st_disjoint() | Test spatial separation | Find rural areas | | st_join() | Join by location | Add county info to tracts | | st_union() | Combine geometries | Merge overlapping buffers | | st_intersection() | Clip geometries | Calculate overlap areas | | st_transform() | Change CRS | Accurate distance/area calculations | | st_area() | Calculate areas | County sizes, coverage | | st_distance() | Calculate distances | Distance to facilities |"
  },
  {
    "objectID": "weekly-notes/week-04-notes.html#questions-challenges",
    "href": "weekly-notes/week-04-notes.html#questions-challenges",
    "title": "Week 4 Notes",
    "section": "Questions & Challenges",
    "text": "Questions & Challenges\n\nI think it is very important to understand what kind of spatial relationship you want to explore. That determines what types of function you need to explore relationships. ‘st_filter()’ and ‘st_intersection()’ might seem confusing at first but they are serving differe kinds of operation."
  },
  {
    "objectID": "weekly-notes/week-04-notes.html#connections-to-policy",
    "href": "weekly-notes/week-04-notes.html#connections-to-policy",
    "title": "Week 4 Notes",
    "section": "Connections to Policy",
    "text": "Connections to Policy\n\nThese operations are heavily related to Policy. Policy decision need spatial analysis to fully understand the root issues."
  },
  {
    "objectID": "weekly-notes/week-04-notes.html#reflection",
    "href": "weekly-notes/week-04-notes.html#reflection",
    "title": "Week 4 Notes",
    "section": "Reflection",
    "text": "Reflection\n\nBeing able to master the spatial analysis and GIS in R will unlock great potential in coming up with policy decision and more in-depth analysis for urban issues."
  },
  {
    "objectID": "weekly-notes/week-02-notes.html",
    "href": "weekly-notes/week-02-notes.html",
    "title": "Week 2 Notes",
    "section": "",
    "text": "I learned about topics on algorithmic decision making and what is census data.\nWe also went through a scenarios practices where we designed the ethical algorithms.\nThe importance of having algorithmic decision making.\nCensus data is really large but it has a lot of potential to do some great things."
  },
  {
    "objectID": "weekly-notes/week-02-notes.html#key-concepts-learned",
    "href": "weekly-notes/week-02-notes.html#key-concepts-learned",
    "title": "Week 2 Notes",
    "section": "",
    "text": "I learned about topics on algorithmic decision making and what is census data.\nWe also went through a scenarios practices where we designed the ethical algorithms.\nThe importance of having algorithmic decision making.\nCensus data is really large but it has a lot of potential to do some great things."
  },
  {
    "objectID": "weekly-notes/week-02-notes.html#coding-techniques",
    "href": "weekly-notes/week-02-notes.html#coding-techniques",
    "title": "Week 2 Notes",
    "section": "Coding Techniques",
    "text": "Coding Techniques\n\nFunctions that I learned include “str_remove(), str_extract(), str_replace(), case_when(),"
  },
  {
    "objectID": "weekly-notes/week-02-notes.html#questions-challenges",
    "href": "weekly-notes/week-02-notes.html#questions-challenges",
    "title": "Week 2 Notes",
    "section": "Questions & Challenges",
    "text": "Questions & Challenges\n\nI think it will take some time to fully understand how to best use the Census Tract. I think it will take a lot of trials and error. The more pract the better we will get at it."
  },
  {
    "objectID": "weekly-notes/week-02-notes.html#connections-to-policy",
    "href": "weekly-notes/week-02-notes.html#connections-to-policy",
    "title": "Week 2 Notes",
    "section": "Connections to Policy",
    "text": "Connections to Policy\n\nIt is important to understand why we want to do algorthimic decision making in policy.\nData analytics is a subjective process because it involves human decisions. So we need to know to be best clean the data and understand the data."
  },
  {
    "objectID": "weekly-notes/week-02-notes.html#reflection",
    "href": "weekly-notes/week-02-notes.html#reflection",
    "title": "Week 2 Notes",
    "section": "Reflection",
    "text": "Reflection\n\nLooking forward to explore the census data more and to see what insightful result we could get. Also really looking forward to the data update in the end of this year. To see how status have changed for people."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "MUSA 5080 Portfolio",
    "section": "",
    "text": "This portfolio documents my learning journey in Public Policy Analytics (MUSA 5080).\n\n\nAdvanced spatial analysis and data science for urban planning and public policy.\n\n\n\n\nWeekly Notes: My learning reflections and key concepts\nAssignment: Completed assignments and analyses\nFinal Project: Capstone modeling challenge\n\n\n\n\n\nMy name is Ye and I did my Bachelor in Urban Planning at USC ✌️✌️.\nMy email is yezhang1@upenn.edu\nMy Github user name is yezhanggg\nI think it is a requirement. Also I don’t have any prior experience with R and I think it is powerful tool that would be really helpful for me to know. It has so many potential and implications that I could use in multiple areas and projects.\n\n\n\n\n\nEmail: [yezhang1@design.upenn.edu]\nGitHub: [@yezhanggg]"
  },
  {
    "objectID": "index.html#about-this-course",
    "href": "index.html#about-this-course",
    "title": "MUSA 5080 Portfolio",
    "section": "",
    "text": "Advanced spatial analysis and data science for urban planning and public policy."
  },
  {
    "objectID": "index.html#portfolio-structure",
    "href": "index.html#portfolio-structure",
    "title": "MUSA 5080 Portfolio",
    "section": "",
    "text": "Weekly Notes: My learning reflections and key concepts\nAssignment: Completed assignments and analyses\nFinal Project: Capstone modeling challenge"
  },
  {
    "objectID": "index.html#about-me",
    "href": "index.html#about-me",
    "title": "MUSA 5080 Portfolio",
    "section": "",
    "text": "My name is Ye and I did my Bachelor in Urban Planning at USC ✌️✌️.\nMy email is yezhang1@upenn.edu\nMy Github user name is yezhanggg\nI think it is a requirement. Also I don’t have any prior experience with R and I think it is powerful tool that would be really helpful for me to know. It has so many potential and implications that I could use in multiple areas and projects."
  },
  {
    "objectID": "index.html#contact",
    "href": "index.html#contact",
    "title": "MUSA 5080 Portfolio",
    "section": "",
    "text": "Email: [yezhang1@design.upenn.edu]\nGitHub: [@yezhanggg]"
  },
  {
    "objectID": "weekly-notes/week-01-notes.html",
    "href": "weekly-notes/week-01-notes.html",
    "title": "Week 1 Notes - Course Introduction",
    "section": "",
    "text": "What is Git, Github, Quarto, and full term for YAML (YAML Ain’t Markup Language).\nSome basic R coding language and went over an example on CAR data."
  },
  {
    "objectID": "weekly-notes/week-01-notes.html#key-concepts-learned",
    "href": "weekly-notes/week-01-notes.html#key-concepts-learned",
    "title": "Week 1 Notes - Course Introduction",
    "section": "",
    "text": "What is Git, Github, Quarto, and full term for YAML (YAML Ain’t Markup Language).\nSome basic R coding language and went over an example on CAR data."
  },
  {
    "objectID": "weekly-notes/week-01-notes.html#coding-techniques",
    "href": "weekly-notes/week-01-notes.html#coding-techniques",
    "title": "Week 1 Notes - Course Introduction",
    "section": "Coding Techniques",
    "text": "Coding Techniques\n\nBold text\nItalic text\nBold and italic\ncode text\nStrikethrough\nselect() - choose columns\nfilter() - choose rows\nmutate() - create new variables\nsummarize() - calculate statistics\ngroup_by() - operate on groups]\ntitle: “My Analysis” author: “Your Name” date: today format: html"
  },
  {
    "objectID": "weekly-notes/week-01-notes.html#questions-challenges",
    "href": "weekly-notes/week-01-notes.html#questions-challenges",
    "title": "Week 1 Notes - Course Introduction",
    "section": "Questions & Challenges",
    "text": "Questions & Challenges\n\nI think it is hard to understand all the R code and notations.\nThis is my first time interact with R and Github. So I need to try everything first and I will find more challenges."
  },
  {
    "objectID": "weekly-notes/week-01-notes.html#connections-to-policy",
    "href": "weekly-notes/week-01-notes.html#connections-to-policy",
    "title": "Week 1 Notes - Course Introduction",
    "section": "Connections to Policy",
    "text": "Connections to Policy\n\nBoth the analysis and the data itself can be biased. The reason is that data were collected by someone who might be biased."
  },
  {
    "objectID": "weekly-notes/week-01-notes.html#reflection",
    "href": "weekly-notes/week-01-notes.html#reflection",
    "title": "Week 1 Notes - Course Introduction",
    "section": "Reflection",
    "text": "Reflection\n\nQuarto is really interesting. Looking forward to see more what Quarto can do.\nThrough our assignment. I think it will be a great practice for me."
  },
  {
    "objectID": "weekly-notes/week-03-notes.html",
    "href": "weekly-notes/week-03-notes.html",
    "title": "Week 3 Notes",
    "section": "",
    "text": "We learn some ways to do visualizations and the asethetic mapping. What is a ggplot. We also learned how to join data."
  },
  {
    "objectID": "weekly-notes/week-03-notes.html#key-concepts-learned",
    "href": "weekly-notes/week-03-notes.html#key-concepts-learned",
    "title": "Week 3 Notes",
    "section": "",
    "text": "We learn some ways to do visualizations and the asethetic mapping. What is a ggplot. We also learned how to join data."
  },
  {
    "objectID": "weekly-notes/week-03-notes.html#coding-techniques",
    "href": "weekly-notes/week-03-notes.html#coding-techniques",
    "title": "Week 3 Notes",
    "section": "Coding Techniques",
    "text": "Coding Techniques\n\nIn class, we learned the basics of ggplot2 and joins in dplyr. In ggplot2, aesthetic mappings like x, y, color, fill, size, shape, and alpha control visual elements such as position, color, and transparency. A typical plot follows the structure ggplot(data) + aes(x, y) + geom_something() + additional_layers(), and layers are added with +. We also covered data joins in dplyr: left_join() keeps all rows from the left dataset, right_join() keeps all from the right, inner_join() keeps only matching rows, and full_join() keeps all rows from both, with left_join() being the most common for adding columns to a main dataset."
  },
  {
    "objectID": "weekly-notes/week-03-notes.html#questions-challenges",
    "href": "weekly-notes/week-03-notes.html#questions-challenges",
    "title": "Week 3 Notes",
    "section": "Questions & Challenges",
    "text": "Questions & Challenges\n\nI think it is very important to follow all the steps correct to make everything work."
  },
  {
    "objectID": "weekly-notes/week-03-notes.html#connections-to-policy",
    "href": "weekly-notes/week-03-notes.html#connections-to-policy",
    "title": "Week 3 Notes",
    "section": "Connections to Policy",
    "text": "Connections to Policy\n\nLearning ggplot2 and dplyr helps us turn data into clear visuals and summaries that make policy trends easier to understand. This makes it simpler to spot patterns and share insights that can guide better, data-informed decisions."
  },
  {
    "objectID": "weekly-notes/week-03-notes.html#reflection",
    "href": "weekly-notes/week-03-notes.html#reflection",
    "title": "Week 3 Notes",
    "section": "Reflection",
    "text": "Reflection\n\nTo become more proficient in this process, it requires a lot of time. Even following the class excercise could be challenging. More practice!"
  },
  {
    "objectID": "weekly-notes/week-05-notes.html",
    "href": "weekly-notes/week-05-notes.html",
    "title": "Week 5 Notes",
    "section": "",
    "text": "We learned about linear regression and different estimator and their purposes. We talked about how to differentiate inference and prediction. We also built our first model to test statistical significance. We talked about the case of overfitting and how test what is considered as a good model."
  },
  {
    "objectID": "weekly-notes/week-05-notes.html#key-concepts-learned",
    "href": "weekly-notes/week-05-notes.html#key-concepts-learned",
    "title": "Week 5 Notes",
    "section": "",
    "text": "We learned about linear regression and different estimator and their purposes. We talked about how to differentiate inference and prediction. We also built our first model to test statistical significance. We talked about the case of overfitting and how test what is considered as a good model."
  },
  {
    "objectID": "weekly-notes/week-05-notes.html#coding-techniques",
    "href": "weekly-notes/week-05-notes.html#coding-techniques",
    "title": "Week 5 Notes",
    "section": "Coding Techniques",
    "text": "Coding Techniques\n\nBelow are some of the core techniques that we covered in class and used in class.\nlm(formula = attribute ~ attribute, data = pa_data) is how to built a model.\nsummary of the model will provide all the important statistics.\nset.seed(123) is to set random number generator\nsqrt(mean is to find RMSE\ntrain_control &lt;- trainControl(method = “cv”, number = 10) sets up the cross-validation technique"
  },
  {
    "objectID": "weekly-notes/week-05-notes.html#questions-challenges",
    "href": "weekly-notes/week-05-notes.html#questions-challenges",
    "title": "Week 5 Notes",
    "section": "Questions & Challenges",
    "text": "Questions & Challenges\n\nI think the hard question is how to find the best predictor for you model while also taking the consideration of different linear model assumption and the biases that we want to avoid. The in-class activity of finding home value was quite challeneging by finding the best predictors that are significant."
  },
  {
    "objectID": "weekly-notes/week-05-notes.html#connections-to-policy",
    "href": "weekly-notes/week-05-notes.html#connections-to-policy",
    "title": "Week 5 Notes",
    "section": "Connections to Policy",
    "text": "Connections to Policy\n\nThis is super useful when we trying to find the correlation or coenction between different inputs and results especially when we are designing public policy."
  },
  {
    "objectID": "weekly-notes/week-05-notes.html#reflection",
    "href": "weekly-notes/week-05-notes.html#reflection",
    "title": "Week 5 Notes",
    "section": "Reflection",
    "text": "Reflection\n\nIt is importan to remember the difference between inference and prediction. We need to carefully look at the model to prevent in-sample fit and overfitting. It is important to check assumption and plot things out to see."
  },
  {
    "objectID": "weekly-notes/week-07-notes.html",
    "href": "weekly-notes/week-07-notes.html",
    "title": "Week 7 Notes",
    "section": "",
    "text": "##Key Concepts Learned\n\nWe learned about spatial autocorrelation and Tobler’s First Law of Geography - near things are more related than distant things\nMoran’s I is a diagnostic tool that detects spatial patterns in model errors, revealing if we’re missing important spatial relationships\nThe key distinction is that spatial lag/error models are for inference about spillover effects, while spatial feature engineering is for prediction\n\n##Coding Techniques\n\nmoran.mc(errors, weights, nsim = 999) tests for spatial autocorrelation in model residuals using Monte Carlo simulation\nAdding progress = FALSE to get_acs(), get_decennial(), and tigris functions suppresses progress messages in rendered documents\nThe workflow is: fit model → extract errors → create spatial weights → test Moran’s I → add spatial features if needed\n\n##Questions & Challenges\n\nThe biggest challenge is deciding what to do when Moran’s I shows significant spatial autocorrelation - should we add more spatial features at different buffer distances or try neighborhood fixed effects?\nAnother challenge is avoiding the “simultaneity problem” where using neighbor prices to predict prices creates circular logic and data leakage\nFinding the right balance between capturing spatial relationships and avoiding overfitting requires both statistical testing and domain knowledge\n\n##Connections to Policy\n\nSpatially clustered model errors can reveal systematic bias in property assessments, leading to inequitable taxation in certain neighborhoods\nUnderstanding spatial autocorrelation helps policymakers identify spillover effects like how crime or transit investments affect surrounding property values\nThis diagnostic approach ensures predictive models don’t perpetuate existing geographic inequalities in housing markets\n\n##Reflection\n\nMoran’s I is a diagnostic tool, not a solution - it tells us our model needs improvement but not exactly how to fix it\nThe iterative process of checking diagnostics, adding spatial features, and re-testing is essential rather than just relying on R-squared\nMoving forward, I’ll always map residuals and test for spatial patterns as part of my standard model evaluation workflow"
  },
  {
    "objectID": "labs/Zhang_Ye_Assignment2.html",
    "href": "labs/Zhang_Ye_Assignment2.html",
    "title": "Assignment 2: Spatial Analysis and Visualization",
    "section": "",
    "text": "Learning Objectives: - Apply spatial operations to answer policy-relevant research questions - Integrate census demographic data with spatial analysis - Create publication-quality visualizations and maps - Work with spatial data from multiple sources - Communicate findings effectively for policy audiences"
  },
  {
    "objectID": "labs/Zhang_Ye_Assignment2.html#assignment-overview",
    "href": "labs/Zhang_Ye_Assignment2.html#assignment-overview",
    "title": "Assignment 2: Spatial Analysis and Visualization",
    "section": "",
    "text": "Learning Objectives: - Apply spatial operations to answer policy-relevant research questions - Integrate census demographic data with spatial analysis - Create publication-quality visualizations and maps - Work with spatial data from multiple sources - Communicate findings effectively for policy audiences"
  },
  {
    "objectID": "labs/Zhang_Ye_Assignment2.html#part-1-healthcare-access-for-vulnerable-populations",
    "href": "labs/Zhang_Ye_Assignment2.html#part-1-healthcare-access-for-vulnerable-populations",
    "title": "Assignment 2: Spatial Analysis and Visualization",
    "section": "Part 1: Healthcare Access for Vulnerable Populations",
    "text": "Part 1: Healthcare Access for Vulnerable Populations\n\nResearch Question\nWhich Pennsylvania counties have the highest proportion of vulnerable populations (elderly + low-income) living far from hospitals?\n\nStep 1: Data Collection (5 points)\n\n# Load required packages\nlibrary(sf)\nlibrary(tidyverse)\nlibrary(tigris)\nlibrary(tidycensus)\nlibrary(scales)\nlibrary(patchwork)\nlibrary(here)\n\noptions(tigris_use_cache = TRUE)\noptions(tigris_progress = FALSE)\n\ncensus_api_key(\"138db656646183a18ae51e4a7c1e1f5cd64d4b40\")\ninstall = TRUE\n\n# Load spatial data\n\npa_counties &lt;- st_read(here(\"~/Documents/GitHub/portfolio-setup-yezhanggg/labs/assignment_2_data/Pennsylvania_County_Boundaries.shp\"))\n\nReading layer `Pennsylvania_County_Boundaries' from data source \n  `/Users/ye/Documents/GitHub/portfolio-setup-yezhanggg/labs/assignment_2_data/Pennsylvania_County_Boundaries.shp' \n  using driver `ESRI Shapefile'\nSimple feature collection with 67 features and 19 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: -8963377 ymin: 4825316 xmax: -8314404 ymax: 5201413\nProjected CRS: WGS 84 / Pseudo-Mercator\n\nhospitals &lt;- st_read(here(\"~/Documents/GitHub/portfolio-setup-yezhanggg/labs/assignment_2_data/hospitals.geojson\"))\n\nReading layer `hospitals' from data source \n  `/Users/ye/Documents/GitHub/portfolio-setup-yezhanggg/labs/assignment_2_data/hospitals.geojson' \n  using driver `GeoJSON'\nSimple feature collection with 223 features and 11 fields\nGeometry type: POINT\nDimension:     XY\nBounding box:  xmin: -80.49621 ymin: 39.75163 xmax: -74.86704 ymax: 42.13403\nGeodetic CRS:  WGS 84\n\ncensus_tracts &lt;- tracts(state = \"PA\", cb = TRUE)\n\n# Check that all data loaded correctly\n\nst_crs(pa_counties)\n\nCoordinate Reference System:\n  User input: WGS 84 / Pseudo-Mercator \n  wkt:\nPROJCRS[\"WGS 84 / Pseudo-Mercator\",\n    BASEGEOGCRS[\"WGS 84\",\n        ENSEMBLE[\"World Geodetic System 1984 ensemble\",\n            MEMBER[\"World Geodetic System 1984 (Transit)\"],\n            MEMBER[\"World Geodetic System 1984 (G730)\"],\n            MEMBER[\"World Geodetic System 1984 (G873)\"],\n            MEMBER[\"World Geodetic System 1984 (G1150)\"],\n            MEMBER[\"World Geodetic System 1984 (G1674)\"],\n            MEMBER[\"World Geodetic System 1984 (G1762)\"],\n            MEMBER[\"World Geodetic System 1984 (G2139)\"],\n            MEMBER[\"World Geodetic System 1984 (G2296)\"],\n            ELLIPSOID[\"WGS 84\",6378137,298.257223563,\n                LENGTHUNIT[\"metre\",1]],\n            ENSEMBLEACCURACY[2.0]],\n        PRIMEM[\"Greenwich\",0,\n            ANGLEUNIT[\"degree\",0.0174532925199433]],\n        ID[\"EPSG\",4326]],\n    CONVERSION[\"Popular Visualisation Pseudo-Mercator\",\n        METHOD[\"Popular Visualisation Pseudo Mercator\",\n            ID[\"EPSG\",1024]],\n        PARAMETER[\"Latitude of natural origin\",0,\n            ANGLEUNIT[\"degree\",0.0174532925199433],\n            ID[\"EPSG\",8801]],\n        PARAMETER[\"Longitude of natural origin\",0,\n            ANGLEUNIT[\"degree\",0.0174532925199433],\n            ID[\"EPSG\",8802]],\n        PARAMETER[\"False easting\",0,\n            LENGTHUNIT[\"metre\",1],\n            ID[\"EPSG\",8806]],\n        PARAMETER[\"False northing\",0,\n            LENGTHUNIT[\"metre\",1],\n            ID[\"EPSG\",8807]]],\n    CS[Cartesian,2],\n        AXIS[\"easting (X)\",east,\n            ORDER[1],\n            LENGTHUNIT[\"metre\",1]],\n        AXIS[\"northing (Y)\",north,\n            ORDER[2],\n            LENGTHUNIT[\"metre\",1]],\n    USAGE[\n        SCOPE[\"Web mapping and visualisation.\"],\n        AREA[\"World between 85.06°S and 85.06°N.\"],\n        BBOX[-85.06,-180,85.06,180]],\n    ID[\"EPSG\",3857]]\n\nst_crs(hospitals)\n\nCoordinate Reference System:\n  User input: WGS 84 \n  wkt:\nGEOGCRS[\"WGS 84\",\n    DATUM[\"World Geodetic System 1984\",\n        ELLIPSOID[\"WGS 84\",6378137,298.257223563,\n            LENGTHUNIT[\"metre\",1]]],\n    PRIMEM[\"Greenwich\",0,\n        ANGLEUNIT[\"degree\",0.0174532925199433]],\n    CS[ellipsoidal,2],\n        AXIS[\"geodetic latitude (Lat)\",north,\n            ORDER[1],\n            ANGLEUNIT[\"degree\",0.0174532925199433]],\n        AXIS[\"geodetic longitude (Lon)\",east,\n            ORDER[2],\n            ANGLEUNIT[\"degree\",0.0174532925199433]],\n    ID[\"EPSG\",4326]]\n\nst_crs(census_tracts)\n\nCoordinate Reference System:\n  User input: NAD83 \n  wkt:\nGEOGCRS[\"NAD83\",\n    DATUM[\"North American Datum 1983\",\n        ELLIPSOID[\"GRS 1980\",6378137,298.257222101,\n            LENGTHUNIT[\"metre\",1]]],\n    PRIMEM[\"Greenwich\",0,\n        ANGLEUNIT[\"degree\",0.0174532925199433]],\n    CS[ellipsoidal,2],\n        AXIS[\"latitude\",north,\n            ORDER[1],\n            ANGLEUNIT[\"degree\",0.0174532925199433]],\n        AXIS[\"longitude\",east,\n            ORDER[2],\n            ANGLEUNIT[\"degree\",0.0174532925199433]],\n    ID[\"EPSG\",4269]]\n\n\nQuestions to answer: - How many hospitals are in your dataset? There aer 223 hosptials in my dataset. - How many census tracts? There are 3445 census tracts. - What coordinate reference system is each dataset in? PA Counties is in WGS 84 Pseudo Mercator, Hospitals is in WGS 84 and Census Tracts is in NAD83.\n\n\n\nStep 2: Get Demographic Data\n\n# Get demographic data from ACS\nacs_vars_2022 &lt;- load_variables(2022, \"acs5\", cache = TRUE)\n\n# Get demographic data from ACS\n\npa_demographics &lt;- get_acs(\n  geography = \"tract\",\n  state = \"PA\",\n  variables = c(\n    total_pop = \"B01003_001\",     \n    median_income = \"B19013_001\",   \n    male_65_66 = \"B01001_020\",\n    male_67_69 = \"B01001_021\",\n    male_70_74 = \"B01001_022\",\n    male_75_79 = \"B01001_023\",\n    male_80_84 = \"B01001_024\",\n    male_85_over = \"B01001_025\",\n    female_65_66 = \"B01001_044\",\n    female_67_69 = \"B01001_045\",\n    female_70_74 = \"B01001_046\",\n    female_75_79 = \"B01001_047\",\n    female_80_84 = \"B01001_048\",\n    female_85_over = \"B01001_049\"\n  ),\n  year = 2022,\n  survey = \"acs5\",\n  output = \"wide\",\n  geometry = FALSE\n)\n\npa_demographics &lt;- pa_demographics %&gt;%\n  mutate(\n    pop_65_over = male_65_66E + male_67_69E + male_70_74E + male_75_79E + \n                  male_80_84E + male_85_overE + \n                  female_65_66E + female_67_69E + female_70_74E + \n                  female_75_79E + female_80_84E + female_85_overE,\n    total_pop = total_popE,\n    median_income = median_incomeE\n  ) %&gt;%\n select(GEOID, total_pop, median_income, pop_65_over)\n\n# Join to tract boundaries\n\ncensus_tracts &lt;- census_tracts %&gt;%\n  left_join(pa_demographics, by = \"GEOID\")\n\nQuestions to answer: - What year of ACS data are you using? I am using ACS 2022 data. - How many tracts have missing income data? There are 62 tracts missing income data. - What is the median income across all PA census tracts? The median household income is $70,188 for all PA Census Tracts.\n\n\n\nStep 3: Define Vulnerable Populations\n\n# Filter for vulnerable tracts based on your criteria\n\nvulnerable_tracts &lt;- census_tracts %&gt;%\n  mutate(pct_elderly = (pop_65_over / total_pop) * 100) %&gt;%\n  filter(median_income &lt; 40000 & pct_elderly &gt; 21)\n\nQuestions to answer: - What income threshold did you choose and why? The income threshold that I chose is 40000. Through research, the poverty guideline in 2024 for a family of four is 31,200.The median household income in PA is $76,081 in 2023. Older population might be more vulnerable and it would be reasonable to adjust threshold higher. It would be reasonable to use this as the guideline to set for the low median household income threshold. - What elderly population threshold did you choose and why? The elderly population threhold that I choose is 21%. Because after reviewing the acs data, there are 14 different age groups and there are three age groups that include age over 65. - How many tracts meet your vulnerability criteria? There are 59 vulnerable tracts that meet my standard of below 40,000 income and over 21% are elderly people. - What percentage of PA census tracts are considered vulnerable by your definition? 1.7% of the PA census tracts are considered as vulnerable by my definition.\n\n\n\nStep 4: Calculate Distance to Hospitals\n\n# Transform to appropriate projected CRS\npa_counties &lt;- pa_counties %&gt;% st_transform(3365)\nhospitals &lt;- hospitals %&gt;% st_transform(3365)\ncensus_tracts &lt;- census_tracts %&gt;% st_transform(3365)\nvulnerable_tracts &lt;- vulnerable_tracts %&gt;% st_transform(3365)\n\n# Calculate distance from each tract centroid to nearest hospital\ntract_centers &lt;- st_centroid(vulnerable_tracts) \ndistance_matrix &lt;- st_distance(tract_centers, hospitals)\n\nmin_distances &lt;- apply(distance_matrix, 1, min)\n\nvulnerable_tracts &lt;- vulnerable_tracts %&gt;%\n  mutate(\n    distance_to_nearest_hospital_m = as.numeric(min_distances),\n    distance_to_nearest_hospital_miles = distance_to_nearest_hospital_m / 1609.34\n  )\n\nQuestions to answer: - What is the average distance to the nearest hospital for vulnerable tracts? The average distance is 7.4093 miles. - What is the maximum distance? The max distance is 61.2651 miles - How many vulnerable tracts are more than 15 miles from the nearest hospital? Based on my standards. There are 4 vulnerable tracts that are more than 15 miles from the nearest hospital.\n\nst_crs(vulnerable_tracts)$units\n\n[1] \"us-ft\"\n\nsummary(vulnerable_tracts)\n\n   STATEFP            COUNTYFP           TRACTCE            GEOIDFQ         \n Length:59          Length:59          Length:59          Length:59         \n Class :character   Class :character   Class :character   Class :character  \n Mode  :character   Mode  :character   Mode  :character   Mode  :character  \n                                                                            \n                                                                            \n                                                                            \n    GEOID               NAME             NAMELSAD            STUSPS         \n Length:59          Length:59          Length:59          Length:59         \n Class :character   Class :character   Class :character   Class :character  \n Mode  :character   Mode  :character   Mode  :character   Mode  :character  \n                                                                            \n                                                                            \n                                                                            \n  NAMELSADCO         STATE_NAME            LSAD               ALAND        \n Length:59          Length:59          Length:59          Min.   : 262958  \n Class :character   Class :character   Class :character   1st Qu.: 692938  \n Mode  :character   Mode  :character   Mode  :character   Median :1153118  \n                                                          Mean   :1782766  \n                                                          3rd Qu.:1827787  \n                                                          Max.   :9908475  \n     AWATER         total_pop    median_income    pop_65_over    \n Min.   :     0   Min.   :  19   Min.   :12336   Min.   :   9.0  \n 1st Qu.:     0   1st Qu.:1600   1st Qu.:24081   1st Qu.: 458.5  \n Median :     0   Median :2373   Median :33771   Median : 599.0  \n Mean   : 51871   Mean   :2456   Mean   :30485   Mean   : 658.7  \n 3rd Qu.: 36576   3rd Qu.:2922   3rd Qu.:37470   3rd Qu.: 787.0  \n Max.   :476215   Max.   :5197   Max.   :39891   Max.   :1649.0  \n          geometry   pct_elderly    distance_to_nearest_hospital_m\n MULTIPOLYGON :59   Min.   :21.04   Min.   :  312.1               \n epsg:3365    : 0   1st Qu.:22.80   1st Qu.: 4124.4               \n +proj=lcc ...: 0   Median :25.50   Median : 7340.2               \n                    Mean   :27.47   Mean   :11924.0               \n                    3rd Qu.:31.17   3rd Qu.:14265.6               \n                    Max.   :47.37   Max.   :98596.3               \n distance_to_nearest_hospital_miles\n Min.   : 0.1939                   \n 1st Qu.: 2.5628                   \n Median : 4.5610                   \n Mean   : 7.4093                   \n 3rd Qu.: 8.8643                   \n Max.   :61.2651                   \n\nsum(vulnerable_tracts$distance_to_nearest_hospital_miles &gt; 15)\n\n[1] 4\n\n\n\n\n\nStep 5: Identify Underserved Areas\n\n# Create underserved variable\n\nvulnerable_tracts &lt;- vulnerable_tracts %&gt;%\n  mutate(underserved = distance_to_nearest_hospital_miles &gt; 15)\n\nQuestions to answer: - How many tracts are underserved? There are 4 tracts that are underserved. - What percentage of vulnerable tracts are underserved? The percentage for vulnerable tracts are underserved is 6.78%. - Does this surprise you? Why or why not? This percentage is little bit lower than I expected. I was expected the vulnerable tracts that are underserved might takes up a larger percentage. It might be how I designed what is considered as a vulnerable tracts.\n\n\n\nStep 6: Aggregate to County Level\n\n# Spatial join tracts to counties\n\ntracts_with_counties &lt;- vulnerable_tracts %&gt;%\n  st_join(pa_counties %&gt;% select(COUNTY_NAM))\n\n# Aggregate statistics by county\n\nvulnerable_by_county &lt;- tracts_with_counties %&gt;%\n  st_drop_geometry() %&gt;%\n  group_by(COUNTY_NAM) %&gt;%\n  summarize(\n    n_vulnerable_tracts = n(),\n    n_underserved_tracts = sum(underserved == TRUE, na.rm = TRUE),\n    pct_vulnerable_underserved = (sum(underserved == TRUE, na.rm = TRUE) / n()) * 100,\n    avg_distance_to_hospital = mean(distance_to_nearest_hospital_miles, na.rm = TRUE),\n    total_vulnerable_pop = sum(total_pop, na.rm = TRUE)\n  ) %&gt;%\n  arrange(desc(n_vulnerable_tracts))\n\n\nvulnerable_by_county %&gt;%\n  arrange(desc(pct_vulnerable_underserved)) %&gt;%\n  head(5)\n\n# A tibble: 5 × 6\n  COUNTY_NAM     n_vulnerable_tracts n_underserved_tracts pct_vulnerable_under…¹\n  &lt;chr&gt;                        &lt;int&gt;                &lt;int&gt;                  &lt;dbl&gt;\n1 CAMERON                          1                    1                 100   \n2 NORTHUMBERLAND                   1                    1                 100   \n3 CAMBRIA                          4                    1                  25   \n4 ALLEGHENY                       18                    1                   5.56\n5 PHILADELPHIA                    10                    0                   0   \n# ℹ abbreviated name: ¹​pct_vulnerable_underserved\n# ℹ 2 more variables: avg_distance_to_hospital &lt;dbl&gt;,\n#   total_vulnerable_pop &lt;dbl&gt;\n\nvulnerable_by_county %&gt;%\n  filter(n_underserved_tracts &gt; 0) %&gt;%\n  arrange(desc(total_vulnerable_pop)) %&gt;%\n  head(10)\n\n# A tibble: 4 × 6\n  COUNTY_NAM     n_vulnerable_tracts n_underserved_tracts pct_vulnerable_under…¹\n  &lt;chr&gt;                        &lt;int&gt;                &lt;int&gt;                  &lt;dbl&gt;\n1 ALLEGHENY                       18                    1                   5.56\n2 CAMBRIA                          4                    1                  25   \n3 NORTHUMBERLAND                   1                    1                 100   \n4 CAMERON                          1                    1                 100   \n# ℹ abbreviated name: ¹​pct_vulnerable_underserved\n# ℹ 2 more variables: avg_distance_to_hospital &lt;dbl&gt;,\n#   total_vulnerable_pop &lt;dbl&gt;\n\n\nQuestions to answer: - Which 5 counties have the highest percentage of underserved vulnerable tracts? Based on my threshold, the top 4 counties with highest percentage of underserved vulnerable tracts. - Which counties have the most vulnerable people living far from hospitals? Allegheny has the most vulnerable people living far from hospitals. - Are there any patterns in where underserved counties are located? Underserved counties tends to also have higher vulnerable tracts and higher average distance to hospital.\n\n\n\nStep 7: Create Summary Table\n\n# Create and format priority counties table\n\nlibrary(knitr)\nlibrary(kableExtra)\n\npriority_counties &lt;- vulnerable_by_county %&gt;%\n  arrange(desc(pct_vulnerable_underserved)) %&gt;%\n  head(10) %&gt;%\n  select(\n    COUNTY_NAM,\n    n_vulnerable_tracts,\n    n_underserved_tracts,\n    pct_vulnerable_underserved,\n    avg_distance_to_hospital,\n    total_vulnerable_pop\n  )\n\npriority_counties %&gt;%\n  kable(\n    col.names = c(\n      \"County\",\n      \"Vulnerable Tracts\",\n      \"Underserved Tracts\",\n      \"% Underserved\",\n      \"Avg Distance (miles)\",\n      \"Vulnerable Population\"\n    ),\n    caption = \"Top 10 Priority Counties for Healthcare Investment: Highest Percentage of Underserved Vulnerable Tracts\",\n    digits = c(0, 0, 0, 1, 1, 0),\n    format.args = list(big.mark = \",\")\n  ) %&gt;%\n  kable_styling(bootstrap_options = c(\"striped\", \"hover\"))\n\n\nTop 10 Priority Counties for Healthcare Investment: Highest Percentage of Underserved Vulnerable Tracts\n\n\nCounty\nVulnerable Tracts\nUnderserved Tracts\n% Underserved\nAvg Distance (miles)\nVulnerable Population\n\n\n\n\nCAMERON\n1\n1\n100.0\n61.3\n1,988\n\n\nNORTHUMBERLAND\n1\n1\n100.0\n31.2\n2,350\n\n\nCAMBRIA\n4\n1\n25.0\n11.6\n8,094\n\n\nALLEGHENY\n18\n1\n5.6\n8.0\n37,378\n\n\nPHILADELPHIA\n10\n0\n0.0\n2.9\n36,700\n\n\nWESTMORELAND\n5\n0\n0.0\n8.1\n8,145\n\n\nERIE\n3\n0\n0.0\n1.4\n5,320\n\n\nLUZERNE\n3\n0\n0.0\n5.9\n8,633\n\n\nMERCER\n3\n0\n0.0\n4.6\n6,560\n\n\nBEAVER\n2\n0\n0.0\n13.8\n3,570"
  },
  {
    "objectID": "labs/Zhang_Ye_Assignment2.html#part-2-comprehensive-visualization",
    "href": "labs/Zhang_Ye_Assignment2.html#part-2-comprehensive-visualization",
    "title": "Assignment 2: Spatial Analysis and Visualization",
    "section": "Part 2: Comprehensive Visualization",
    "text": "Part 2: Comprehensive Visualization\n\nMap 1: County-Level Choropleth\n\n# Create county-level access map\n\nlibrary(scales)\n\ncounties_with_stats &lt;- pa_counties %&gt;%\n  left_join(vulnerable_by_county, by = \"COUNTY_NAM\")\n\nggplot() +\n  geom_sf(data = counties_with_stats, \n          aes(fill = pct_vulnerable_underserved), \n          color = \"white\", \n          size = 0.5) +\n  geom_sf(data = hospitals, \n          color = \"red\", \n          size = 1.5, \n          alpha = 0.6) +\n  scale_fill_viridis_c(\n    name = \"% Vulnerable\\nTracts\\nUnderserved\",\n    labels = comma,\n    option = \"plasma\",\n    na.value = \"lightgray\"\n  ) +\n  labs(\n    title = \"Healthcare Access Challenges in Pennsylvania Counties\",\n    subtitle = \"Percentage of vulnerable tracts located far from hospitals\",\n    caption = \"Source: ACS 2022 | Red points = Hospital locations\"\n  ) +\n  theme_void() +\n  theme(\n    plot.title = element_text(face = \"bold\", size = 14),\n    plot.subtitle = element_text(size = 11),\n    legend.position = \"right\"\n  )\n\n\n\n\n\n\n\n\n\n\n\nMap 2: Detailed Vulnerability Map\n\n# Create detailed tract-level map\n\nallegheny &lt;- pa_counties %&gt;%\n  filter(COUNTY_NAM == \"ALLEGHENY\")\n\nallegheny_tracts &lt;- vulnerable_tracts %&gt;%\n  st_filter(allegheny)\n\nallegheny_hospitals &lt;- hospitals %&gt;%\n  st_filter(allegheny)\n\nggplot() +\n  geom_sf(data = allegheny, \n          fill = \"white\", \n          color = \"black\", \n          size = 1) +\n  geom_sf(data = allegheny_tracts %&gt;% filter(underserved == FALSE),\n          fill = \"blue\", \n          color = \"gray80\",\n          size = 0.3,\n          alpha = 0.6) +\n  geom_sf(data = allegheny_tracts %&gt;% filter(underserved == TRUE),\n          fill = \"red\", \n          color = \"gray80\",\n          size = 0.3,\n          alpha = 0.7) +\n  geom_sf(data = allegheny_hospitals, \n          color = \"lightgreen\", \n          size = 2, \n          shape = 17) +\n  labs(\n    title = \"Underserved Vulnerable Populations in Allegheny County\",\n    subtitle = \"Red = Underserved tracts | Blue = Vulnerable tracts | Triangles = Hospitals\"\n  ) +\n  theme_void()\n\n\n\n\n\n\n\n\n\n\n\nChart: Distribution Analysis of distances to hospitals for vulnerable populations.\n\n# Create distribution visualization\n\nggplot(vulnerable_tracts, aes(x = distance_to_nearest_hospital_miles)) +\n  geom_histogram(bins = 20, fill = \"steelblue\", alpha = 0.7, color = \"white\") +\n  geom_vline(xintercept = 15, color = \"red\", linetype = \"dashed\", size = 1) +\n  labs(\n    title = \"Distribution of Distance to Nearest Hospital\",\n    subtitle = \"For vulnerable census tracts in Pennsylvania\",\n    x = \"Distance to Nearest Hospital (miles)\",\n    y = \"Number of Vulnerable Tracts\",\n    caption = \"Red dashed line = 15-mile threshold for underserved status. \n    Most vulnerable tracts are within 15 miles of a hospital, but a concerning number remain beyond this threshold.\"\n  ) +\n  theme_minimal() +\n  scale_x_continuous(labels = comma)"
  },
  {
    "objectID": "labs/Zhang_Ye_Assignment2.html#part-3-bring-your-own-data-analysis",
    "href": "labs/Zhang_Ye_Assignment2.html#part-3-bring-your-own-data-analysis",
    "title": "Assignment 2: Spatial Analysis and Visualization",
    "section": "Part 3: Bring Your Own Data Analysis",
    "text": "Part 3: Bring Your Own Data Analysis\n\nYour Analysis\nOption B: School Safety Zones - Data: Schools, Crime Incidents, Bike Network - Question: “Are school zones safe for walking/biking, or are they crime hotspots?” - Operations: Buffer schools (500m safety zone), spatial join with crime incidents, assess bike infrastructure coverage - Policy relevance: Safe Routes to School programs, crossing guard placement\nYour Task:\n\nFind and load additional data\n\n\n# Load your additional dataset\n\n\nschools &lt;- st_read(here(\"~/Documents/GitHub/portfolio-setup-yezhanggg/labs/assignment_2_data/Challenege/Schools/Schools.shp\"))\n\nReading layer `Schools' from data source \n  `/Users/ye/Documents/GitHub/portfolio-setup-yezhanggg/labs/assignment_2_data/Challenege/Schools/Schools.shp' \n  using driver `ESRI Shapefile'\nSimple feature collection with 495 features and 14 fields\nGeometry type: POINT\nDimension:     XY\nBounding box:  xmin: -8378628 ymin: 4852555 xmax: -8345686 ymax: 4884813\nProjected CRS: WGS 84 / Pseudo-Mercator\n\ncrime &lt;- st_read(here(\"~/Documents/GitHub/portfolio-setup-yezhanggg/labs/assignment_2_data/Challenege/incidents_part1_part2/incidents_part1_part2.shp\"))\n\nReading layer `incidents_part1_part2' from data source \n  `/Users/ye/Documents/GitHub/portfolio-setup-yezhanggg/labs/assignment_2_data/Challenege/incidents_part1_part2/incidents_part1_part2.shp' \n  using driver `ESRI Shapefile'\nreplacing null geometries with empty geometries\nSimple feature collection with 169017 features and 13 fields (with 6985 geometries empty)\nGeometry type: POINT\nDimension:     XY\nBounding box:  xmin: -75.2753 ymin: 39.87465 xmax: -74.95761 ymax: 40.13762\nGeodetic CRS:  WGS 84\n\nbike &lt;- st_read(here(\"~/Documents/GitHub/portfolio-setup-yezhanggg/labs/assignment_2_data/Challenege/Bike_Network/Bike_Network.shp\"))\n\nReading layer `Bike_Network' from data source \n  `/Users/ye/Documents/GitHub/portfolio-setup-yezhanggg/labs/assignment_2_data/Challenege/Bike_Network/Bike_Network.shp' \n  using driver `ESRI Shapefile'\nSimple feature collection with 5225 features and 8 fields\nGeometry type: LINESTRING\nDimension:     XY\nBounding box:  xmin: -8378937 ymin: 4847835 xmax: -8345146 ymax: 4883978\nProjected CRS: WGS 84 / Pseudo-Mercator\n\n\n\nst_crs(schools)\n\nCoordinate Reference System:\n  User input: WGS 84 / Pseudo-Mercator \n  wkt:\nPROJCRS[\"WGS 84 / Pseudo-Mercator\",\n    BASEGEOGCRS[\"WGS 84\",\n        ENSEMBLE[\"World Geodetic System 1984 ensemble\",\n            MEMBER[\"World Geodetic System 1984 (Transit)\"],\n            MEMBER[\"World Geodetic System 1984 (G730)\"],\n            MEMBER[\"World Geodetic System 1984 (G873)\"],\n            MEMBER[\"World Geodetic System 1984 (G1150)\"],\n            MEMBER[\"World Geodetic System 1984 (G1674)\"],\n            MEMBER[\"World Geodetic System 1984 (G1762)\"],\n            MEMBER[\"World Geodetic System 1984 (G2139)\"],\n            MEMBER[\"World Geodetic System 1984 (G2296)\"],\n            ELLIPSOID[\"WGS 84\",6378137,298.257223563,\n                LENGTHUNIT[\"metre\",1]],\n            ENSEMBLEACCURACY[2.0]],\n        PRIMEM[\"Greenwich\",0,\n            ANGLEUNIT[\"degree\",0.0174532925199433]],\n        ID[\"EPSG\",4326]],\n    CONVERSION[\"Popular Visualisation Pseudo-Mercator\",\n        METHOD[\"Popular Visualisation Pseudo Mercator\",\n            ID[\"EPSG\",1024]],\n        PARAMETER[\"Latitude of natural origin\",0,\n            ANGLEUNIT[\"degree\",0.0174532925199433],\n            ID[\"EPSG\",8801]],\n        PARAMETER[\"Longitude of natural origin\",0,\n            ANGLEUNIT[\"degree\",0.0174532925199433],\n            ID[\"EPSG\",8802]],\n        PARAMETER[\"False easting\",0,\n            LENGTHUNIT[\"metre\",1],\n            ID[\"EPSG\",8806]],\n        PARAMETER[\"False northing\",0,\n            LENGTHUNIT[\"metre\",1],\n            ID[\"EPSG\",8807]]],\n    CS[Cartesian,2],\n        AXIS[\"easting (X)\",east,\n            ORDER[1],\n            LENGTHUNIT[\"metre\",1]],\n        AXIS[\"northing (Y)\",north,\n            ORDER[2],\n            LENGTHUNIT[\"metre\",1]],\n    USAGE[\n        SCOPE[\"Web mapping and visualisation.\"],\n        AREA[\"World between 85.06°S and 85.06°N.\"],\n        BBOX[-85.06,-180,85.06,180]],\n    ID[\"EPSG\",3857]]\n\nst_crs(crime)\n\nCoordinate Reference System:\n  User input: WGS 84 \n  wkt:\nGEOGCRS[\"WGS 84\",\n    DATUM[\"World Geodetic System 1984\",\n        ELLIPSOID[\"WGS 84\",6378137,298.257223563,\n            LENGTHUNIT[\"metre\",1]]],\n    PRIMEM[\"Greenwich\",0,\n        ANGLEUNIT[\"degree\",0.0174532925199433]],\n    CS[ellipsoidal,2],\n        AXIS[\"latitude\",north,\n            ORDER[1],\n            ANGLEUNIT[\"degree\",0.0174532925199433]],\n        AXIS[\"longitude\",east,\n            ORDER[2],\n            ANGLEUNIT[\"degree\",0.0174532925199433]],\n    ID[\"EPSG\",4326]]\n\nst_crs(bike)\n\nCoordinate Reference System:\n  User input: WGS 84 / Pseudo-Mercator \n  wkt:\nPROJCRS[\"WGS 84 / Pseudo-Mercator\",\n    BASEGEOGCRS[\"WGS 84\",\n        ENSEMBLE[\"World Geodetic System 1984 ensemble\",\n            MEMBER[\"World Geodetic System 1984 (Transit)\"],\n            MEMBER[\"World Geodetic System 1984 (G730)\"],\n            MEMBER[\"World Geodetic System 1984 (G873)\"],\n            MEMBER[\"World Geodetic System 1984 (G1150)\"],\n            MEMBER[\"World Geodetic System 1984 (G1674)\"],\n            MEMBER[\"World Geodetic System 1984 (G1762)\"],\n            MEMBER[\"World Geodetic System 1984 (G2139)\"],\n            MEMBER[\"World Geodetic System 1984 (G2296)\"],\n            ELLIPSOID[\"WGS 84\",6378137,298.257223563,\n                LENGTHUNIT[\"metre\",1]],\n            ENSEMBLEACCURACY[2.0]],\n        PRIMEM[\"Greenwich\",0,\n            ANGLEUNIT[\"degree\",0.0174532925199433]],\n        ID[\"EPSG\",4326]],\n    CONVERSION[\"Popular Visualisation Pseudo-Mercator\",\n        METHOD[\"Popular Visualisation Pseudo Mercator\",\n            ID[\"EPSG\",1024]],\n        PARAMETER[\"Latitude of natural origin\",0,\n            ANGLEUNIT[\"degree\",0.0174532925199433],\n            ID[\"EPSG\",8801]],\n        PARAMETER[\"Longitude of natural origin\",0,\n            ANGLEUNIT[\"degree\",0.0174532925199433],\n            ID[\"EPSG\",8802]],\n        PARAMETER[\"False easting\",0,\n            LENGTHUNIT[\"metre\",1],\n            ID[\"EPSG\",8806]],\n        PARAMETER[\"False northing\",0,\n            LENGTHUNIT[\"metre\",1],\n            ID[\"EPSG\",8807]]],\n    CS[Cartesian,2],\n        AXIS[\"easting (X)\",east,\n            ORDER[1],\n            LENGTHUNIT[\"metre\",1]],\n        AXIS[\"northing (Y)\",north,\n            ORDER[2],\n            LENGTHUNIT[\"metre\",1]],\n    USAGE[\n        SCOPE[\"Web mapping and visualisation.\"],\n        AREA[\"World between 85.06°S and 85.06°N.\"],\n        BBOX[-85.06,-180,85.06,180]],\n    ID[\"EPSG\",3857]]\n\nschools &lt;- st_transform(schools, 3365)\ncrime &lt;- st_transform(crime, 3365)\nbike &lt;- st_transform(bike, 3365)\n\nnrow(schools)\n\n[1] 495\n\nnrow(crime)\n\n[1] 169017\n\nnrow(bike)\n\n[1] 5225\n\nplot(st_geometry(schools))\n\n\n\n\n\n\n\nplot(st_geometry(crime))\n\n\n\n\n\n\n\nplot(st_geometry(bike))\n\n\n\n\n\n\n\n\nQuestions to answer: - What dataset did you choose and why? I choose schools, bike network and crime incidents to look school safety topics. - What is the data source and date? All of the data are downloaded from OpenDataPhilly. Crime incidents data is 2025 and bike network and schools don’t have their date included. - How many features does it contain? There are 15 features for schoos, 14 features for crime incidents, and 9 features for bike network. - What CRS is it in? Did you need to transform it? I transformed all of the data into EPSG:3365 for their CRS because it is an optimal option for Philadelphia. —\n\nPose a research question\n\nAre school zones safe for walking/biking, or are they crime hotspots?\n\n\nConduct spatial analysis\n\nUse at least TWO spatial operations to answer your research question.\n\n# Your spatial analysis\n\n# Operation 1: BUFFERS\n\n#find the buffers around school\nschools_buffer &lt;- schools %&gt;%\n  st_buffer(dist = 500)\n\n# Count crime incidents near each school\nschools_with_crime &lt;- schools %&gt;%\n  mutate(\n    crime_within_500m = lengths(st_intersects(schools_buffer, crime))\n  )\n\n# Summary statistics\ncat(\"Crime incidents near schools:\\n\")\n\nCrime incidents near schools:\n\ncat(\"Mean crimes per school:\", mean(schools_with_crime$crime_within_500m), \"\\n\")\n\nMean crimes per school: 60.67677 \n\ncat(\"Max crimes near one school:\", max(schools_with_crime$crime_within_500m), \"\\n\")\n\nMax crimes near one school: 490 \n\ncat(\"Schools with 10+ crimes nearby:\", \n    sum(schools_with_crime$crime_within_500m &gt;= 10), \"\\n\\n\")\n\nSchools with 10+ crimes nearby: 439 \n\ncat(\"% Schools with 10+ crimes nearby:\",\n    (sum(schools_with_crime$crime_within_500m &gt;= 10))/495, \"\\n\\n\")\n\n% Schools with 10+ crimes nearby: 0.8868687 \n\n# Operation 2: SPATIAL JOIN\n\n# Find schools within 200m of bike infrastructure\nschools_near_bikes &lt;- schools %&gt;%\n  st_filter(st_buffer(bike, 200), .predicate = st_intersects) %&gt;%\n  mutate(has_bike_access = TRUE)\n# Calculate percentage\npct_bike_access &lt;- (nrow(schools_near_bikes) / nrow(schools)) * 100\ncat(\"Schools with bike access:\", nrow(schools_near_bikes), \n    \"out of\", nrow(schools), \n    \"(\", round(pct_bike_access, 1), \"%)\")\n\nSchools with bike access: 120 out of 495 ( 24.2 %)\n\n# Create map\nggplot() +\n  geom_sf(data = bike, color = \"green\", size = 0.5, alpha = 1) +\n  geom_sf(data = crime, color = \"red\", size = 0.000000001, alpha = 0.01) +\n  geom_sf(data = schools_buffer, fill = \"yellow\", alpha = 0.8, color = NA) +\n  geom_sf(data = schools_with_crime, \n          aes(color = crime_within_500m), \n          size = 1) +\n  scale_color_viridis_c(\n    name = \"Crimes within 500m\",\n    option = \"plasma\"\n  ) +\n  labs(\n    title = \"School Safety Analysis: Crime Incidents and Bike Access\",\n    subtitle = \"Yellow zones = 500m school buffers | Green lines = Bike network\",\n    caption = \"Red points = Crime incidents\"\n  ) +\n  theme_void()\n\n\n\n\n\n\n\n\nYour interpretation:\nThe mean crime per school is around 60 and the max crime around school is 490 which is super high. There are 495 schools in total and 439 school has crimes over 10+ nearby. 88.7% of all school has crime nearby. 24.2% of the school has access to bike route. With the high percentage of crime present around school is not safe to bike around schools. There might be only a limited number of schools has the level of safety to allow students to bike around schools."
  },
  {
    "objectID": "labs/Zhang_Ye_Assignment2.html#finally---a-few-comments-about-your-incorporation-of-feedback",
    "href": "labs/Zhang_Ye_Assignment2.html#finally---a-few-comments-about-your-incorporation-of-feedback",
    "title": "Assignment 2: Spatial Analysis and Visualization",
    "section": "Finally - A few comments about your incorporation of feedback!",
    "text": "Finally - A few comments about your incorporation of feedback!\nRemove extra instruction text and only leave what is important and present the assignment like a presentation. No need to keep the instructions. Be sure to review everything."
  },
  {
    "objectID": "labs/Zhang_Ye_Appendix.html",
    "href": "labs/Zhang_Ye_Appendix.html",
    "title": "Technical Appendix",
    "section": "",
    "text": "── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.2\n✔ ggplot2   4.0.0     ✔ tibble    3.3.0\n✔ lubridate 1.9.4     ✔ tidyr     1.3.1\n✔ purrr     1.1.0     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nAttaching package: 'scales'\n\n\nThe following object is masked from 'package:purrr':\n\n    discard\n\n\nThe following object is masked from 'package:readr':\n\n    col_factor\n\n\nLinking to GEOS 3.13.0, GDAL 3.8.5, PROJ 9.5.1; sf_use_s2() is TRUE\n\nTo enable caching of data, set `options(tigris_use_cache = TRUE)`\nin your R script or .Rprofile.\n\nhere() starts at /Users/ye/Documents/GitHub/portfolio-setup-yezhanggg\n\nData (c) OpenStreetMap contributors, ODbL 1.0. https://www.openstreetmap.org/copyright.\nCheck the package website, https://docs.ropensci.org/osmextract/, for more details.\n\nLoading required package: spatstat.data\n\nLoading required package: spatstat.univar\n\nspatstat.univar 3.1-4\n\nspatstat.geom 3.6-0\n\n\nAttaching package: 'spatstat.geom'\n\n\nThe following object is masked from 'package:patchwork':\n\n    area\n\n\nThe following object is masked from 'package:scales':\n\n    rescale\n\n\nLoading required package: spatstat.random\n\nspatstat.random 3.4-2\n\nLoading required package: spatstat.explore\n\nLoading required package: nlme\n\n\nAttaching package: 'nlme'\n\n\nThe following object is masked from 'package:dplyr':\n\n    collapse\n\n\nspatstat.explore 3.5-3\n\nLoading required package: spatstat.model\n\nLoading required package: rpart\n\nspatstat.model 3.4-2\n\nLoading required package: spatstat.linnet\n\nspatstat.linnet 3.3-2\n\n\nspatstat 3.4-1 \nFor an introduction to spatstat, type 'beginner' \n\n\nterra 1.8.70\n\n\nAttaching package: 'terra'\n\n\nThe following objects are masked from 'package:spatstat.geom':\n\n    area, delaunay, is.empty, rescale, rotate, shift, where.max,\n    where.min\n\n\nThe following object is masked from 'package:patchwork':\n\n    area\n\n\nThe following object is masked from 'package:tigris':\n\n    blocks\n\n\nThe following object is masked from 'package:scales':\n\n    rescale\n\n\nThe following object is masked from 'package:knitr':\n\n    spin\n\n\nThe following object is masked from 'package:tidyr':\n\n    extract\n\n\n\nAttaching package: 'jsonlite'\n\n\nThe following object is masked from 'package:purrr':\n\n    flatten\n\n\n\nPlease cite as: \n\n\n Hlavac, Marek (2022). stargazer: Well-Formatted Regression and Summary Statistics Tables.\n\n R package version 5.2.3. https://CRAN.R-project.org/package=stargazer \n\n\nLoading required package: carData\n\n\nAttaching package: 'car'\n\n\nThe following object is masked from 'package:spatstat.model':\n\n    bc\n\n\nThe following object is masked from 'package:spatstat.geom':\n\n    ellipse\n\n\nThe following object is masked from 'package:dplyr':\n\n    recode\n\n\nThe following object is masked from 'package:purrr':\n\n    some\n\n\nLoading required package: zoo\n\n\nAttaching package: 'zoo'\n\n\nThe following object is masked from 'package:terra':\n\n    time&lt;-\n\n\nThe following objects are masked from 'package:base':\n\n    as.Date, as.Date.numeric"
  },
  {
    "objectID": "labs/Zhang_Ye_Appendix.html#load-and-philadelphia-house-sales-data",
    "href": "labs/Zhang_Ye_Appendix.html#load-and-philadelphia-house-sales-data",
    "title": "Technical Appendix",
    "section": "1.1 Load and Philadelphia house sales data",
    "text": "1.1 Load and Philadelphia house sales data\n\n\nCode\n# Load Philly Property Sales data\nphl_sales &lt;- read_csv(\"~/Desktop/FW 25/MUSA 5080/opa_properties_public.csv\")\n\n\nWarning: One or more parsing issues, call `problems()` on your data frame for details,\ne.g.:\n  dat &lt;- vroom(...)\n  problems(dat)\n\n\nRows: 583750 Columns: 79\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr  (38): basements, beginning_point, book_and_page, building_code, buildin...\ndbl  (31): objectid, category_code, census_tract, depth, exempt_building, ex...\nlgl   (7): cross_reference, date_exterior_condition, mailing_address_2, mark...\ndttm  (3): assessment_date, recording_date, sale_date\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\n\nFilter to residential properties, 2023-2024 sales\n\n\nCode\n# Check data types\n# glimpse(phl_sales)\n\nphl_sales_res_23_24 &lt;- phl_sales |&gt;\n  filter(\n    category_code == 1, # Residential\n    year(sale_date) %in% c(2023, 2024), # 2023-24 sales\n    !is.na(category_code) & !is.na(sale_date) # Handle nulls\n  )\n\n\n\n\nRemove obvious errors\n\n\nCode\nphl_sales_clean &lt;- phl_sales_res_23_24 |&gt;\n  filter(\n    # Some sale_price are unrealistically too low ($0, $1 etc.)\n    sale_price &gt;= 10000,\n    # Exclude homes with 0 bathrooms\n    number_of_bathrooms &gt; 0,\n    # Some areas are unrealistically low (0, 1, etc.)\n    total_area &gt; 1,\n    # Some 0's remain in total_liveable_area after first area filter\n    total_livable_area &gt; 0,\n    # Filter our unrealistic year built\n    year_built &gt;= 1750\n    ) \n\n\n\n\nHandle missing values\n\n\nCode\n# Check how many features have NA values\n# sum(is.na(phl_sales_clean$number_of_bedrooms))\n# sum(is.na(phl_sales_clean$number_of_bathrooms))\n# sum(is.na(phl_sales_clean$total_livable_area))\n# sum(is.na(phl_sales_clean$year_built))\n\n# Remove the 2 observations with NA values for number of bedrooms\nphl_sales_clean &lt;- phl_sales_clean |&gt;\n  filter(\n    !is.na(number_of_bedrooms)\n  )\n\n\n\n\nPreliminary Feature Engineering: Age = sale date - year built\n\n\nCode\nphl_sales_clean &lt;- phl_sales_clean |&gt;\n  mutate(\n    sale_year = year(sale_date),\n    age = sale_year - year_built\n  )\n\n\n\n\nDocument all cleaning decisions\n\nOur methodology for cleaning the Philadelphia home sales data is to focus on the features used in our model. As a group, we decided on the following independent variables to consider in our data exploration and model building to be: number of bathrooms, number of bedrooms, total livable area, and year built. We recognize that there is some risk of collinearity between these structural features, which will later be monitored and addressed if needed in the model building stage. Additionally, we also had to clean the sales price column since this is the variable we aim to predict in our model.\nFilter for only residential properties & sales made in 2023-24 (per instructions).\nFilter for realistic sales price &gt;= $10,000.\nFilter for houses with at least 1 bathroom. We will keep observations where number of bedrooms = 0 as this likely signifies a studio apartment. However, it is not feasible for homes to have zero bathrooms, so we will enforce a constraint that a home must have at least 1 bathroom to preserve data integrity.\nFilter for realistic total area &gt; 1 sq ft & realistic total livable area &gt; 0 sq ft.\nFilter for year built &gt;= 1750 (some homes were built in year 0).\nHandle missing values: We removed any missing values in our dependent variable of sales price, since it is crucial we have a true and accurate measure for prediction. We also checked which of our predictor variables had NA values after filtering. Only number of bedrooms had 2 remaining NA values. The rest had no NA values. To remedy this, we will remove the 2 observations from our data. Note, if there was substantial missing values in our predictors, we could use strategies such as imputing the NA values with the mean or median to use when building our model.\nPreliminary feature engineering: Rather than using year built in our Automated Valuation Model, it makes more sense to create a new variable age that is equal to the sale date minus the year built. The age variable is often easier to interpret in exploratory plots with the newer houses appearing on the left and older ones on the right. This is primarily a stylistic preference: the overall pattern of the data will remain the same but mirrored."
  },
  {
    "objectID": "labs/Zhang_Ye_Appendix.html#load-secondary-data",
    "href": "labs/Zhang_Ye_Appendix.html#load-secondary-data",
    "title": "Technical Appendix",
    "section": "1.2 Load Secondary Data",
    "text": "1.2 Load Secondary Data\n\nCensus\nPurpose: Pull demographic and housing data at the census tract level for Philadelphia from the 2023 5-year ACS. This data will provide predictors for neighborhood characteristics in our modeling.\nVariables Collected:\nMedian household income (B19013)\nPercentage of family households (B11001)\nEducation attainment: percent of population 25+ with a bachelor’s degree or higher (B15003)\nHousing vacancy rate (B25002)\nRacial composition: percent white (B02001)\n\nLoad Philly Census Data from Previously Retrieved Files\n\n\nCode\n# Relative to project root\ncensus_path &lt;- here(\"data\", \"Philly Census\")\n\ncensus_csv_path &lt;- file.path(census_path, \"philly_tract_metrics.csv\")\ncensus_shp_path &lt;- file.path(census_path, \"philly_tract.shp\")\n\n# Csv with census tract Geo IDs and metrics\nphilly_censustract &lt;- read_csv(census_csv_path, show_col_types = FALSE)\n\n# Shp File including geometry\nphilly_tract_sf &lt;- st_read(census_shp_path, quiet = TRUE)\n\n\n\n\nObserve Summary Statistics from target metrics.x\n\n\nCode\nnumeric_vars &lt;- c(\"median_income\", \"pct_white\", \"pct_bachelors\", \"pct_vacant\")\nphilly_censustract %&gt;%\n  select(all_of(numeric_vars)) %&gt;%\n  summary()\n\n\n median_income      pct_white      pct_bachelors      pct_vacant     \n Min.   : 13721   Min.   : 0.000   Min.   : 1.504   Min.   :  0.000  \n 1st Qu.: 42469   1st Qu.: 8.826   1st Qu.:16.094   1st Qu.:  5.366  \n Median : 60817   Median :34.740   Median :28.426   Median :  8.516  \n Mean   : 66877   Mean   :37.944   Mean   :36.325   Mean   :  9.844  \n 3rd Qu.: 85298   3rd Qu.:64.202   3rd Qu.:55.345   3rd Qu.: 12.801  \n Max.   :192727   Max.   :95.513   Max.   :96.632   Max.   :100.000  \n NA's   :27       NA's   :17       NA's   :17       NA's   :19       \n\n\nA quick check of the census variables reveals some missing values and lower than epected values in median income. We will note this information but retain the missing values for now to maintain the full pitcure of census blocks.\n\n\n\nCleaning Methodology (Census)\nMedian income: Selected only the estimate column and renamed it for clarity.\nHousehold composition: Pivoted ACS table to wide format, then calculated total households and family households.\nEducation: Pivoted to wide format, summed relevant categories to compute percent of population with a bachelor’s degree or higher.\nVacancy: Pivoted to wide format, calculated percent of homes vacant (vacant_units / total_units * 100).\nRacial composition: Pivoted to wide format, computed percent white.\nMerging: Combined all datasets by GEOID to create a single dataframe philly_blockgroup with all variables.\nGeometry: Pulled census tract shapefiles with ACS geometry and merged with philly_blockgroup to create philly_bg_map.\n\n\nNeighborhood (Polygon)\nReading in Philadelphia Neighborhoods as a shp object. This will allow us to aggregate data on neighborhoods to identify catagorical metrics.\n\n\nCode\nneighborhood_folder &lt;- here(\"data\", \"philadelphia-neighborhoods\")\nneighborhood_path   &lt;- file.path(neighborhood_folder, \"philadelphia-neighborhoods.shp\")\n\n# Read the shapefile\nphilly_neighborhoods &lt;- st_read(neighborhood_path, quiet = TRUE)\n\nhead(philly_neighborhoods)\n\n\nSimple feature collection with 6 features and 5 fields\nGeometry type: POLYGON\nDimension:     XY\nBounding box:  xmin: -75.23049 ymin: 39.98491 xmax: -75.0156 ymax: 40.11269\nGeodetic CRS:  WGS 84\n             NAME         LISTNAME         MAPNAME Shape_Leng Shape_Area\n1      BRIDESBURG       Bridesburg      Bridesburg   27814.55   44586264\n2       BUSTLETON        Bustleton       Bustleton   48868.46  114050424\n3      CEDARBROOK       Cedarbrook      Cedarbrook   20021.42   24871745\n4   CHESTNUT_HILL    Chestnut Hill   Chestnut Hill   56394.30   79664975\n5      EAST_FALLS       East Falls      East Falls   27400.78   40576888\n6 MOUNT_AIRY_EAST Mount Airy, East East Mount Airy   28845.55   43152470\n                        geometry\n1 POLYGON ((-75.06773 40.0054...\n2 POLYGON ((-75.0156 40.09487...\n3 POLYGON ((-75.18848 40.0727...\n4 POLYGON ((-75.21221 40.0860...\n5 POLYGON ((-75.18476 40.0282...\n6 POLYGON ((-75.18087 40.0432...\n\n\n\n\nCommercial and office points of interests (Amenities)(alternative in the next section if you do not want to download pbf data)\n\n\nCode\n# downloading osm data from geofabrik:https://download.geofabrik.de/north-america/us-northeast.html\n\n#input_pbf &lt;- \"the pdf file downloaded from the link above\"\n\n\n# get boundary of Philadelphia County\npa_counties &lt;- counties(state = \"PA\", year = 2023)\n\n# Filter to Philadelphia County\nphilly_boundary &lt;- subset(pa_counties, NAME == \"Philadelphia\")\n\n# read the full OSM PBF (you can select layer types like points, lines, polygons)\npoi &lt;- oe_read(input_pbf, \n                       boundary = philly_boundary, \n                       boundary_type = \"clipsrc\", \n                       layer = \"points\")  # or \"lines\" / \"multipolygons\"\n\n\nkeywords &lt;- c(\"shop\",\"amenity\",\"office\",\"historic\",\"tourism\",\"healthcare\",\n              \"building\",\"leisure\")\npattern &lt;- paste0(keywords, collapse = \"|\")\n\n# ==== Filter by 'other_tags' ====\nif (\"other_tags\" %in% names(poi)) {\n  poi$other_tags &lt;- iconv(as.character(poi$other_tags), from = \"\", to = \"UTF-8\", sub = \"\")\n  poi$other_tags[is.na(poi$other_tags)] &lt;- \"\"\n  \n  poi_filtered &lt;- poi %&gt;%\n    filter(grepl(pattern, other_tags, ignore.case = TRUE))\n  \n  cat(\"filtered POIs found:\", nrow(poi_filtered), \"of\", nrow(poi), \"\\n\")\n  \n} \n\n\n\n\nAlternative: filtered POI if you donot want to download osm data\n\n\nCode\npoi_path &lt;- here(\"data\", \"filtered poi\")\npoi_shp_path=file.path(poi_path, \"philadelphia_poi_filtered.shp\")\npoi=st_read(poi_shp_path)\n\n\nReading layer `philadelphia_poi_filtered' from data source \n  `/Users/ye/Documents/GitHub/portfolio-setup-yezhanggg/data/filtered poi/philadelphia_poi_filtered.shp' \n  using driver `ESRI Shapefile'\nSimple feature collection with 11161 features and 10 fields\nGeometry type: POINT\nDimension:     XY\nBounding box:  xmin: -75.27472 ymin: 39.87383 xmax: -74.95777 ymax: 40.13445\nGeodetic CRS:  WGS 84\n\n\n\n\nKernel Density Rasters (Economic activities density)\nInstead of using distance to CBD, we extracted commercial and office points of interests from OpenStreetMap (OSM), and we opreate a Kernel Density Estimation (KDE) with a bandwidth of 300 meters. By doing that, we manage to get a surface of density of economic activities across the whole city. The higher the KDE value is, the more economic activities it will be, implying a higher likelyhood of the area as a city centers.\nThere are several benefits using this approach compared to distance to CBD. First, with the development of suburbanization, even within the context of Philadelphia County, there is still a shift from monocentric model to polycentric model, meaning there multiple centers/subcenters. Using one CBD fail to capture these subcenters, which may also influence housing price. Second, CBD is an area rather than a point, distance method fail to capture this while the continuous surface computed by KDE would have a value of economic activities across the whole city.\n\n\nCode\n# get boundary of Philadelphia County\npa_counties &lt;- counties(state = \"PA\", year = 2023)\n\n# Filter to Philadelphia County\nphilly_boundary &lt;- subset(pa_counties, NAME == \"Philadelphia\")\n\nphilly_boundary &lt;- st_transform(philly_boundary, 3364)  \npoi &lt;- st_transform(poi, 3364)\n# ==== Prepare point pattern ====\n# Convert sf points to spatstat ppp object\nwin &lt;- as.owin(st_union(philly_boundary))  # window from county boundary\ncoords &lt;- st_coordinates(poi)\npp &lt;- ppp(x = coords[,1], y = coords[,2], window = win)\n\n\nWarning: data contain duplicated points\n\n\nCode\n# ==== Run Kernel Density Estimation ====\n# Sigma = bandwidth in map units (here, meters)\ndensity_map &lt;- density.ppp(pp, sigma = 300* 3.28084, edge = TRUE, at = \"pixels\",eps = c(100, 100))\n\n# ==== Convert to raster ====\nr_Economic &lt;- rast(density_map)\ncrs(r_Economic) &lt;- st_crs(philly_boundary)$proj4string\nr_Economic &lt;- mask(r_Economic, vect(philly_boundary))\n\n\n\n\nEducation\nWe used two datasets from OpenDataPhilly.com to identify schools geolocation and populated the metrics off Attendance percent and Withdrawal volumes from those schools.\n\n\nCode\n# Relative to project root\neducation_path &lt;- here(\"data\", \"Education\")\n\neducation_csv_path &lt;- file.path(education_path, \"philadelphia_schools.csv\")\neducation_shp_path &lt;- file.path(education_path, \"Schools Shape\", \"Schools.shp\")\n\n# Csv with School Names and metrics\nphilly_schools &lt;- read_csv(education_csv_path, show_col_types = FALSE)\n\n# Shp File including geometry\nphilly_schools_sf &lt;- st_read(education_shp_path, quiet = TRUE)\n\n\nWe joined the csv file containing the metrics with the shp file containing geoloaction.\n\n\nCode\n# Joining Schools csv metrics to shp file. Joined on 'location_i' (shp) and 'School_code (csv)\n# Keeping metrics for Attendance and Withdrawals\n\n# Select relevant metrics from CSV\nschool_metrics &lt;- philly_schools %&gt;%\n  select(School_code, Attendance, Withdrawals) %&gt;%\n  mutate(School_code = as.character(School_code))\n\nphilly_schools_sf &lt;- philly_schools_sf %&gt;%\n  left_join(school_metrics,\n            by = c(\"location_i\" = \"School_code\"))\n\n\n\n\nCode\nphilly_schools_sf_clean &lt;- philly_schools_sf %&gt;%\n  filter(!is.na(Attendance) & !is.na(Withdrawals))\n\n\n\n\nCode\nnames(philly_schools_sf_clean)\n\n\n [1] \"aun\"         \"school_num\"  \"location_i\"  \"school_nam\"  \"school_n_1\" \n [6] \"street_add\"  \"zip_code\"    \"phone_numb\"  \"grade_leve\"  \"grade_org\"  \n[11] \"enrollment\"  \"type\"        \"type_speci\"  \"objectid\"    \"Attendance\" \n[16] \"Withdrawals\" \"geometry\"   \n\n\nCode\nnrow(philly_schools_sf_clean)\n\n\n[1] 204\n\n\nOnce joined, we dropped rows that did not have values in Attendance and Withdrawal. This resulted in 204 public schools and their metrics located in Philadelphia City Limits.\n###Tree density Location of trees data was extracted from Opendata Philly. A Kernel Density Estimation was used to estimate the density of trees. The higher the value is, the more trees there will be in this (and surronding) cell\n\n\nCode\ntree_path &lt;- here(\"data\", \"ppr_tree_inventory_2024\")\n\ntree_shp_path &lt;- file.path(tree_path, \"ppr_tree_inventory_2024.shp\")\n\n\ntrees=st_read(tree_shp_path)\n\n\nReading layer `ppr_tree_inventory_2024' from data source \n  `/Users/ye/Documents/GitHub/portfolio-setup-yezhanggg/data/ppr_tree_inventory_2024/ppr_tree_inventory_2024.shp' \n  using driver `ESRI Shapefile'\nSimple feature collection with 151713 features and 6 fields\nGeometry type: POINT\nDimension:     XY\nBounding box:  xmin: -8380434 ymin: 4847791 xmax: -8344373 ymax: 4885938\nProjected CRS: WGS 84 / Pseudo-Mercator\n\n\nCode\ntrees=st_transform(trees,3364)\n\n# Convert sf points to spatstat ppp object\ncoords_trees &lt;- st_coordinates(trees)\npp_trees &lt;- ppp(x = coords_trees[,1], y = coords_trees[,2], window = win)\n\n\nWarning: 355 points were rejected as lying outside the specified window\n\n\nWarning: data contain duplicated points\n\n\nCode\n# ==== Run Kernel Density Estimation ====\n# Sigma = bandwidth in map units (here, meters)\ndensity_map_trees &lt;- density.ppp(pp_trees, edge = TRUE, at = \"pixels\",eps = c(100, 100))\n\n# ==== Convert to raster ====\nr_trees &lt;- rast(density_map_trees)\ncrs(r_trees) &lt;- st_crs(philly_boundary)$proj4string\nr_trees &lt;- mask(r_trees, vect(philly_boundary))  # mask to county boundary\n\n\n\n\nJoining data together\nFirst, since commercial and office POI and trees are point data, and the more they cluster the higher the housing price will be. As a result, we use Kernel Density method to estimate the density of them. Value of the cell was assign to the point (housing prices), if the point falls within it.\nSecond, as census tracts and neighborhood are polygon data, a st_within spatial join was used to join them with housing data. Values from polygon data will assign to the points when points fall within it.\n\n\nCode\n# Geometry - Cenus Data Merge\nphilly_tract_sf$GEOID=as.numeric(philly_tract_sf$GEOID)\nphilly_tract_map &lt;- philly_tract_sf %&gt;%\n  left_join(philly_censustract, by = \"GEOID\")\n\n#convert housing prices data into point data\nphl_sales_clean_sf = phl_sales_clean%&gt;%\n  mutate(geometry = st_as_sfc(shape)) %&gt;%   # parse WKT into geometry\n  st_as_sf(crs = 2272)  \n#convert and match the crs\nphilly_schools_sf_clean=philly_schools_sf_clean%&gt;%\n  st_transform(2272)\nphilly_neighborhoods=philly_neighborhoods%&gt;%\n  st_transform(2272)\nphilly_tract_map=philly_tract_map%&gt;%\n  st_transform(2272)\n#merge them together\nphl_sales_clean_sf_final=phl_sales_clean_sf%&gt;%\n  st_join(philly_tract_map,join=st_within)%&gt;%\n  st_join(philly_neighborhoods,join = st_within)\nphl_sales_clean_sf_final &lt;- st_transform(phl_sales_clean_sf_final, crs = st_crs(r_trees))\nphl_sales_clean_sf_final$EconKDE &lt;- raster::extract(r_Economic,phl_sales_clean_sf_final)\nphl_sales_clean_sf_final$TreeKDE &lt;- raster::extract(r_trees, phl_sales_clean_sf_final)  \nphl_sales_clean_sf_final=phl_sales_clean_sf_final%&gt;%\n  st_transform(2272)\n\n\n\n\nSummary table before and after dimensions\n\n\nCode\nbefore_after_summary &lt;- data.frame(\n  Stage = c(\"Raw Data\", \n            \"After Residential Filter (2023–24)\", \n            \"After Removing Errors & NAs\", \n            \"After Spatial Joins & Final Cleaning\"),\n  Rows = c(nrow(phl_sales),\n           nrow(phl_sales_res_23_24),\n           nrow(phl_sales_clean),\n           nrow(phl_sales_clean_sf_final)),\n  Columns = c(ncol(phl_sales),\n              ncol(phl_sales_res_23_24),\n              ncol(phl_sales_clean),\n              ncol(phl_sales_clean_sf_final))\n)\n\nknitr::kable(before_after_summary, caption = \"Data dimensions before and after cleaning\")\n\n\n\nData dimensions before and after cleaning\n\n\nStage\nRows\nColumns\n\n\n\n\nRaw Data\n583750\n79\n\n\nAfter Residential Filter (2023–24)\n34537\n79\n\n\nAfter Removing Errors & NAs\n22018\n81\n\n\nAfter Spatial Joins & Final Cleaning\n22018\n106"
  }
]