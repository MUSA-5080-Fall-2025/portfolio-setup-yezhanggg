[
  {
    "objectID": "labs/Zhang_Ye_Assignment1.html",
    "href": "labs/Zhang_Ye_Assignment1.html",
    "title": "Assignment 1: Census Data Quality for Policy Decisions",
    "section": "",
    "text": "You are a data analyst for the [California] Department of Human Services. The department is considering implementing an algorithmic system to identify communities that should receive priority for social service funding and outreach programs. Your supervisor has asked you to evaluate the quality and reliability of available census data to inform this decision.\nDrawing on our Week 2 discussion of algorithmic bias, you need to assess not just what the data shows, but how reliable it is and what communities might be affected by data quality issues.\n\n\n\n\nApply dplyr functions to real census data for policy analysis\nEvaluate data quality using margins of error\nConnect technical analysis to algorithmic decision-making\nIdentify potential equity implications of data reliability issues\nCreate professional documentation for policy stakeholders\n\n\n\n\nSubmit by posting your updated portfolio link on Canvas. Your assignment should be accessible at your-portfolio-url/assignments/assignment_1/\nMake sure to update your _quarto.yml navigation to include this assignment under an “Assignments” menu."
  },
  {
    "objectID": "labs/Zhang_Ye_Assignment1.html#scenario",
    "href": "labs/Zhang_Ye_Assignment1.html#scenario",
    "title": "Assignment 1: Census Data Quality for Policy Decisions",
    "section": "",
    "text": "You are a data analyst for the [California] Department of Human Services. The department is considering implementing an algorithmic system to identify communities that should receive priority for social service funding and outreach programs. Your supervisor has asked you to evaluate the quality and reliability of available census data to inform this decision.\nDrawing on our Week 2 discussion of algorithmic bias, you need to assess not just what the data shows, but how reliable it is and what communities might be affected by data quality issues."
  },
  {
    "objectID": "labs/Zhang_Ye_Assignment1.html#learning-objectives",
    "href": "labs/Zhang_Ye_Assignment1.html#learning-objectives",
    "title": "Assignment 1: Census Data Quality for Policy Decisions",
    "section": "",
    "text": "Apply dplyr functions to real census data for policy analysis\nEvaluate data quality using margins of error\nConnect technical analysis to algorithmic decision-making\nIdentify potential equity implications of data reliability issues\nCreate professional documentation for policy stakeholders"
  },
  {
    "objectID": "labs/Zhang_Ye_Assignment1.html#submission-instructions",
    "href": "labs/Zhang_Ye_Assignment1.html#submission-instructions",
    "title": "Assignment 1: Census Data Quality for Policy Decisions",
    "section": "",
    "text": "Submit by posting your updated portfolio link on Canvas. Your assignment should be accessible at your-portfolio-url/assignments/assignment_1/\nMake sure to update your _quarto.yml navigation to include this assignment under an “Assignments” menu."
  },
  {
    "objectID": "labs/Zhang_Ye_Assignment1.html#data-retrieval",
    "href": "labs/Zhang_Ye_Assignment1.html#data-retrieval",
    "title": "Assignment 1: Census Data Quality for Policy Decisions",
    "section": "2.1 Data Retrieval",
    "text": "2.1 Data Retrieval\nYour Task: Use get_acs() to retrieve county-level data for your chosen state.\nRequirements: - Geography: county level - Variables: median household income (B19013_001) and total population (B01003_001)\n- Year: 2022 - Survey: acs5 - Output format: wide\nHint: Remember to give your variables descriptive names using the variables = c(name = \"code\") syntax.\n\n# Write your get_acs() code here\n\ncounty_data &lt;- get_acs(\n  geography = \"county\",\n  variables = c(\n    median_income = \"B19013_001\",  \n    total_pop = \"B01003_001\"    \n  ),\n  state = my_state,\n  year = 2022,\n  output = \"wide\"\n)\n\n# Clean the county names to remove state name and \"County\" \n# Hint: use mutate() with str_remove()\n\ncounty_data &lt;- county_data %&gt;%\n  mutate(NAME = str_remove(NAME, \" County, California\"))\n\n# Display the first few rows\nhead(county_data)\n\n# A tibble: 6 × 6\n  GEOID NAME      median_incomeE median_incomeM total_popE total_popM\n  &lt;chr&gt; &lt;chr&gt;              &lt;dbl&gt;          &lt;dbl&gt;      &lt;dbl&gt;      &lt;dbl&gt;\n1 06001 Alameda           122488           1231    1663823         NA\n2 06003 Alpine            101125          17442       1515        206\n3 06005 Amador             74853           6048      40577         NA\n4 06007 Butte              66085           2261     213605         NA\n5 06009 Calaveras          77526           3875      45674         NA\n6 06011 Colusa             69619           5745      21811         NA"
  },
  {
    "objectID": "labs/Zhang_Ye_Assignment1.html#data-quality-assessment",
    "href": "labs/Zhang_Ye_Assignment1.html#data-quality-assessment",
    "title": "Assignment 1: Census Data Quality for Policy Decisions",
    "section": "2.2 Data Quality Assessment",
    "text": "2.2 Data Quality Assessment\nYour Task: Calculate margin of error percentages and create reliability categories.\nRequirements: - Calculate MOE percentage: (margin of error / estimate) * 100 - Create reliability categories: - High Confidence: MOE &lt; 5% - Moderate Confidence: MOE 5-10%\n- Low Confidence: MOE &gt; 10% - Create a flag for unreliable estimates (MOE &gt; 10%)\nHint: Use mutate() with case_when() for the categories.\n\n# Calculate MOE percentage and reliability categories using mutate()\n\ncounty_data &lt;- county_data %&gt;%\n  mutate(\n    income_moe_pct = (median_incomeM / median_incomeE) * 100,\n     reliability_category = case_when(\n      income_moe_pct &lt; 5 ~ \"High Confidence\",\n      income_moe_pct &lt;= 10 ~ \"Moderate Confidence\",\n      TRUE ~ \"Low Confidence\"),\n        unreliable_flag = income_moe_pct &gt; 10)\n\n# Create a summary showing count of counties in each reliability category\n# Hint: use count() and mutate() to add percentages\n\nreliability_summary &lt;- county_data %&gt;%\n  count(reliability_category) %&gt;%\n  mutate(\n    percentage = round(n / sum(n) * 100, 1)\n  )"
  },
  {
    "objectID": "labs/Zhang_Ye_Assignment1.html#high-uncertainty-counties",
    "href": "labs/Zhang_Ye_Assignment1.html#high-uncertainty-counties",
    "title": "Assignment 1: Census Data Quality for Policy Decisions",
    "section": "2.3 High Uncertainty Counties",
    "text": "2.3 High Uncertainty Counties\nYour Task: Identify the 5 counties with the highest MOE percentages.\nRequirements: - Sort by MOE percentage (highest first) - Select the top 5 counties - Display: county name, median income, margin of error, MOE percentage, reliability category - Format as a professional table using kable()\nHint: Use arrange(), slice(), and select() functions.\n\n# Create table of top 5 counties by MOE percentage\n\nhigh_uncertainty_table &lt;- county_data %&gt;%\n  arrange(desc(income_moe_pct)) %&gt;%  \n  slice_head(n = 5) %&gt;% \n  select(NAME, median_incomeE, median_incomeM, income_moe_pct, reliability_category)\n\n# Format as table with kable() - include appropriate column names and caption\n\nlibrary(knitr)\nlibrary(kableExtra)\n\nhigh_uncertainty_table %&gt;%\n  kable(\n    col.names = c(\"County\", \"Median Income\", \"Margin of Error\", \"MOE (%)\", \"Reliability\"),\n    caption = \"Top 5 California Counties with Highest Income Uncertainty\",\n    format.args = list(big.mark = \",\"),\n    digits = c(0, 0, 0, 1, 0) \n  ) %&gt;%\n  kable_styling(bootstrap_options = c(\"striped\", \"hover\"))\n\n\nTop 5 California Counties with Highest Income Uncertainty\n\n\nCounty\nMedian Income\nMargin of Error\nMOE (%)\nReliability\n\n\n\n\nMono\n82,038\n15,388\n18.8\nLow Confidence\n\n\nAlpine\n101,125\n17,442\n17.2\nLow Confidence\n\n\nSierra\n61,108\n9,237\n15.1\nLow Confidence\n\n\nTrinity\n47,317\n5,890\n12.4\nLow Confidence\n\n\nPlumas\n67,885\n7,772\n11.4\nLow Confidence\n\n\n\n\n\nData Quality Commentary:\n[Counties that have higher margin of error will have unreliable income estimates and could lead to misleading policy plan and resource allocation. The high margin of error could because these are counties that have low population and the data could easily skewed.]"
  },
  {
    "objectID": "labs/Zhang_Ye_Assignment1.html#focus-area-selection",
    "href": "labs/Zhang_Ye_Assignment1.html#focus-area-selection",
    "title": "Assignment 1: Census Data Quality for Policy Decisions",
    "section": "3.1 Focus Area Selection",
    "text": "3.1 Focus Area Selection\nYour Task: Select 2-3 counties from your reliability analysis for detailed tract-level study.\nStrategy: Choose counties that represent different reliability levels (e.g., 1 high confidence, 1 moderate, 1 low confidence) to compare how data quality varies.\n\n# Use filter() to select 2-3 counties from your county_reliability data\n# Store the selected counties in a variable called selected_counties\n\nselected_counties &lt;- county_data %&gt;%\n  filter(NAME %in% c(\"Los Angeles\", \"Trinity\", \"Tehama\")) %&gt;%\n  select(NAME, median_incomeE, income_moe_pct, reliability_category)\n\n# Display the selected counties with their key characteristics\n# Show: county name, median income, MOE percentage, reliability category\n\nprint(selected_counties)\n\n# A tibble: 3 × 4\n  NAME        median_incomeE income_moe_pct reliability_category\n  &lt;chr&gt;                &lt;dbl&gt;          &lt;dbl&gt; &lt;chr&gt;               \n1 Los Angeles          83411          0.526 High Confidence     \n2 Tehama               59029          6.95  Moderate Confidence \n3 Trinity              47317         12.4   Low Confidence      \n\n\nComment on the output: [Los Angeles has a High Confidence category because of its extremely low income margin of error percent. This is because Los Angeles is one of the most populated county and that lead to low margin of error. On the another other hand, Tehama has a income margin of error percentage around 7 and Trinity is at 12. The data for Tehama and Trinity are less reliable compare to Los Angeles County.]"
  },
  {
    "objectID": "labs/Zhang_Ye_Assignment1.html#tract-level-demographics",
    "href": "labs/Zhang_Ye_Assignment1.html#tract-level-demographics",
    "title": "Assignment 1: Census Data Quality for Policy Decisions",
    "section": "3.2 Tract-Level Demographics",
    "text": "3.2 Tract-Level Demographics\nYour Task: Get demographic data for census tracts in your selected counties.\nRequirements: - Geography: tract level - Variables: white alone (B03002_003), Black/African American (B03002_004), Hispanic/Latino (B03002_012), total population (B03002_001) - Use the same state and year as before - Output format: wide - Challenge: You’ll need county codes, not names. Look at the GEOID patterns in your county data for hints.\n\n# Define your race/ethnicity variables with descriptive names\n\nrace_vars &lt;- c(\n  total_pop = \"B03002_001\",\n  white_alone = \"B03002_003\",\n  black_alone = \"B03002_004\",\n  hispanic = \"B03002_012\"\n)\n\n# Use get_acs() to retrieve tract-level data\n# Hint: You may need to specify county codes in the county parameter\n\ntract_demographics &lt;- get_acs(\n  geography = \"tract\",\n  variables = race_vars,\n  state = my_state,\n  county = c(\"037\", \"105\", \"103\"),\n  year = 2022,\n  output = \"wide\"\n)\n\n# Calculate percentage of each group using mutate()\n# Create percentages for white, Black, and Hispanic populations\n\ntract_demographics &lt;- tract_demographics %&gt;%\n  mutate(\n    white_pct = (white_aloneE / total_popE) * 100,\n    black_pct = (black_aloneE / total_popE) * 100,\n    hispanic_pct = (hispanicE / total_popE) * 100\n  )\n\n# Add readable tract and county name columns using str_extract() or similar\n\ntract_demographics &lt;- tract_demographics %&gt;%\n  mutate(\n    census_tract = str_remove(NAME, \";.*\"),\n    county = str_remove(NAME, \"Census Tract [0-9.]+; \") %&gt;%\n             str_remove(\"; California\") %&gt;%\n             str_remove(\" County\")\n  )"
  },
  {
    "objectID": "labs/Zhang_Ye_Assignment1.html#demographic-analysis",
    "href": "labs/Zhang_Ye_Assignment1.html#demographic-analysis",
    "title": "Assignment 1: Census Data Quality for Policy Decisions",
    "section": "3.3 Demographic Analysis",
    "text": "3.3 Demographic Analysis\nYour Task: Analyze the demographic patterns in your selected areas.\n\n# Find the tract with the highest percentage of Hispanic/Latino residents\n# Hint: use arrange() and slice() to get the top tract\n\nhighest_hispanic &lt;- tract_demographics %&gt;%\n  arrange(desc(hispanic_pct)) %&gt;%\n  slice_head(n = 1) %&gt;%\n  select(census_tract, county, hispanic_pct)\n\n# Calculate average demographics by county using group_by() and summarize()\n# Show: number of tracts, average percentage for each racial/ethnic group\n\ncounty_demographics &lt;- tract_demographics %&gt;%\n  group_by(county) %&gt;%\n  summarize(\n    num_tracts = n(),\n    avg_white_pct = mean(white_pct, na.rm = TRUE),\n    avg_black_pct = mean(black_pct, na.rm = TRUE),\n    avg_hispanic_pct = mean(hispanic_pct, na.rm = TRUE)\n  )\n\n# Create a nicely formatted table of your results using kable()\n\nlibrary(knitr)          \ncounty_demographics %&gt;%\n  kable(\n    col.names = c(\"County\", \"Number of Tracts\", \"Avg White %\", \"Avg Black %\", \"Avg Hispanic %\"), \n    caption = \"Average Demographics by County\",  \n    digits = 1           \n  )\n\n\nAverage Demographics by County\n\n\n\n\n\n\n\n\n\nCounty\nNumber of Tracts\nAvg White %\nAvg Black %\nAvg Hispanic %\n\n\n\n\nLos Angeles\n2498\n26.3\n7.6\n47.6\n\n\nTehama\n14\n65.9\n0.9\n26.0\n\n\nTrinity\n4\n79.2\n1.7\n7.0"
  },
  {
    "objectID": "labs/Zhang_Ye_Assignment1.html#moe-analysis-for-demographic-variables",
    "href": "labs/Zhang_Ye_Assignment1.html#moe-analysis-for-demographic-variables",
    "title": "Assignment 1: Census Data Quality for Policy Decisions",
    "section": "4.1 MOE Analysis for Demographic Variables",
    "text": "4.1 MOE Analysis for Demographic Variables\nYour Task: Examine margins of error for demographic variables to see if some communities have less reliable data.\nRequirements: - Calculate MOE percentages for each demographic variable - Flag tracts where any demographic variable has MOE &gt; 15% - Create summary statistics\n\n# Calculate MOE percentages for white, Black, and Hispanic variables\n# Hint: use the same formula as before (margin/estimate * 100)\n\ntract_demographics &lt;- tract_demographics %&gt;%\n  mutate(\n    white_moe_pct = (white_aloneM / white_aloneE) * 100,\n    black_moe_pct = (black_aloneM / black_aloneE) * 100,\n    hispanic_moe_pct = (hispanicM / hispanicE) * 100,\n    high_moe_flag = ifelse(white_moe_pct &gt; 15 | black_moe_pct &gt; 15 | hispanic_moe_pct &gt; 15, TRUE, FALSE)\n  )\n\n# Create a flag for tracts with high MOE on any demographic variable\n# Use logical operators (| for OR) in an ifelse() statement\n\n# Create summary statistics showing how many tracts have data quality issues\n\nmoe_summary &lt;- tract_demographics %&gt;%\n  summarize(\n    total_tracts = n(),\n    tracts_with_issues = sum(high_moe_flag, na.rm = TRUE),\n    pct_with_issues = round(sum(high_moe_flag, na.rm = TRUE) / n() * 100, 1)\n  )\n\nmoe_summary\n\n# A tibble: 1 × 3\n  total_tracts tracts_with_issues pct_with_issues\n         &lt;int&gt;              &lt;int&gt;           &lt;dbl&gt;\n1         2516               2515             100"
  },
  {
    "objectID": "labs/Zhang_Ye_Assignment1.html#pattern-analysis",
    "href": "labs/Zhang_Ye_Assignment1.html#pattern-analysis",
    "title": "Assignment 1: Census Data Quality for Policy Decisions",
    "section": "4.2 Pattern Analysis",
    "text": "4.2 Pattern Analysis\nYour Task: Investigate whether data quality problems are randomly distributed or concentrated in certain types of communities.\n\n# Group tracts by whether they have high MOE issues\n# Calculate average characteristics for each group:\n# - population size, demographic percentages\n\npattern_comparison &lt;- tract_demographics %&gt;%\n  group_by(high_moe_flag) %&gt;%\n  summarize(\n    num_tracts = n(),\n    avg_population = mean(total_popE, na.rm = TRUE),\n    avg_pct_white = mean(white_pct, na.rm = TRUE),\n    avg_pct_black = mean(black_pct, na.rm = TRUE),\n    avg_pct_hispanic = mean(hispanic_pct, na.rm = TRUE)\n  )\n\n# Use group_by() and summarize() to create this comparison\n# Create a professional table showing the patterns\n\nlibrary(knitr)\npattern_comparison %&gt;%\n  kable(\n    col.names = c(\"High MOE Issues\", \"Number of Tracts\", \"Avg Population\", \n                  \"Avg White% \", \"Avg Black %\", \"Avg Hispanic %\"),\n    caption = \"Comparison of Tract Characteristics by Data Quality\",\n    digits = c(0, 0, 0, 1, 1, 1)\n  )\n\n\nComparison of Tract Characteristics by Data Quality\n\n\n\n\n\n\n\n\n\n\nHigh MOE Issues\nNumber of Tracts\nAvg Population\nAvg White%\nAvg Black %\nAvg Hispanic %\n\n\n\n\nFALSE\n1\n8994\n16.8\n33.6\n41.4\n\n\nTRUE\n2515\n3980\n26.6\n7.5\n47.4\n\n\n\n\n\nPattern Analysis: [It is crazy that almost all of the data are not reliable. The estimate for racial group is not reliable on the census tract level through their high margin of error. I think this might further stress the point of how sampling population is critical in doing analysis. The average population for all the HIGH MOE Issues tracts is at 3980. The only one tract without HIGH MOE Issue has a population 0f 8994.]"
  },
  {
    "objectID": "labs/Zhang_Ye_Assignment1.html#analysis-integration-and-professional-summary",
    "href": "labs/Zhang_Ye_Assignment1.html#analysis-integration-and-professional-summary",
    "title": "Assignment 1: Census Data Quality for Policy Decisions",
    "section": "5.1 Analysis Integration and Professional Summary",
    "text": "5.1 Analysis Integration and Professional Summary\nYour Task: Write an executive summary that integrates findings from all four analyses.\nExecutive Summary Requirements: 1. Overall Pattern Identification: What are the systematic patterns across all your analyses? 2. Equity Assessment: Which communities face the greatest risk of algorithmic bias based on your findings? 3. Root Cause Analysis: What underlying factors drive both data quality issues and bias risk? 4. Strategic Recommendations: What should the Department implement to address these systematic issues?\nExecutive Summary:\n[I think the most important takeaway from this assignment is the important of sampling size. This situation has appear in multiple analysis and multple times. If a data has some reliability issue, one of the characteristic of that data is the small sampling size.\nThe community that faced the greatest risk of algorithmic bias is Trinity. Trinity only has 4 census tract and it such a small county that the data could highly likely to be skewed because of sampling error. It is also show as a “Low Confidence” in county level income data. The interpretation and result could be really not reliable due to data quality issue.\nThe root cause may be the survey design and also just how insufficient for the data to be reliable if we want to look at small geographic areas like census tracts. Rural places like Trinity and Tehama make reliable sampling really challenging.\nI think one of the most important is to apply strict MOE thresholds. Data that are not reliable especially on small sample and big geographic area it is really important to examine the data closely. Aggregate the data and looking at large scale is necessary if the public organization want to implement any policies. Careful decision making process is necessary and key for community success. ]"
  },
  {
    "objectID": "labs/Zhang_Ye_Assignment1.html#specific-recommendations",
    "href": "labs/Zhang_Ye_Assignment1.html#specific-recommendations",
    "title": "Assignment 1: Census Data Quality for Policy Decisions",
    "section": "6.3 Specific Recommendations",
    "text": "6.3 Specific Recommendations\nYour Task: Create a decision framework for algorithm implementation.\n\n# Create a summary table using your county reliability data\n# Include: county name, median income, MOE percentage, reliability category\n\ndecision_framework &lt;- county_data %&gt;%\n  select(NAME, median_incomeE, income_moe_pct, reliability_category) %&gt;%\n  mutate(\n    algorithm_recommendation = case_when(\n      reliability_category == \"High Confidence\" ~ \"Safe for algorithmic decisions\",\n      reliability_category == \"Moderate Confidence\" ~ \"Use with caution - monitor outcomes\",\n      reliability_category == \"Low Confidence\" ~ \"Requires manual review or additional data\"\n    )\n  )\n\n# Add a new column with algorithm recommendations using case_when():\n# - High Confidence: \"Safe for algorithmic decisions\"\n# - Moderate Confidence: \"Use with caution - monitor outcomes\"  \n# - Low Confidence: \"Requires manual review or additional data\"\n\n# Format as a professional table with kable()\n\nlibrary(knitr)\nlibrary(kableExtra)\n\ndecision_framework %&gt;%\n  kable(\n    col.names = c(\"County\", \"Median Income\", \"MOE %\", \"Reliability\", \"Algorithm Recommendation\"),\n    caption = \"Decision Framework for Algorithm Implementation by County\",\n    format.args = list(big.mark = \",\"),\n    digits = c(0, 0, 1, 0, 0)\n  ) %&gt;%\n  kable_styling(bootstrap_options = c(\"striped\", \"hover\"))\n\n\nDecision Framework for Algorithm Implementation by County\n\n\nCounty\nMedian Income\nMOE %\nReliability\nAlgorithm Recommendation\n\n\n\n\nAlameda\n122,488\n1.0\nHigh Confidence\nSafe for algorithmic decisions\n\n\nAlpine\n101,125\n17.2\nLow Confidence\nRequires manual review or additional data\n\n\nAmador\n74,853\n8.1\nModerate Confidence\nUse with caution - monitor outcomes\n\n\nButte\n66,085\n3.4\nHigh Confidence\nSafe for algorithmic decisions\n\n\nCalaveras\n77,526\n5.0\nHigh Confidence\nSafe for algorithmic decisions\n\n\nColusa\n69,619\n8.3\nModerate Confidence\nUse with caution - monitor outcomes\n\n\nContra Costa\n120,020\n1.2\nHigh Confidence\nSafe for algorithmic decisions\n\n\nDel Norte\n61,149\n7.2\nModerate Confidence\nUse with caution - monitor outcomes\n\n\nEl Dorado\n99,246\n3.4\nHigh Confidence\nSafe for algorithmic decisions\n\n\nFresno\n67,756\n1.4\nHigh Confidence\nSafe for algorithmic decisions\n\n\nGlenn\n64,033\n6.2\nModerate Confidence\nUse with caution - monitor outcomes\n\n\nHumboldt\n57,881\n3.7\nHigh Confidence\nSafe for algorithmic decisions\n\n\nImperial\n53,847\n4.1\nHigh Confidence\nSafe for algorithmic decisions\n\n\nInyo\n63,417\n8.6\nModerate Confidence\nUse with caution - monitor outcomes\n\n\nKern\n63,883\n2.1\nHigh Confidence\nSafe for algorithmic decisions\n\n\nKings\n68,540\n3.3\nHigh Confidence\nSafe for algorithmic decisions\n\n\nLake\n56,259\n4.3\nHigh Confidence\nSafe for algorithmic decisions\n\n\nLassen\n59,515\n6.0\nModerate Confidence\nUse with caution - monitor outcomes\n\n\nLos Angeles\n83,411\n0.5\nHigh Confidence\nSafe for algorithmic decisions\n\n\nMadera\n73,543\n3.9\nHigh Confidence\nSafe for algorithmic decisions\n\n\nMarin\n142,019\n2.9\nHigh Confidence\nSafe for algorithmic decisions\n\n\nMariposa\n60,021\n8.8\nModerate Confidence\nUse with caution - monitor outcomes\n\n\nMendocino\n61,335\n3.6\nHigh Confidence\nSafe for algorithmic decisions\n\n\nMerced\n64,772\n3.3\nHigh Confidence\nSafe for algorithmic decisions\n\n\nModoc\n54,962\n9.8\nModerate Confidence\nUse with caution - monitor outcomes\n\n\nMono\n82,038\n18.8\nLow Confidence\nRequires manual review or additional data\n\n\nMonterey\n91,043\n2.1\nHigh Confidence\nSafe for algorithmic decisions\n\n\nNapa\n105,809\n2.8\nHigh Confidence\nSafe for algorithmic decisions\n\n\nNevada\n79,395\n4.8\nHigh Confidence\nSafe for algorithmic decisions\n\n\nOrange\n109,361\n0.8\nHigh Confidence\nSafe for algorithmic decisions\n\n\nPlacer\n109,375\n1.7\nHigh Confidence\nSafe for algorithmic decisions\n\n\nPlumas\n67,885\n11.4\nLow Confidence\nRequires manual review or additional data\n\n\nRiverside\n84,505\n1.3\nHigh Confidence\nSafe for algorithmic decisions\n\n\nSacramento\n84,010\n1.0\nHigh Confidence\nSafe for algorithmic decisions\n\n\nSan Benito\n104,451\n5.2\nModerate Confidence\nUse with caution - monitor outcomes\n\n\nSan Bernardino\n77,423\n1.0\nHigh Confidence\nSafe for algorithmic decisions\n\n\nSan Diego\n96,974\n1.0\nHigh Confidence\nSafe for algorithmic decisions\n\n\nSan Francisco\n136,689\n1.4\nHigh Confidence\nSafe for algorithmic decisions\n\n\nSan Joaquin\n82,837\n1.7\nHigh Confidence\nSafe for algorithmic decisions\n\n\nSan Luis Obispo\n90,158\n2.6\nHigh Confidence\nSafe for algorithmic decisions\n\n\nSan Mateo\n149,907\n1.7\nHigh Confidence\nSafe for algorithmic decisions\n\n\nSanta Barbara\n92,332\n2.0\nHigh Confidence\nSafe for algorithmic decisions\n\n\nSanta Clara\n153,792\n1.0\nHigh Confidence\nSafe for algorithmic decisions\n\n\nSanta Cruz\n104,409\n3.0\nHigh Confidence\nSafe for algorithmic decisions\n\n\nShasta\n68,347\n3.6\nHigh Confidence\nSafe for algorithmic decisions\n\n\nSierra\n61,108\n15.1\nLow Confidence\nRequires manual review or additional data\n\n\nSiskiyou\n53,898\n4.9\nHigh Confidence\nSafe for algorithmic decisions\n\n\nSolano\n97,037\n1.8\nHigh Confidence\nSafe for algorithmic decisions\n\n\nSonoma\n99,266\n2.0\nHigh Confidence\nSafe for algorithmic decisions\n\n\nStanislaus\n74,872\n1.8\nHigh Confidence\nSafe for algorithmic decisions\n\n\nSutter\n72,654\n4.7\nHigh Confidence\nSafe for algorithmic decisions\n\n\nTehama\n59,029\n7.0\nModerate Confidence\nUse with caution - monitor outcomes\n\n\nTrinity\n47,317\n12.4\nLow Confidence\nRequires manual review or additional data\n\n\nTulare\n64,474\n2.3\nHigh Confidence\nSafe for algorithmic decisions\n\n\nTuolumne\n70,432\n6.7\nModerate Confidence\nUse with caution - monitor outcomes\n\n\nVentura\n102,141\n1.5\nHigh Confidence\nSafe for algorithmic decisions\n\n\nYolo\n85,097\n2.7\nHigh Confidence\nSafe for algorithmic decisions\n\n\nYuba\n66,693\n4.2\nHigh Confidence\nSafe for algorithmic decisions\n\n\n\n\n\nKey Recommendations:\nYour Task: Use your analysis results to provide specific guidance to the department.\n\nhigh_confidence &lt;- decision_framework %&gt;%\n  filter(reliability_category == \"High Confidence\") %&gt;%\n  select(NAME)\n\nmoderate_confidence &lt;- decision_framework %&gt;%\n  filter(reliability_category == \"Moderate Confidence\") %&gt;%\n  select(NAME)\n\nlow_confidence &lt;- decision_framework %&gt;%\n  filter(reliability_category == \"Low Confidence\") %&gt;%\n  select(NAME)\n\nprint(\"High Confidence Counties:\")\n\n[1] \"High Confidence Counties:\"\n\nprint(high_confidence)\n\n# A tibble: 42 × 1\n   NAME        \n   &lt;chr&gt;       \n 1 Alameda     \n 2 Butte       \n 3 Calaveras   \n 4 Contra Costa\n 5 El Dorado   \n 6 Fresno      \n 7 Humboldt    \n 8 Imperial    \n 9 Kern        \n10 Kings       \n# ℹ 32 more rows\n\nprint(\"Moderate Confidence Counties:\")\n\n[1] \"Moderate Confidence Counties:\"\n\nprint(moderate_confidence)\n\n# A tibble: 11 × 1\n   NAME      \n   &lt;chr&gt;     \n 1 Amador    \n 2 Colusa    \n 3 Del Norte \n 4 Glenn     \n 5 Inyo      \n 6 Lassen    \n 7 Mariposa  \n 8 Modoc     \n 9 San Benito\n10 Tehama    \n11 Tuolumne  \n\nprint(\"Low Confidence Counties:\")\n\n[1] \"Low Confidence Counties:\"\n\nprint(low_confidence)\n\n# A tibble: 5 × 1\n  NAME   \n  &lt;chr&gt;  \n1 Alpine \n2 Mono   \n3 Plumas \n4 Sierra \n5 Trinity\n\n\n\nCounties suitable for immediate algorithmic implementation: [The counties that are suitabale for immediate algorithmic implementation are Alameda, Butte, Calaveras, Contra Costa, El Dorado, Fresno, Humboldt, Imperial, Kern, Kings, Lake, Los Angeles, Madera, Marin, Mendocino, Merced, Monterey, Napa, Nevada, Orange, Placer, Riverside, Sacramento, San Bernardino, San Diego, San Francisco, San Joaquin, San Luis Obispo, San Mateo, Santa Barbara, Santa Clara, Santa Cruz, Shasta, Siskiyou, Solano, Sonoma, Stanislaus, Sutter, Tulare, Ventura, Yolo, Yuba. These county are in the high confidence category for the new decision making framework.]\nCounties requiring additional oversight: [Counties need additional oversight are Amador, Colusa, Del Norte, Glenn, Inyo, Lassen, Mariposa, Modoc, San Benito, Tehama, Tuolumne. These county are in the moderate confidence category for the new decision making framework.]\nCounties needing alternative approaches: [Counties that need alternative approaches are Alpine, Mono, Plumas, Sierra, Trinity because they are in the low confidence category for the new decision making framework.]"
  },
  {
    "objectID": "labs/Zhang_Ye_Assignment1.html#questions-for-further-investigation",
    "href": "labs/Zhang_Ye_Assignment1.html#questions-for-further-investigation",
    "title": "Assignment 1: Census Data Quality for Policy Decisions",
    "section": "Questions for Further Investigation",
    "text": "Questions for Further Investigation\n[- How has the reliability of ACS estimates changed over time for small counties like Trinity? - Do neighboring census tracts with high MOEs cluster together spatially?]"
  },
  {
    "objectID": "labs/Zhang_Ye_Assignment1.html#submission-checklist",
    "href": "labs/Zhang_Ye_Assignment1.html#submission-checklist",
    "title": "Assignment 1: Census Data Quality for Policy Decisions",
    "section": "Submission Checklist",
    "text": "Submission Checklist\nBefore submitting your portfolio link on Canvas:\n\nAll code chunks run without errors\nAll “[Fill this in]” prompts have been completed\nTables are properly formatted and readable\nExecutive summary addresses all four required components\nPortfolio navigation includes this assignment\nCensus API key is properly set\nDocument renders correctly to HTML\n\nRemember: Submit your portfolio URL on Canvas, not the file itself. Your assignment should be accessible at your-portfolio-url/assignments/assignment_1/your_file_name.html"
  },
  {
    "objectID": "weekly-notes/week-05-notes.html",
    "href": "weekly-notes/week-05-notes.html",
    "title": "Week 5 Notes",
    "section": "",
    "text": "We learned about linear regression and different estimator and their purposes. We talked about how to differentiate inference and prediction. We also built our first model to test statistical significance. We talked about the case of overfitting and how test what is considered as a good model."
  },
  {
    "objectID": "weekly-notes/week-05-notes.html#key-concepts-learned",
    "href": "weekly-notes/week-05-notes.html#key-concepts-learned",
    "title": "Week 5 Notes",
    "section": "",
    "text": "We learned about linear regression and different estimator and their purposes. We talked about how to differentiate inference and prediction. We also built our first model to test statistical significance. We talked about the case of overfitting and how test what is considered as a good model."
  },
  {
    "objectID": "weekly-notes/week-05-notes.html#coding-techniques",
    "href": "weekly-notes/week-05-notes.html#coding-techniques",
    "title": "Week 5 Notes",
    "section": "Coding Techniques",
    "text": "Coding Techniques\n\nBelow are some of the core techniques that we covered in class and used in class.\nlm(formula = attribute ~ attribute, data = pa_data) is how to built a model.\nsummary of the model will provide all the important statistics.\nset.seed(123) is to set random number generator\nsqrt(mean is to find RMSE\ntrain_control &lt;- trainControl(method = “cv”, number = 10) sets up the cross-validation technique"
  },
  {
    "objectID": "weekly-notes/week-05-notes.html#questions-challenges",
    "href": "weekly-notes/week-05-notes.html#questions-challenges",
    "title": "Week 5 Notes",
    "section": "Questions & Challenges",
    "text": "Questions & Challenges\n\nI think the hard question is how to find the best predictor for you model while also taking the consideration of different linear model assumption and the biases that we want to avoid. The in-class activity of finding home value was quite challeneging by finding the best predictors that are significant."
  },
  {
    "objectID": "weekly-notes/week-05-notes.html#connections-to-policy",
    "href": "weekly-notes/week-05-notes.html#connections-to-policy",
    "title": "Week 5 Notes",
    "section": "Connections to Policy",
    "text": "Connections to Policy\n\nThis is super useful when we trying to find the correlation or coenction between different inputs and results especially when we are designing public policy."
  },
  {
    "objectID": "weekly-notes/week-05-notes.html#reflection",
    "href": "weekly-notes/week-05-notes.html#reflection",
    "title": "Week 5 Notes",
    "section": "Reflection",
    "text": "Reflection\n\nIt is importan to remember the difference between inference and prediction. We need to carefully look at the model to prevent in-sample fit and overfitting. It is important to check assumption and plot things out to see."
  },
  {
    "objectID": "weekly-notes/week-03-notes.html",
    "href": "weekly-notes/week-03-notes.html",
    "title": "Week 3 Notes",
    "section": "",
    "text": "We learn some ways to do visualizations and the asethetic mapping. What is a ggplot. We also learned how to join data."
  },
  {
    "objectID": "weekly-notes/week-03-notes.html#key-concepts-learned",
    "href": "weekly-notes/week-03-notes.html#key-concepts-learned",
    "title": "Week 3 Notes",
    "section": "",
    "text": "We learn some ways to do visualizations and the asethetic mapping. What is a ggplot. We also learned how to join data."
  },
  {
    "objectID": "weekly-notes/week-03-notes.html#coding-techniques",
    "href": "weekly-notes/week-03-notes.html#coding-techniques",
    "title": "Week 3 Notes",
    "section": "Coding Techniques",
    "text": "Coding Techniques\n\nIn class, we learned the basics of ggplot2 and joins in dplyr. In ggplot2, aesthetic mappings like x, y, color, fill, size, shape, and alpha control visual elements such as position, color, and transparency. A typical plot follows the structure ggplot(data) + aes(x, y) + geom_something() + additional_layers(), and layers are added with +. We also covered data joins in dplyr: left_join() keeps all rows from the left dataset, right_join() keeps all from the right, inner_join() keeps only matching rows, and full_join() keeps all rows from both, with left_join() being the most common for adding columns to a main dataset."
  },
  {
    "objectID": "weekly-notes/week-03-notes.html#questions-challenges",
    "href": "weekly-notes/week-03-notes.html#questions-challenges",
    "title": "Week 3 Notes",
    "section": "Questions & Challenges",
    "text": "Questions & Challenges\n\nI think it is very important to follow all the steps correct to make everything work."
  },
  {
    "objectID": "weekly-notes/week-03-notes.html#connections-to-policy",
    "href": "weekly-notes/week-03-notes.html#connections-to-policy",
    "title": "Week 3 Notes",
    "section": "Connections to Policy",
    "text": "Connections to Policy\n\nLearning ggplot2 and dplyr helps us turn data into clear visuals and summaries that make policy trends easier to understand. This makes it simpler to spot patterns and share insights that can guide better, data-informed decisions."
  },
  {
    "objectID": "weekly-notes/week-03-notes.html#reflection",
    "href": "weekly-notes/week-03-notes.html#reflection",
    "title": "Week 3 Notes",
    "section": "Reflection",
    "text": "Reflection\n\nTo become more proficient in this process, it requires a lot of time. Even following the class excercise could be challenging. More practice!"
  },
  {
    "objectID": "weekly-notes/week-01-notes.html",
    "href": "weekly-notes/week-01-notes.html",
    "title": "Week 1 Notes - Course Introduction",
    "section": "",
    "text": "What is Git, Github, Quarto, and full term for YAML (YAML Ain’t Markup Language).\nSome basic R coding language and went over an example on CAR data."
  },
  {
    "objectID": "weekly-notes/week-01-notes.html#key-concepts-learned",
    "href": "weekly-notes/week-01-notes.html#key-concepts-learned",
    "title": "Week 1 Notes - Course Introduction",
    "section": "",
    "text": "What is Git, Github, Quarto, and full term for YAML (YAML Ain’t Markup Language).\nSome basic R coding language and went over an example on CAR data."
  },
  {
    "objectID": "weekly-notes/week-01-notes.html#coding-techniques",
    "href": "weekly-notes/week-01-notes.html#coding-techniques",
    "title": "Week 1 Notes - Course Introduction",
    "section": "Coding Techniques",
    "text": "Coding Techniques\n\nBold text\nItalic text\nBold and italic\ncode text\nStrikethrough\nselect() - choose columns\nfilter() - choose rows\nmutate() - create new variables\nsummarize() - calculate statistics\ngroup_by() - operate on groups]\ntitle: “My Analysis” author: “Your Name” date: today format: html"
  },
  {
    "objectID": "weekly-notes/week-01-notes.html#questions-challenges",
    "href": "weekly-notes/week-01-notes.html#questions-challenges",
    "title": "Week 1 Notes - Course Introduction",
    "section": "Questions & Challenges",
    "text": "Questions & Challenges\n\nI think it is hard to understand all the R code and notations.\nThis is my first time interact with R and Github. So I need to try everything first and I will find more challenges."
  },
  {
    "objectID": "weekly-notes/week-01-notes.html#connections-to-policy",
    "href": "weekly-notes/week-01-notes.html#connections-to-policy",
    "title": "Week 1 Notes - Course Introduction",
    "section": "Connections to Policy",
    "text": "Connections to Policy\n\nBoth the analysis and the data itself can be biased. The reason is that data were collected by someone who might be biased."
  },
  {
    "objectID": "weekly-notes/week-01-notes.html#reflection",
    "href": "weekly-notes/week-01-notes.html#reflection",
    "title": "Week 1 Notes - Course Introduction",
    "section": "Reflection",
    "text": "Reflection\n\nQuarto is really interesting. Looking forward to see more what Quarto can do.\nThrough our assignment. I think it will be a great practice for me."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "MUSA 5080 Portfolio",
    "section": "",
    "text": "This portfolio documents my learning journey in Public Policy Analytics (MUSA 5080).\n\n\nAdvanced spatial analysis and data science for urban planning and public policy.\n\n\n\n\nWeekly Notes: My learning reflections and key concepts\nAssignment: Completed assignments and analyses\nFinal Project: Capstone modeling challenge\n\n\n\n\n\nMy name is Ye and I did my Bachelor in Urban Planning at USC ✌️✌️.\nMy email is yezhang1@upenn.edu\nMy Github user name is yezhanggg\nI think it is a requirement. Also I don’t have any prior experience with R and I think it is powerful tool that would be really helpful for me to know. It has so many potential and implications that I could use in multiple areas and projects.\n\n\n\n\n\nEmail: [yezhang1@design.upenn.edu]\nGitHub: [@yezhanggg]"
  },
  {
    "objectID": "index.html#about-this-course",
    "href": "index.html#about-this-course",
    "title": "MUSA 5080 Portfolio",
    "section": "",
    "text": "Advanced spatial analysis and data science for urban planning and public policy."
  },
  {
    "objectID": "index.html#portfolio-structure",
    "href": "index.html#portfolio-structure",
    "title": "MUSA 5080 Portfolio",
    "section": "",
    "text": "Weekly Notes: My learning reflections and key concepts\nAssignment: Completed assignments and analyses\nFinal Project: Capstone modeling challenge"
  },
  {
    "objectID": "index.html#about-me",
    "href": "index.html#about-me",
    "title": "MUSA 5080 Portfolio",
    "section": "",
    "text": "My name is Ye and I did my Bachelor in Urban Planning at USC ✌️✌️.\nMy email is yezhang1@upenn.edu\nMy Github user name is yezhanggg\nI think it is a requirement. Also I don’t have any prior experience with R and I think it is powerful tool that would be really helpful for me to know. It has so many potential and implications that I could use in multiple areas and projects."
  },
  {
    "objectID": "index.html#contact",
    "href": "index.html#contact",
    "title": "MUSA 5080 Portfolio",
    "section": "",
    "text": "Email: [yezhang1@design.upenn.edu]\nGitHub: [@yezhanggg]"
  },
  {
    "objectID": "weekly-notes/week-02-notes.html",
    "href": "weekly-notes/week-02-notes.html",
    "title": "Week 2 Notes",
    "section": "",
    "text": "I learned about topics on algorithmic decision making and what is census data.\nWe also went through a scenarios practices where we designed the ethical algorithms.\nThe importance of having algorithmic decision making.\nCensus data is really large but it has a lot of potential to do some great things."
  },
  {
    "objectID": "weekly-notes/week-02-notes.html#key-concepts-learned",
    "href": "weekly-notes/week-02-notes.html#key-concepts-learned",
    "title": "Week 2 Notes",
    "section": "",
    "text": "I learned about topics on algorithmic decision making and what is census data.\nWe also went through a scenarios practices where we designed the ethical algorithms.\nThe importance of having algorithmic decision making.\nCensus data is really large but it has a lot of potential to do some great things."
  },
  {
    "objectID": "weekly-notes/week-02-notes.html#coding-techniques",
    "href": "weekly-notes/week-02-notes.html#coding-techniques",
    "title": "Week 2 Notes",
    "section": "Coding Techniques",
    "text": "Coding Techniques\n\nFunctions that I learned include “str_remove(), str_extract(), str_replace(), case_when(),"
  },
  {
    "objectID": "weekly-notes/week-02-notes.html#questions-challenges",
    "href": "weekly-notes/week-02-notes.html#questions-challenges",
    "title": "Week 2 Notes",
    "section": "Questions & Challenges",
    "text": "Questions & Challenges\n\nI think it will take some time to fully understand how to best use the Census Tract. I think it will take a lot of trials and error. The more pract the better we will get at it."
  },
  {
    "objectID": "weekly-notes/week-02-notes.html#connections-to-policy",
    "href": "weekly-notes/week-02-notes.html#connections-to-policy",
    "title": "Week 2 Notes",
    "section": "Connections to Policy",
    "text": "Connections to Policy\n\nIt is important to understand why we want to do algorthimic decision making in policy.\nData analytics is a subjective process because it involves human decisions. So we need to know to be best clean the data and understand the data."
  },
  {
    "objectID": "weekly-notes/week-02-notes.html#reflection",
    "href": "weekly-notes/week-02-notes.html#reflection",
    "title": "Week 2 Notes",
    "section": "Reflection",
    "text": "Reflection\n\nLooking forward to explore the census data more and to see what insightful result we could get. Also really looking forward to the data update in the end of this year. To see how status have changed for people."
  },
  {
    "objectID": "weekly-notes/week-04-notes.html",
    "href": "weekly-notes/week-04-notes.html",
    "title": "Week 4 Notes",
    "section": "",
    "text": "We learned about spatial data in R. We talked about vector data model and different type of spatial data to use in R. We also talked about some spatial data operation of how to filter and select the data that we need. We also mentioned some corrdinate reference system and how each is created."
  },
  {
    "objectID": "weekly-notes/week-04-notes.html#key-concepts-learned",
    "href": "weekly-notes/week-04-notes.html#key-concepts-learned",
    "title": "Week 4 Notes",
    "section": "",
    "text": "We learned about spatial data in R. We talked about vector data model and different type of spatial data to use in R. We also talked about some spatial data operation of how to filter and select the data that we need. We also mentioned some corrdinate reference system and how each is created."
  },
  {
    "objectID": "weekly-notes/week-04-notes.html#coding-techniques",
    "href": "weekly-notes/week-04-notes.html#coding-techniques",
    "title": "Week 4 Notes",
    "section": "Coding Techniques",
    "text": "Coding Techniques\n\nBelow are some of the core techniques that we covered in class and used in class. | st_filter() | Select features by spatial relationship | Find neighboring counties | | st_buffer() | Create zones around features | Hospital service areas | | st_intersects() | Test spatial overlap | Check access to services | | st_disjoint() | Test spatial separation | Find rural areas | | st_join() | Join by location | Add county info to tracts | | st_union() | Combine geometries | Merge overlapping buffers | | st_intersection() | Clip geometries | Calculate overlap areas | | st_transform() | Change CRS | Accurate distance/area calculations | | st_area() | Calculate areas | County sizes, coverage | | st_distance() | Calculate distances | Distance to facilities |"
  },
  {
    "objectID": "weekly-notes/week-04-notes.html#questions-challenges",
    "href": "weekly-notes/week-04-notes.html#questions-challenges",
    "title": "Week 4 Notes",
    "section": "Questions & Challenges",
    "text": "Questions & Challenges\n\nI think it is very important to understand what kind of spatial relationship you want to explore. That determines what types of function you need to explore relationships. ‘st_filter()’ and ‘st_intersection()’ might seem confusing at first but they are serving differe kinds of operation."
  },
  {
    "objectID": "weekly-notes/week-04-notes.html#connections-to-policy",
    "href": "weekly-notes/week-04-notes.html#connections-to-policy",
    "title": "Week 4 Notes",
    "section": "Connections to Policy",
    "text": "Connections to Policy\n\nThese operations are heavily related to Policy. Policy decision need spatial analysis to fully understand the root issues."
  },
  {
    "objectID": "weekly-notes/week-04-notes.html#reflection",
    "href": "weekly-notes/week-04-notes.html#reflection",
    "title": "Week 4 Notes",
    "section": "Reflection",
    "text": "Reflection\n\nBeing able to master the spatial analysis and GIS in R will unlock great potential in coming up with policy decision and more in-depth analysis for urban issues."
  },
  {
    "objectID": "labs/Zhang_Ye_Assignment2.html",
    "href": "labs/Zhang_Ye_Assignment2.html",
    "title": "Assignment 2: Spatial Analysis and Visualization",
    "section": "",
    "text": "Learning Objectives: - Apply spatial operations to answer policy-relevant research questions - Integrate census demographic data with spatial analysis - Create publication-quality visualizations and maps - Work with spatial data from multiple sources - Communicate findings effectively for policy audiences"
  },
  {
    "objectID": "labs/Zhang_Ye_Assignment2.html#assignment-overview",
    "href": "labs/Zhang_Ye_Assignment2.html#assignment-overview",
    "title": "Assignment 2: Spatial Analysis and Visualization",
    "section": "",
    "text": "Learning Objectives: - Apply spatial operations to answer policy-relevant research questions - Integrate census demographic data with spatial analysis - Create publication-quality visualizations and maps - Work with spatial data from multiple sources - Communicate findings effectively for policy audiences"
  },
  {
    "objectID": "labs/Zhang_Ye_Assignment2.html#part-1-healthcare-access-for-vulnerable-populations",
    "href": "labs/Zhang_Ye_Assignment2.html#part-1-healthcare-access-for-vulnerable-populations",
    "title": "Assignment 2: Spatial Analysis and Visualization",
    "section": "Part 1: Healthcare Access for Vulnerable Populations",
    "text": "Part 1: Healthcare Access for Vulnerable Populations\n\nResearch Question\nWhich Pennsylvania counties have the highest proportion of vulnerable populations (elderly + low-income) living far from hospitals?\n\nStep 1: Data Collection (5 points)\n\n# Load required packages\nlibrary(sf)\nlibrary(tidyverse)\nlibrary(tigris)\nlibrary(tidycensus)\nlibrary(scales)\nlibrary(patchwork)\nlibrary(here)\n\ncensus_api_key(\"138db656646183a18ae51e4a7c1e1f5cd64d4b40\")\ninstall = TRUE\n\n# Load spatial data\n\npa_counties &lt;- st_read(here(\"~/Documents/GitHub/portfolio-setup-yezhanggg/labs/assignment_2_data/Pennsylvania_County_Boundaries.shp\"))\n\nReading layer `Pennsylvania_County_Boundaries' from data source \n  `/Users/ye/Documents/GitHub/portfolio-setup-yezhanggg/labs/assignment_2_data/Pennsylvania_County_Boundaries.shp' \n  using driver `ESRI Shapefile'\nSimple feature collection with 67 features and 19 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: -8963377 ymin: 4825316 xmax: -8314404 ymax: 5201413\nProjected CRS: WGS 84 / Pseudo-Mercator\n\nhospitals &lt;- st_read(here(\"~/Documents/GitHub/portfolio-setup-yezhanggg/labs/assignment_2_data/hospitals.geojson\"))\n\nReading layer `hospitals' from data source \n  `/Users/ye/Documents/GitHub/portfolio-setup-yezhanggg/labs/assignment_2_data/hospitals.geojson' \n  using driver `GeoJSON'\nSimple feature collection with 223 features and 11 fields\nGeometry type: POINT\nDimension:     XY\nBounding box:  xmin: -80.49621 ymin: 39.75163 xmax: -74.86704 ymax: 42.13403\nGeodetic CRS:  WGS 84\n\ncensus_tracts &lt;- tracts(state = \"PA\", cb = TRUE)\n\n\n  |                                                                            \n  |                                                                      |   0%\n  |                                                                            \n  |======================================================================| 100%\n\n# Check that all data loaded correctly\n\nst_crs(pa_counties)\n\nCoordinate Reference System:\n  User input: WGS 84 / Pseudo-Mercator \n  wkt:\nPROJCRS[\"WGS 84 / Pseudo-Mercator\",\n    BASEGEOGCRS[\"WGS 84\",\n        ENSEMBLE[\"World Geodetic System 1984 ensemble\",\n            MEMBER[\"World Geodetic System 1984 (Transit)\"],\n            MEMBER[\"World Geodetic System 1984 (G730)\"],\n            MEMBER[\"World Geodetic System 1984 (G873)\"],\n            MEMBER[\"World Geodetic System 1984 (G1150)\"],\n            MEMBER[\"World Geodetic System 1984 (G1674)\"],\n            MEMBER[\"World Geodetic System 1984 (G1762)\"],\n            MEMBER[\"World Geodetic System 1984 (G2139)\"],\n            MEMBER[\"World Geodetic System 1984 (G2296)\"],\n            ELLIPSOID[\"WGS 84\",6378137,298.257223563,\n                LENGTHUNIT[\"metre\",1]],\n            ENSEMBLEACCURACY[2.0]],\n        PRIMEM[\"Greenwich\",0,\n            ANGLEUNIT[\"degree\",0.0174532925199433]],\n        ID[\"EPSG\",4326]],\n    CONVERSION[\"Popular Visualisation Pseudo-Mercator\",\n        METHOD[\"Popular Visualisation Pseudo Mercator\",\n            ID[\"EPSG\",1024]],\n        PARAMETER[\"Latitude of natural origin\",0,\n            ANGLEUNIT[\"degree\",0.0174532925199433],\n            ID[\"EPSG\",8801]],\n        PARAMETER[\"Longitude of natural origin\",0,\n            ANGLEUNIT[\"degree\",0.0174532925199433],\n            ID[\"EPSG\",8802]],\n        PARAMETER[\"False easting\",0,\n            LENGTHUNIT[\"metre\",1],\n            ID[\"EPSG\",8806]],\n        PARAMETER[\"False northing\",0,\n            LENGTHUNIT[\"metre\",1],\n            ID[\"EPSG\",8807]]],\n    CS[Cartesian,2],\n        AXIS[\"easting (X)\",east,\n            ORDER[1],\n            LENGTHUNIT[\"metre\",1]],\n        AXIS[\"northing (Y)\",north,\n            ORDER[2],\n            LENGTHUNIT[\"metre\",1]],\n    USAGE[\n        SCOPE[\"Web mapping and visualisation.\"],\n        AREA[\"World between 85.06°S and 85.06°N.\"],\n        BBOX[-85.06,-180,85.06,180]],\n    ID[\"EPSG\",3857]]\n\nst_crs(hospitals)\n\nCoordinate Reference System:\n  User input: WGS 84 \n  wkt:\nGEOGCRS[\"WGS 84\",\n    DATUM[\"World Geodetic System 1984\",\n        ELLIPSOID[\"WGS 84\",6378137,298.257223563,\n            LENGTHUNIT[\"metre\",1]]],\n    PRIMEM[\"Greenwich\",0,\n        ANGLEUNIT[\"degree\",0.0174532925199433]],\n    CS[ellipsoidal,2],\n        AXIS[\"geodetic latitude (Lat)\",north,\n            ORDER[1],\n            ANGLEUNIT[\"degree\",0.0174532925199433]],\n        AXIS[\"geodetic longitude (Lon)\",east,\n            ORDER[2],\n            ANGLEUNIT[\"degree\",0.0174532925199433]],\n    ID[\"EPSG\",4326]]\n\nst_crs(census_tracts)\n\nCoordinate Reference System:\n  User input: NAD83 \n  wkt:\nGEOGCRS[\"NAD83\",\n    DATUM[\"North American Datum 1983\",\n        ELLIPSOID[\"GRS 1980\",6378137,298.257222101,\n            LENGTHUNIT[\"metre\",1]]],\n    PRIMEM[\"Greenwich\",0,\n        ANGLEUNIT[\"degree\",0.0174532925199433]],\n    CS[ellipsoidal,2],\n        AXIS[\"latitude\",north,\n            ORDER[1],\n            ANGLEUNIT[\"degree\",0.0174532925199433]],\n        AXIS[\"longitude\",east,\n            ORDER[2],\n            ANGLEUNIT[\"degree\",0.0174532925199433]],\n    ID[\"EPSG\",4269]]\n\n\nQuestions to answer: - How many hospitals are in your dataset? There aer 223 hosptials in my dataset. - How many census tracts? There are 3445 census tracts. - What coordinate reference system is each dataset in? PA Counties is in WGS 84 Pseudo Mercator, Hospitals is in WGS 84 and Census Tracts is in NAD83.\n\n\n\nStep 2: Get Demographic Data\n\n# Get demographic data from ACS\nacs_vars_2022 &lt;- load_variables(2022, \"acs5\", cache = TRUE)\n\n# Get demographic data from ACS\n\npa_demographics &lt;- get_acs(\n  geography = \"tract\",\n  state = \"PA\",\n  variables = c(\n    total_pop = \"B01003_001\",     \n    median_income = \"B19013_001\",   \n    male_65_66 = \"B01001_020\",\n    male_67_69 = \"B01001_021\",\n    male_70_74 = \"B01001_022\",\n    male_75_79 = \"B01001_023\",\n    male_80_84 = \"B01001_024\",\n    male_85_over = \"B01001_025\",\n    female_65_66 = \"B01001_044\",\n    female_67_69 = \"B01001_045\",\n    female_70_74 = \"B01001_046\",\n    female_75_79 = \"B01001_047\",\n    female_80_84 = \"B01001_048\",\n    female_85_over = \"B01001_049\"\n  ),\n  year = 2022,\n  survey = \"acs5\",\n  output = \"wide\",\n  geometry = FALSE\n)\n\npa_demographics &lt;- pa_demographics %&gt;%\n  mutate(\n    pop_65_over = male_65_66E + male_67_69E + male_70_74E + male_75_79E + \n                  male_80_84E + male_85_overE + \n                  female_65_66E + female_67_69E + female_70_74E + \n                  female_75_79E + female_80_84E + female_85_overE,\n    total_pop = total_popE,\n    median_income = median_incomeE\n  ) %&gt;%\n select(GEOID, total_pop, median_income, pop_65_over)\n\n# Join to tract boundaries\n\ncensus_tracts &lt;- census_tracts %&gt;%\n  left_join(pa_demographics, by = \"GEOID\")\n\nQuestions to answer: - What year of ACS data are you using? I am using ACS 2022 data. - How many tracts have missing income data? There are 62 tracts missing income data. - What is the median income across all PA census tracts? The median household income is $70,188 for all PA Census Tracts.\n\n\n\nStep 3: Define Vulnerable Populations\n\n# Filter for vulnerable tracts based on your criteria\n\nvulnerable_tracts &lt;- census_tracts %&gt;%\n  mutate(pct_elderly = (pop_65_over / total_pop) * 100) %&gt;%\n  filter(median_income &lt; 40000 & pct_elderly &gt; 21)\n\nQuestions to answer: - What income threshold did you choose and why? The income threshold that I chose is 40000. Through research, the poverty guideline in 2024 for a family of four is 31,200.The median household income in PA is $76,081 in 2023. Older population might be more vulnerable and it would be reasonable to adjust threshold higher. It would be reasonable to use this as the guideline to set for the low median household income threshold. - What elderly population threshold did you choose and why? The elderly population threhold that I choose is 21%. Because after reviewing the acs data, there are 14 different age groups and there are three age groups that include age over 65. - How many tracts meet your vulnerability criteria? There are 59 vulnerable tracts that meet my standard of below 40,000 income and over 21% are elderly people. - What percentage of PA census tracts are considered vulnerable by your definition? 1.7% of the PA census tracts are considered as vulnerable by my definition.\n\n\n\nStep 4: Calculate Distance to Hospitals\n\n# Transform to appropriate projected CRS\npa_counties &lt;- pa_counties %&gt;% st_transform(3365)\nhospitals &lt;- hospitals %&gt;% st_transform(3365)\ncensus_tracts &lt;- census_tracts %&gt;% st_transform(3365)\nvulnerable_tracts &lt;- vulnerable_tracts %&gt;% st_transform(3365)\n\n# Calculate distance from each tract centroid to nearest hospital\ntract_centers &lt;- st_centroid(vulnerable_tracts) \ndistance_matrix &lt;- st_distance(tract_centers, hospitals)\n\nmin_distances &lt;- apply(distance_matrix, 1, min)\n\nvulnerable_tracts &lt;- vulnerable_tracts %&gt;%\n  mutate(\n    distance_to_nearest_hospital_m = as.numeric(min_distances),\n    distance_to_nearest_hospital_miles = distance_to_nearest_hospital_m / 1609.34\n  )\n\nQuestions to answer: - What is the average distance to the nearest hospital for vulnerable tracts? The average distance is 7.4093 miles. - What is the maximum distance? The max distance is 61.2651 miles - How many vulnerable tracts are more than 15 miles from the nearest hospital? Based on my standards. There are 4 vulnerable tracts that are more than 15 miles from the nearest hospital.\n\nst_crs(vulnerable_tracts)$units\n\n[1] \"us-ft\"\n\nsummary(vulnerable_tracts)\n\n   STATEFP            COUNTYFP           TRACTCE            GEOIDFQ         \n Length:59          Length:59          Length:59          Length:59         \n Class :character   Class :character   Class :character   Class :character  \n Mode  :character   Mode  :character   Mode  :character   Mode  :character  \n                                                                            \n                                                                            \n                                                                            \n    GEOID               NAME             NAMELSAD            STUSPS         \n Length:59          Length:59          Length:59          Length:59         \n Class :character   Class :character   Class :character   Class :character  \n Mode  :character   Mode  :character   Mode  :character   Mode  :character  \n                                                                            \n                                                                            \n                                                                            \n  NAMELSADCO         STATE_NAME            LSAD               ALAND        \n Length:59          Length:59          Length:59          Min.   : 262958  \n Class :character   Class :character   Class :character   1st Qu.: 692938  \n Mode  :character   Mode  :character   Mode  :character   Median :1153118  \n                                                          Mean   :1782766  \n                                                          3rd Qu.:1827787  \n                                                          Max.   :9908475  \n     AWATER         total_pop    median_income    pop_65_over    \n Min.   :     0   Min.   :  19   Min.   :12336   Min.   :   9.0  \n 1st Qu.:     0   1st Qu.:1600   1st Qu.:24081   1st Qu.: 458.5  \n Median :     0   Median :2373   Median :33771   Median : 599.0  \n Mean   : 51871   Mean   :2456   Mean   :30485   Mean   : 658.7  \n 3rd Qu.: 36576   3rd Qu.:2922   3rd Qu.:37470   3rd Qu.: 787.0  \n Max.   :476215   Max.   :5197   Max.   :39891   Max.   :1649.0  \n          geometry   pct_elderly    distance_to_nearest_hospital_m\n MULTIPOLYGON :59   Min.   :21.04   Min.   :  312.1               \n epsg:3365    : 0   1st Qu.:22.80   1st Qu.: 4124.4               \n +proj=lcc ...: 0   Median :25.50   Median : 7340.2               \n                    Mean   :27.47   Mean   :11924.0               \n                    3rd Qu.:31.17   3rd Qu.:14265.6               \n                    Max.   :47.37   Max.   :98596.3               \n distance_to_nearest_hospital_miles\n Min.   : 0.1939                   \n 1st Qu.: 2.5628                   \n Median : 4.5610                   \n Mean   : 7.4093                   \n 3rd Qu.: 8.8643                   \n Max.   :61.2651                   \n\nsum(vulnerable_tracts$distance_to_nearest_hospital_miles &gt; 15)\n\n[1] 4\n\n\n\n\n\nStep 5: Identify Underserved Areas\n\n# Create underserved variable\n\nvulnerable_tracts &lt;- vulnerable_tracts %&gt;%\n  mutate(underserved = distance_to_nearest_hospital_miles &gt; 15)\n\nQuestions to answer: - How many tracts are underserved? There are 4 tracts that are underserved. - What percentage of vulnerable tracts are underserved? The percentage for vulnerable tracts are underserved is 6.78%. - Does this surprise you? Why or why not? This percentage is little bit lower than I expected. I was expected the vulnerable tracts that are underserved might takes up a larger percentage. It might be how I designed what is considered as a vulnerable tracts.\n\n\n\nStep 6: Aggregate to County Level\n\n# Spatial join tracts to counties\n\ntracts_with_counties &lt;- vulnerable_tracts %&gt;%\n  st_join(pa_counties %&gt;% select(COUNTY_NAM))\n\n# Aggregate statistics by county\n\nvulnerable_by_county &lt;- tracts_with_counties %&gt;%\n  st_drop_geometry() %&gt;%\n  group_by(COUNTY_NAM) %&gt;%\n  summarize(\n    n_vulnerable_tracts = n(),\n    n_underserved_tracts = sum(underserved == TRUE, na.rm = TRUE),\n    pct_vulnerable_underserved = (sum(underserved == TRUE, na.rm = TRUE) / n()) * 100,\n    avg_distance_to_hospital = mean(distance_to_nearest_hospital_miles, na.rm = TRUE),\n    total_vulnerable_pop = sum(total_pop, na.rm = TRUE)\n  ) %&gt;%\n  arrange(desc(n_vulnerable_tracts))\n\n\nvulnerable_by_county %&gt;%\n  arrange(desc(pct_vulnerable_underserved)) %&gt;%\n  head(5)\n\n# A tibble: 5 × 6\n  COUNTY_NAM     n_vulnerable_tracts n_underserved_tracts pct_vulnerable_under…¹\n  &lt;chr&gt;                        &lt;int&gt;                &lt;int&gt;                  &lt;dbl&gt;\n1 CAMERON                          1                    1                 100   \n2 NORTHUMBERLAND                   1                    1                 100   \n3 CAMBRIA                          4                    1                  25   \n4 ALLEGHENY                       18                    1                   5.56\n5 PHILADELPHIA                    10                    0                   0   \n# ℹ abbreviated name: ¹​pct_vulnerable_underserved\n# ℹ 2 more variables: avg_distance_to_hospital &lt;dbl&gt;,\n#   total_vulnerable_pop &lt;dbl&gt;\n\nvulnerable_by_county %&gt;%\n  filter(n_underserved_tracts &gt; 0) %&gt;%\n  arrange(desc(total_vulnerable_pop)) %&gt;%\n  head(10)\n\n# A tibble: 4 × 6\n  COUNTY_NAM     n_vulnerable_tracts n_underserved_tracts pct_vulnerable_under…¹\n  &lt;chr&gt;                        &lt;int&gt;                &lt;int&gt;                  &lt;dbl&gt;\n1 ALLEGHENY                       18                    1                   5.56\n2 CAMBRIA                          4                    1                  25   \n3 NORTHUMBERLAND                   1                    1                 100   \n4 CAMERON                          1                    1                 100   \n# ℹ abbreviated name: ¹​pct_vulnerable_underserved\n# ℹ 2 more variables: avg_distance_to_hospital &lt;dbl&gt;,\n#   total_vulnerable_pop &lt;dbl&gt;\n\n\nQuestions to answer: - Which 5 counties have the highest percentage of underserved vulnerable tracts? Based on my threshold, the top 4 counties with highest percentage of underserved vulnerable tracts. - Which counties have the most vulnerable people living far from hospitals? Allegheny has the most vulnerable people living far from hospitals. - Are there any patterns in where underserved counties are located? Underserved counties tends to also have higher vulnerable tracts and higher average distance to hospital.\n\n\n\nStep 7: Create Summary Table\n\n# Create and format priority counties table\n\nlibrary(knitr)\nlibrary(kableExtra)\n\npriority_counties &lt;- vulnerable_by_county %&gt;%\n  arrange(desc(pct_vulnerable_underserved)) %&gt;%\n  head(10) %&gt;%\n  select(\n    COUNTY_NAM,\n    n_vulnerable_tracts,\n    n_underserved_tracts,\n    pct_vulnerable_underserved,\n    avg_distance_to_hospital,\n    total_vulnerable_pop\n  )\n\npriority_counties %&gt;%\n  kable(\n    col.names = c(\n      \"County\",\n      \"Vulnerable Tracts\",\n      \"Underserved Tracts\",\n      \"% Underserved\",\n      \"Avg Distance (miles)\",\n      \"Vulnerable Population\"\n    ),\n    caption = \"Top 10 Priority Counties for Healthcare Investment: Highest Percentage of Underserved Vulnerable Tracts\",\n    digits = c(0, 0, 0, 1, 1, 0),\n    format.args = list(big.mark = \",\")\n  ) %&gt;%\n  kable_styling(bootstrap_options = c(\"striped\", \"hover\"))\n\n\nTop 10 Priority Counties for Healthcare Investment: Highest Percentage of Underserved Vulnerable Tracts\n\n\nCounty\nVulnerable Tracts\nUnderserved Tracts\n% Underserved\nAvg Distance (miles)\nVulnerable Population\n\n\n\n\nCAMERON\n1\n1\n100.0\n61.3\n1,988\n\n\nNORTHUMBERLAND\n1\n1\n100.0\n31.2\n2,350\n\n\nCAMBRIA\n4\n1\n25.0\n11.6\n8,094\n\n\nALLEGHENY\n18\n1\n5.6\n8.0\n37,378\n\n\nPHILADELPHIA\n10\n0\n0.0\n2.9\n36,700\n\n\nWESTMORELAND\n5\n0\n0.0\n8.1\n8,145\n\n\nERIE\n3\n0\n0.0\n1.4\n5,320\n\n\nLUZERNE\n3\n0\n0.0\n5.9\n8,633\n\n\nMERCER\n3\n0\n0.0\n4.6\n6,560\n\n\nBEAVER\n2\n0\n0.0\n13.8\n3,570"
  },
  {
    "objectID": "labs/Zhang_Ye_Assignment2.html#part-2-comprehensive-visualization",
    "href": "labs/Zhang_Ye_Assignment2.html#part-2-comprehensive-visualization",
    "title": "Assignment 2: Spatial Analysis and Visualization",
    "section": "Part 2: Comprehensive Visualization",
    "text": "Part 2: Comprehensive Visualization\n\nMap 1: County-Level Choropleth\n\n# Create county-level access map\n\nlibrary(scales)\n\ncounties_with_stats &lt;- pa_counties %&gt;%\n  left_join(vulnerable_by_county, by = \"COUNTY_NAM\")\n\nggplot() +\n  geom_sf(data = counties_with_stats, \n          aes(fill = pct_vulnerable_underserved), \n          color = \"white\", \n          size = 0.5) +\n  geom_sf(data = hospitals, \n          color = \"red\", \n          size = 1.5, \n          alpha = 0.6) +\n  scale_fill_viridis_c(\n    name = \"% Vulnerable\\nTracts\\nUnderserved\",\n    labels = comma,\n    option = \"plasma\",\n    na.value = \"lightgray\"\n  ) +\n  labs(\n    title = \"Healthcare Access Challenges in Pennsylvania Counties\",\n    subtitle = \"Percentage of vulnerable tracts located far from hospitals\",\n    caption = \"Source: ACS 2022 | Red points = Hospital locations\"\n  ) +\n  theme_void() +\n  theme(\n    plot.title = element_text(face = \"bold\", size = 14),\n    plot.subtitle = element_text(size = 11),\n    legend.position = \"right\"\n  )\n\n\n\n\n\n\n\n\n\n\n\nMap 2: Detailed Vulnerability Map\n\n# Create detailed tract-level map\n\nallegheny &lt;- pa_counties %&gt;%\n  filter(COUNTY_NAM == \"ALLEGHENY\")\n\nallegheny_tracts &lt;- vulnerable_tracts %&gt;%\n  st_filter(allegheny)\n\nallegheny_hospitals &lt;- hospitals %&gt;%\n  st_filter(allegheny)\n\nggplot() +\n  geom_sf(data = allegheny, \n          fill = \"white\", \n          color = \"black\", \n          size = 1) +\n  geom_sf(data = allegheny_tracts %&gt;% filter(underserved == FALSE),\n          fill = \"blue\", \n          color = \"gray80\",\n          size = 0.3,\n          alpha = 0.6) +\n  geom_sf(data = allegheny_tracts %&gt;% filter(underserved == TRUE),\n          fill = \"red\", \n          color = \"gray80\",\n          size = 0.3,\n          alpha = 0.7) +\n  geom_sf(data = allegheny_hospitals, \n          color = \"lightgreen\", \n          size = 2, \n          shape = 17) +\n  labs(\n    title = \"Underserved Vulnerable Populations in Allegheny County\",\n    subtitle = \"Red = Underserved tracts | Blue = Vulnerable tracts | Triangles = Hospitals\"\n  ) +\n  theme_void()\n\n\n\n\n\n\n\n\n\n\n\nChart: Distribution Analysis of distances to hospitals for vulnerable populations.\n\n# Create distribution visualization\n\nggplot(vulnerable_tracts, aes(x = distance_to_nearest_hospital_miles)) +\n  geom_histogram(bins = 20, fill = \"steelblue\", alpha = 0.7, color = \"white\") +\n  geom_vline(xintercept = 15, color = \"red\", linetype = \"dashed\", size = 1) +\n  labs(\n    title = \"Distribution of Distance to Nearest Hospital\",\n    subtitle = \"For vulnerable census tracts in Pennsylvania\",\n    x = \"Distance to Nearest Hospital (miles)\",\n    y = \"Number of Vulnerable Tracts\",\n    caption = \"Red dashed line = 15-mile threshold for underserved status. \n    Most vulnerable tracts are within 15 miles of a hospital, but a concerning number remain beyond this threshold.\"\n  ) +\n  theme_minimal() +\n  scale_x_continuous(labels = comma)"
  },
  {
    "objectID": "labs/Zhang_Ye_Assignment2.html#part-3-bring-your-own-data-analysis",
    "href": "labs/Zhang_Ye_Assignment2.html#part-3-bring-your-own-data-analysis",
    "title": "Assignment 2: Spatial Analysis and Visualization",
    "section": "Part 3: Bring Your Own Data Analysis",
    "text": "Part 3: Bring Your Own Data Analysis\n\nYour Analysis\nOption B: School Safety Zones - Data: Schools, Crime Incidents, Bike Network - Question: “Are school zones safe for walking/biking, or are they crime hotspots?” - Operations: Buffer schools (500m safety zone), spatial join with crime incidents, assess bike infrastructure coverage - Policy relevance: Safe Routes to School programs, crossing guard placement\nYour Task:\n\nFind and load additional data\n\n\n# Load your additional dataset\n\n\nschools &lt;- st_read(here(\"~/Documents/GitHub/portfolio-setup-yezhanggg/labs/assignment_2_data/Challenege/Schools/Schools.shp\"))\n\nReading layer `Schools' from data source \n  `/Users/ye/Documents/GitHub/portfolio-setup-yezhanggg/labs/assignment_2_data/Challenege/Schools/Schools.shp' \n  using driver `ESRI Shapefile'\nSimple feature collection with 495 features and 14 fields\nGeometry type: POINT\nDimension:     XY\nBounding box:  xmin: -8378628 ymin: 4852555 xmax: -8345686 ymax: 4884813\nProjected CRS: WGS 84 / Pseudo-Mercator\n\ncrime &lt;- st_read(here(\"~/Documents/GitHub/portfolio-setup-yezhanggg/labs/assignment_2_data/Challenege/incidents_part1_part2/incidents_part1_part2.shp\"))\n\nReading layer `incidents_part1_part2' from data source \n  `/Users/ye/Documents/GitHub/portfolio-setup-yezhanggg/labs/assignment_2_data/Challenege/incidents_part1_part2/incidents_part1_part2.shp' \n  using driver `ESRI Shapefile'\nreplacing null geometries with empty geometries\nSimple feature collection with 169017 features and 13 fields (with 6985 geometries empty)\nGeometry type: POINT\nDimension:     XY\nBounding box:  xmin: -75.2753 ymin: 39.87465 xmax: -74.95761 ymax: 40.13762\nGeodetic CRS:  WGS 84\n\nbike &lt;- st_read(here(\"~/Documents/GitHub/portfolio-setup-yezhanggg/labs/assignment_2_data/Challenege/Bike_Network/Bike_Network.shp\"))\n\nReading layer `Bike_Network' from data source \n  `/Users/ye/Documents/GitHub/portfolio-setup-yezhanggg/labs/assignment_2_data/Challenege/Bike_Network/Bike_Network.shp' \n  using driver `ESRI Shapefile'\nSimple feature collection with 5225 features and 8 fields\nGeometry type: LINESTRING\nDimension:     XY\nBounding box:  xmin: -8378937 ymin: 4847835 xmax: -8345146 ymax: 4883978\nProjected CRS: WGS 84 / Pseudo-Mercator\n\n\n\nst_crs(schools)\n\nCoordinate Reference System:\n  User input: WGS 84 / Pseudo-Mercator \n  wkt:\nPROJCRS[\"WGS 84 / Pseudo-Mercator\",\n    BASEGEOGCRS[\"WGS 84\",\n        ENSEMBLE[\"World Geodetic System 1984 ensemble\",\n            MEMBER[\"World Geodetic System 1984 (Transit)\"],\n            MEMBER[\"World Geodetic System 1984 (G730)\"],\n            MEMBER[\"World Geodetic System 1984 (G873)\"],\n            MEMBER[\"World Geodetic System 1984 (G1150)\"],\n            MEMBER[\"World Geodetic System 1984 (G1674)\"],\n            MEMBER[\"World Geodetic System 1984 (G1762)\"],\n            MEMBER[\"World Geodetic System 1984 (G2139)\"],\n            MEMBER[\"World Geodetic System 1984 (G2296)\"],\n            ELLIPSOID[\"WGS 84\",6378137,298.257223563,\n                LENGTHUNIT[\"metre\",1]],\n            ENSEMBLEACCURACY[2.0]],\n        PRIMEM[\"Greenwich\",0,\n            ANGLEUNIT[\"degree\",0.0174532925199433]],\n        ID[\"EPSG\",4326]],\n    CONVERSION[\"Popular Visualisation Pseudo-Mercator\",\n        METHOD[\"Popular Visualisation Pseudo Mercator\",\n            ID[\"EPSG\",1024]],\n        PARAMETER[\"Latitude of natural origin\",0,\n            ANGLEUNIT[\"degree\",0.0174532925199433],\n            ID[\"EPSG\",8801]],\n        PARAMETER[\"Longitude of natural origin\",0,\n            ANGLEUNIT[\"degree\",0.0174532925199433],\n            ID[\"EPSG\",8802]],\n        PARAMETER[\"False easting\",0,\n            LENGTHUNIT[\"metre\",1],\n            ID[\"EPSG\",8806]],\n        PARAMETER[\"False northing\",0,\n            LENGTHUNIT[\"metre\",1],\n            ID[\"EPSG\",8807]]],\n    CS[Cartesian,2],\n        AXIS[\"easting (X)\",east,\n            ORDER[1],\n            LENGTHUNIT[\"metre\",1]],\n        AXIS[\"northing (Y)\",north,\n            ORDER[2],\n            LENGTHUNIT[\"metre\",1]],\n    USAGE[\n        SCOPE[\"Web mapping and visualisation.\"],\n        AREA[\"World between 85.06°S and 85.06°N.\"],\n        BBOX[-85.06,-180,85.06,180]],\n    ID[\"EPSG\",3857]]\n\nst_crs(crime)\n\nCoordinate Reference System:\n  User input: WGS 84 \n  wkt:\nGEOGCRS[\"WGS 84\",\n    DATUM[\"World Geodetic System 1984\",\n        ELLIPSOID[\"WGS 84\",6378137,298.257223563,\n            LENGTHUNIT[\"metre\",1]]],\n    PRIMEM[\"Greenwich\",0,\n        ANGLEUNIT[\"degree\",0.0174532925199433]],\n    CS[ellipsoidal,2],\n        AXIS[\"latitude\",north,\n            ORDER[1],\n            ANGLEUNIT[\"degree\",0.0174532925199433]],\n        AXIS[\"longitude\",east,\n            ORDER[2],\n            ANGLEUNIT[\"degree\",0.0174532925199433]],\n    ID[\"EPSG\",4326]]\n\nst_crs(bike)\n\nCoordinate Reference System:\n  User input: WGS 84 / Pseudo-Mercator \n  wkt:\nPROJCRS[\"WGS 84 / Pseudo-Mercator\",\n    BASEGEOGCRS[\"WGS 84\",\n        ENSEMBLE[\"World Geodetic System 1984 ensemble\",\n            MEMBER[\"World Geodetic System 1984 (Transit)\"],\n            MEMBER[\"World Geodetic System 1984 (G730)\"],\n            MEMBER[\"World Geodetic System 1984 (G873)\"],\n            MEMBER[\"World Geodetic System 1984 (G1150)\"],\n            MEMBER[\"World Geodetic System 1984 (G1674)\"],\n            MEMBER[\"World Geodetic System 1984 (G1762)\"],\n            MEMBER[\"World Geodetic System 1984 (G2139)\"],\n            MEMBER[\"World Geodetic System 1984 (G2296)\"],\n            ELLIPSOID[\"WGS 84\",6378137,298.257223563,\n                LENGTHUNIT[\"metre\",1]],\n            ENSEMBLEACCURACY[2.0]],\n        PRIMEM[\"Greenwich\",0,\n            ANGLEUNIT[\"degree\",0.0174532925199433]],\n        ID[\"EPSG\",4326]],\n    CONVERSION[\"Popular Visualisation Pseudo-Mercator\",\n        METHOD[\"Popular Visualisation Pseudo Mercator\",\n            ID[\"EPSG\",1024]],\n        PARAMETER[\"Latitude of natural origin\",0,\n            ANGLEUNIT[\"degree\",0.0174532925199433],\n            ID[\"EPSG\",8801]],\n        PARAMETER[\"Longitude of natural origin\",0,\n            ANGLEUNIT[\"degree\",0.0174532925199433],\n            ID[\"EPSG\",8802]],\n        PARAMETER[\"False easting\",0,\n            LENGTHUNIT[\"metre\",1],\n            ID[\"EPSG\",8806]],\n        PARAMETER[\"False northing\",0,\n            LENGTHUNIT[\"metre\",1],\n            ID[\"EPSG\",8807]]],\n    CS[Cartesian,2],\n        AXIS[\"easting (X)\",east,\n            ORDER[1],\n            LENGTHUNIT[\"metre\",1]],\n        AXIS[\"northing (Y)\",north,\n            ORDER[2],\n            LENGTHUNIT[\"metre\",1]],\n    USAGE[\n        SCOPE[\"Web mapping and visualisation.\"],\n        AREA[\"World between 85.06°S and 85.06°N.\"],\n        BBOX[-85.06,-180,85.06,180]],\n    ID[\"EPSG\",3857]]\n\nschools &lt;- st_transform(schools, 3365)\ncrime &lt;- st_transform(crime, 3365)\nbike &lt;- st_transform(bike, 3365)\n\nnrow(schools)\n\n[1] 495\n\nnrow(crime)\n\n[1] 169017\n\nnrow(bike)\n\n[1] 5225\n\nplot(st_geometry(schools))\n\n\n\n\n\n\n\nplot(st_geometry(crime))\n\n\n\n\n\n\n\nplot(st_geometry(bike))\n\n\n\n\n\n\n\n\nQuestions to answer: - What dataset did you choose and why? I choose schools, bike network and crime incidents to look school safety topics. - What is the data source and date? All of the data are downloaded from OpenDataPhilly. Crime incidents data is 2025 and bike network and schools don’t have their date included. - How many features does it contain? There are 495 features for schoos, 169017 features for crime incidents, and 5225 features for bike network. - What CRS is it in? Did you need to transform it? I transformed all of the data into EPSG:3365 for their CRS because it is an optimal option for Philadelphia. —\n\nPose a research question\n\nAre school zones safe for walking/biking, or are they crime hotspots?\n\n\nConduct spatial analysis\n\nUse at least TWO spatial operations to answer your research question.\n\n# Your spatial analysis\n\n# Operation 1: BUFFERS\n\n#find the buffers around school\nschools_buffer &lt;- schools %&gt;%\n  st_buffer(dist = 500)\n\n# Count crime incidents near each school\nschools_with_crime &lt;- schools %&gt;%\n  mutate(\n    crime_within_500m = lengths(st_intersects(schools_buffer, crime))\n  )\n\n# Summary statistics\ncat(\"Crime incidents near schools:\\n\")\n\nCrime incidents near schools:\n\ncat(\"Mean crimes per school:\", mean(schools_with_crime$crime_within_500m), \"\\n\")\n\nMean crimes per school: 60.67677 \n\ncat(\"Max crimes near one school:\", max(schools_with_crime$crime_within_500m), \"\\n\")\n\nMax crimes near one school: 490 \n\ncat(\"Schools with 10+ crimes nearby:\", \n    sum(schools_with_crime$crime_within_500m &gt;= 10), \"\\n\\n\")\n\nSchools with 10+ crimes nearby: 439 \n\ncat(\"% Schools with 10+ crimes nearby:\",\n    (sum(schools_with_crime$crime_within_500m &gt;= 10))/495, \"\\n\\n\")\n\n% Schools with 10+ crimes nearby: 0.8868687 \n\n# Operation 2: SPATIAL JOIN\n\n# Find schools within 200m of bike infrastructure\nschools_near_bikes &lt;- schools %&gt;%\n  st_filter(st_buffer(bike, 200), .predicate = st_intersects) %&gt;%\n  mutate(has_bike_access = TRUE)\n# Calculate percentage\npct_bike_access &lt;- (nrow(schools_near_bikes) / nrow(schools)) * 100\ncat(\"Schools with bike access:\", nrow(schools_near_bikes), \n    \"out of\", nrow(schools), \n    \"(\", round(pct_bike_access, 1), \"%)\")\n\nSchools with bike access: 120 out of 495 ( 24.2 %)\n\n# Create map\nggplot() +\n  geom_sf(data = bike, color = \"green\", size = 0.5, alpha = 1) +\n  geom_sf(data = crime, color = \"red\", size = 0.000000001, alpha = 0.01) +\n  geom_sf(data = schools_buffer, fill = \"yellow\", alpha = 0.8, color = NA) +\n  geom_sf(data = schools_with_crime, \n          aes(color = crime_within_500m), \n          size = 1) +\n  scale_color_viridis_c(\n    name = \"Crimes within 500m\",\n    option = \"plasma\"\n  ) +\n  labs(\n    title = \"School Safety Analysis: Crime Incidents and Bike Access\",\n    subtitle = \"Yellow zones = 500m school buffers | Green lines = Bike network\",\n    caption = \"Red points = Crime incidents\"\n  ) +\n  theme_void()\n\n\n\n\n\n\n\n\nYour interpretation:\nThe mean crime per school is around 60 and the max crime around school is 490 which is super high. There are 495 schools in total and 439 school has crimes over 10+ nearby. 88.7% of all school has crime nearby. 24.2% of the school has access to bike route. With the high percentage of crime present around school is not safe to bike around schools. There might be only a limited number of schools has the level of safety to allow students to bike around schools."
  },
  {
    "objectID": "labs/Zhang_Ye_Assignment2.html#finally---a-few-comments-about-your-incorporation-of-feedback",
    "href": "labs/Zhang_Ye_Assignment2.html#finally---a-few-comments-about-your-incorporation-of-feedback",
    "title": "Assignment 2: Spatial Analysis and Visualization",
    "section": "Finally - A few comments about your incorporation of feedback!",
    "text": "Finally - A few comments about your incorporation of feedback!\nRemove extra instruction text and only leave what is important and present the assignment like a presentation. No need to keep the instructions. Be sure to review everything."
  }
]