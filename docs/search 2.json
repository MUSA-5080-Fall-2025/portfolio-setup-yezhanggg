[
  {
    "objectID": "labs/presentation/presentation.html#project-intention",
    "href": "labs/presentation/presentation.html#project-intention",
    "title": "Predicting Housing Sales",
    "section": "Project Intention",
    "text": "Project Intention\nWhy it Matters\nPhiladelphia’s current Automated Valuation Model (AVM) is underperforming, leading to inconsistent and inequitable property assessments.\nOur Guiding Question\nWhat key predictors are missing from the legacy model, and how can we improve its accuracy and fairness?"
  },
  {
    "objectID": "labs/presentation/presentation.html#data-sources",
    "href": "labs/presentation/presentation.html#data-sources",
    "title": "Predicting Housing Sales",
    "section": "Data Sources",
    "text": "Data Sources\n\nCity of Philadelphia\n\nPhiladelphia Properties and Current Assessments (n= 21,807 Home Sales)\n\nOpenDataPhilly\n\nNeighborhood Boundaries (Polygon)\nSchool Locations, Attendance, and Withdrawals\n\nGeofabrik (OpenStreetMap)\n\nCommercial and Economic Activity Points\n\nU.S. Census (ACS 5-Year)\n\nEducation\nVacancy Rates\nFamily Household Share\nPercent White\nMedian Household Income"
  },
  {
    "objectID": "labs/presentation/presentation.html#mapped-observations",
    "href": "labs/presentation/presentation.html#mapped-observations",
    "title": "Predicting Housing Sales",
    "section": "Mapped Observations",
    "text": "Mapped Observations\n\n\n\n\n\nHome Price Across the City\n\n\n\nKey Findings\n\nPrices cluster spatially\n\nCenter City & Northwest Philadelphia show highest home values\n\nAdditional hotspots in South Philadelphia"
  },
  {
    "objectID": "labs/presentation/presentation.html#structural-price-drivers",
    "href": "labs/presentation/presentation.html#structural-price-drivers",
    "title": "Predicting Housing Sales",
    "section": "Structural Price Drivers",
    "text": "Structural Price Drivers"
  },
  {
    "objectID": "labs/presentation/presentation.html#progressive-regression-models",
    "href": "labs/presentation/presentation.html#progressive-regression-models",
    "title": "Predicting Housing Sales",
    "section": "Progressive Regression Models",
    "text": "Progressive Regression Models\nGoal: Test whether place-based factors improve predictive accuracy.\n\\[\n\\begin{aligned}\n\\log(\\text{Price}) = \\beta_0 &+ \\color{red}{\\beta_1 \\log(\\text{Livable Area}) + \\beta_2 \\text{Bedrooms} + \\beta_3 \\text{Bathrooms}} \\\\\n&+ \\color{red}{\\beta_4 \\text{Age} + \\beta_5 \\text{Interior Condition} + \\beta_6 \\text{Quality Grade}} \\\\\n&+ \\color{orange}{\\beta_7 \\text{Median Income} + \\beta_8 \\text{Family Household Ratio}}  \\\\\n&+ \\color{orange}{\\beta_9 \\text{% Bachelors}+ \\beta_{10} \\text{% Vacant} + \\beta_{11} \\text{% White}} \\\\\n&+ \\color{blue}{\\beta_{12} \\text{EconKDE} + \\beta_{13} \\text{EconKDE}^2 + \\beta_{14} \\text{TreeKDE} + \\beta_{15} \\text{Mean 3NN Dist}} \\\\\n&+ \\color{green}{\\beta_{16} \\text{MHI Quantile} + \\beta_{17} (\\text{MHI Quantile} \\times \\text{Age})} + \\varepsilon\n\\end{aligned}\n\\]\nModel Layers\n- Model 1: Structural\n- Model 2: + Demographics\n- Model 3: + Spatial\n- Model 4: + Interactions"
  },
  {
    "objectID": "labs/presentation/presentation.html#model-performance-improves-at-each-stage",
    "href": "labs/presentation/presentation.html#model-performance-improves-at-each-stage",
    "title": "Predicting Housing Sales",
    "section": "Model Performance Improves at Each Stage",
    "text": "Model Performance Improves at Each Stage\n\n\n\n\n\n\n\n\nModel\nAdj. R²\nInterpretation\n\n\n\n\n(1) Structural\n0.38\nHouse features alone are not enough\n\n\n(2) + Census\n0.60\nNeighborhood socioeconomic patterns matter\n\n\n(3) + Spatial\n0.61\nLocation and amenities add signal\n\n\n(4) + Interactions\n0.64\nWealthy areas shape how homes retain value\n\n\n\nTakeaway: Spatial + neighborhood effects are essential to explaining price variation in Philadelphia."
  },
  {
    "objectID": "labs/presentation/presentation.html#hardest-neighborhoods-to-predict",
    "href": "labs/presentation/presentation.html#hardest-neighborhoods-to-predict",
    "title": "Predicting Housing Sales",
    "section": "Hardest Neighborhoods to Predict",
    "text": "Hardest Neighborhoods to Predict\n\n\n\n\n\nProblem Neighborhoods\n\n\n\nKey Insight\n\nThe model struggles most in high-value, high-amenity neighborhoods\nUnder-predicted (red): Center City, Northwest Philly, Fishtown, South Philly\n→ Homes sell for more than the model expects\nOver-predicted (blue): Economically underserved neighborhoods\n→ Homes sell for less than predicted\nWhy? Prestige, walkability, and growing interest are not fully captured by structural or census variables"
  },
  {
    "objectID": "labs/presentation/presentation.html#key-findings-1",
    "href": "labs/presentation/presentation.html#key-findings-1",
    "title": "Predicting Housing Sales",
    "section": "Key Findings",
    "text": "Key Findings\nModel Accuracy: RMSE = 0.479 (log scale)\nModel Fit: R² = 0.64 (0.65 with K-fold CV)\nTop Predictors\n\nEconomic Density (U-shaped effect): Strongest spatial driver of price\nHome Size (β = 0.509, p &lt; 0.001): 1% increase in area → ~0.5% ↑ in price\nBathrooms (β = 0.151, p &lt; 0.001): Large value premium per added bath\nNeighborhood Composition: Higher education (+) and vacancy (–) strongly shape prices\n\nKey Insight:\nNeighborhood effects matter as much as home features — location premiums are real, measurable, and spatially clustered. —"
  },
  {
    "objectID": "labs/presentation/presentation.html#recommendations",
    "href": "labs/presentation/presentation.html#recommendations",
    "title": "Predicting Housing Sales",
    "section": "Recommendations",
    "text": "Recommendations\nImprove the City’s AVM by Incorporating:\n\nAmenity-based spatial features (EconKDE, trees, school access)\nNeighborhood context variables to capture local market conditions\n\nPolicy Implications\n\nCurrent AVM likely undervalues booming neighborhoods near economic hubs\nAffluent areas remain hardest to predict due to prestige and perception effects\nAdd monitoring for rapidly changing neighborhoods (e.g., Fishtown)\n\nBottom Line:\nTo reduce inequity and improve accuracy, the AVM must move beyond structure-only predictors and account for context, amenities, and spatial clustering."
  },
  {
    "objectID": "labs/presentation/presentation.html#limitations-next-steps",
    "href": "labs/presentation/presentation.html#limitations-next-steps",
    "title": "Predicting Housing Sales",
    "section": "Limitations & Next Steps",
    "text": "Limitations & Next Steps\nKey Limitations\n\nThe model cannot quantify word-of-mouth effects\n→ Perceived “vibe shifts” can rapidly change prices\nFuture plans are not incorporated\n→ Planned infrastructure can greatly alter market outcomes\n\nWhere Do We Go From Here?\n\nBuild an enhanced model that includes:\n→ Amenity proximity\n→ Economic development indicators\n→ Spatial clustering of sale prices\nContinue data collection to track a changing housing landscape\nConsult with local government on new projects and emerging trends"
  },
  {
    "objectID": "labs/presentation/presentation.html#questions",
    "href": "labs/presentation/presentation.html#questions",
    "title": "Predicting Housing Sales",
    "section": "Questions?",
    "text": "Questions?\n\nThank you!\n\n\nContact Information:\n\nJack Bader\n\njbader14@upenn.edu\n\nMatthew Levy\n\nmblevy@upenn.edu\n\nTim Wen\n\nsw6as@upenn.edu\n\n\n\n\nJoey Cahill\n\ncahill1@upenn.edu\n\nSam Sen\n\nsen1@upenn.edu\n\nYe Zhang\n\nyezhang1@upenn.edu"
  },
  {
    "objectID": "labs/Zhang_Ye_Appendix.html",
    "href": "labs/Zhang_Ye_Appendix.html",
    "title": "Technical Appendix",
    "section": "",
    "text": "── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.2\n✔ ggplot2   4.0.0     ✔ tibble    3.3.0\n✔ lubridate 1.9.4     ✔ tidyr     1.3.1\n✔ purrr     1.1.0     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nAttaching package: 'scales'\n\n\nThe following object is masked from 'package:purrr':\n\n    discard\n\n\nThe following object is masked from 'package:readr':\n\n    col_factor\n\n\nLinking to GEOS 3.13.0, GDAL 3.8.5, PROJ 9.5.1; sf_use_s2() is TRUE\n\nTo enable caching of data, set `options(tigris_use_cache = TRUE)`\nin your R script or .Rprofile.\n\nhere() starts at /Users/ye/Documents/GitHub/portfolio-setup-yezhanggg\n\nData (c) OpenStreetMap contributors, ODbL 1.0. https://www.openstreetmap.org/copyright.\nCheck the package website, https://docs.ropensci.org/osmextract/, for more details.\n\nLoading required package: spatstat.data\n\nLoading required package: spatstat.univar\n\nspatstat.univar 3.1-4\n\nspatstat.geom 3.6-0\n\n\nAttaching package: 'spatstat.geom'\n\n\nThe following object is masked from 'package:patchwork':\n\n    area\n\n\nThe following object is masked from 'package:scales':\n\n    rescale\n\n\nLoading required package: spatstat.random\n\nspatstat.random 3.4-2\n\nLoading required package: spatstat.explore\n\nLoading required package: nlme\n\n\nAttaching package: 'nlme'\n\n\nThe following object is masked from 'package:dplyr':\n\n    collapse\n\n\nspatstat.explore 3.5-3\n\nLoading required package: spatstat.model\n\nLoading required package: rpart\n\nspatstat.model 3.4-2\n\nLoading required package: spatstat.linnet\n\nspatstat.linnet 3.3-2\n\n\nspatstat 3.4-1 \nFor an introduction to spatstat, type 'beginner' \n\n\nterra 1.8.70\n\n\nAttaching package: 'terra'\n\n\nThe following objects are masked from 'package:spatstat.geom':\n\n    area, delaunay, is.empty, rescale, rotate, shift, where.max,\n    where.min\n\n\nThe following object is masked from 'package:patchwork':\n\n    area\n\n\nThe following object is masked from 'package:tigris':\n\n    blocks\n\n\nThe following object is masked from 'package:scales':\n\n    rescale\n\n\nThe following object is masked from 'package:knitr':\n\n    spin\n\n\nThe following object is masked from 'package:tidyr':\n\n    extract\n\n\n\nAttaching package: 'jsonlite'\n\n\nThe following object is masked from 'package:purrr':\n\n    flatten\n\n\n\nPlease cite as: \n\n\n Hlavac, Marek (2022). stargazer: Well-Formatted Regression and Summary Statistics Tables.\n\n R package version 5.2.3. https://CRAN.R-project.org/package=stargazer \n\n\nLoading required package: carData\n\n\nAttaching package: 'car'\n\n\nThe following object is masked from 'package:spatstat.model':\n\n    bc\n\n\nThe following object is masked from 'package:spatstat.geom':\n\n    ellipse\n\n\nThe following object is masked from 'package:dplyr':\n\n    recode\n\n\nThe following object is masked from 'package:purrr':\n\n    some\n\n\nLoading required package: zoo\n\n\nAttaching package: 'zoo'\n\n\nThe following object is masked from 'package:terra':\n\n    time&lt;-\n\n\nThe following objects are masked from 'package:base':\n\n    as.Date, as.Date.numeric"
  },
  {
    "objectID": "labs/Zhang_Ye_Appendix.html#load-and-philadelphia-house-sales-data",
    "href": "labs/Zhang_Ye_Appendix.html#load-and-philadelphia-house-sales-data",
    "title": "Technical Appendix",
    "section": "1.1 Load and Philadelphia house sales data",
    "text": "1.1 Load and Philadelphia house sales data\n\n\nCode\n# Load Philly Property Sales data\nphl_sales &lt;- read_csv(\"~/Desktop/FW 25/MUSA 5080/opa_properties_public.csv\")\n\n\nWarning: One or more parsing issues, call `problems()` on your data frame for details,\ne.g.:\n  dat &lt;- vroom(...)\n  problems(dat)\n\n\nRows: 583750 Columns: 79\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr  (38): basements, beginning_point, book_and_page, building_code, buildin...\ndbl  (31): objectid, category_code, census_tract, depth, exempt_building, ex...\nlgl   (7): cross_reference, date_exterior_condition, mailing_address_2, mark...\ndttm  (3): assessment_date, recording_date, sale_date\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\n\nFilter to residential properties, 2023-2024 sales\n\n\nCode\n# Check data types\n# glimpse(phl_sales)\n\nphl_sales_res_23_24 &lt;- phl_sales |&gt;\n  filter(\n    category_code == 1, # Residential\n    year(sale_date) %in% c(2023, 2024), # 2023-24 sales\n    !is.na(category_code) & !is.na(sale_date) # Handle nulls\n  )\n\n\n\n\nRemove obvious errors\n\n\nCode\nphl_sales_clean &lt;- phl_sales_res_23_24 |&gt;\n  filter(\n    # Some sale_price are unrealistically too low ($0, $1 etc.)\n    sale_price &gt;= 10000,\n    # Exclude homes with 0 bathrooms\n    number_of_bathrooms &gt; 0,\n    # Some areas are unrealistically low (0, 1, etc.)\n    total_area &gt; 1,\n    # Some 0's remain in total_liveable_area after first area filter\n    total_livable_area &gt; 0,\n    # Filter our unrealistic year built\n    year_built &gt;= 1750\n    ) \n\n\n\n\nHandle missing values\n\n\nCode\n# Check how many features have NA values\n# sum(is.na(phl_sales_clean$number_of_bedrooms))\n# sum(is.na(phl_sales_clean$number_of_bathrooms))\n# sum(is.na(phl_sales_clean$total_livable_area))\n# sum(is.na(phl_sales_clean$year_built))\n\n# Remove the 2 observations with NA values for number of bedrooms\nphl_sales_clean &lt;- phl_sales_clean |&gt;\n  filter(\n    !is.na(number_of_bedrooms)\n  )\n\n\n\n\nPreliminary Feature Engineering: Age = sale date - year built\n\n\nCode\nphl_sales_clean &lt;- phl_sales_clean |&gt;\n  mutate(\n    sale_year = year(sale_date),\n    age = sale_year - year_built\n  )\n\n\n\n\nDocument all cleaning decisions\n\nOur methodology for cleaning the Philadelphia home sales data is to focus on the features used in our model. As a group, we decided on the following independent variables to consider in our data exploration and model building to be: number of bathrooms, number of bedrooms, total livable area, and year built. We recognize that there is some risk of collinearity between these structural features, which will later be monitored and addressed if needed in the model building stage. Additionally, we also had to clean the sales price column since this is the variable we aim to predict in our model.\nFilter for only residential properties & sales made in 2023-24 (per instructions).\nFilter for realistic sales price &gt;= $10,000.\nFilter for houses with at least 1 bathroom. We will keep observations where number of bedrooms = 0 as this likely signifies a studio apartment. However, it is not feasible for homes to have zero bathrooms, so we will enforce a constraint that a home must have at least 1 bathroom to preserve data integrity.\nFilter for realistic total area &gt; 1 sq ft & realistic total livable area &gt; 0 sq ft.\nFilter for year built &gt;= 1750 (some homes were built in year 0).\nHandle missing values: We removed any missing values in our dependent variable of sales price, since it is crucial we have a true and accurate measure for prediction. We also checked which of our predictor variables had NA values after filtering. Only number of bedrooms had 2 remaining NA values. The rest had no NA values. To remedy this, we will remove the 2 observations from our data. Note, if there was substantial missing values in our predictors, we could use strategies such as imputing the NA values with the mean or median to use when building our model.\nPreliminary feature engineering: Rather than using year built in our Automated Valuation Model, it makes more sense to create a new variable age that is equal to the sale date minus the year built. The age variable is often easier to interpret in exploratory plots with the newer houses appearing on the left and older ones on the right. This is primarily a stylistic preference: the overall pattern of the data will remain the same but mirrored."
  },
  {
    "objectID": "labs/Zhang_Ye_Appendix.html#load-secondary-data",
    "href": "labs/Zhang_Ye_Appendix.html#load-secondary-data",
    "title": "Technical Appendix",
    "section": "1.2 Load Secondary Data",
    "text": "1.2 Load Secondary Data\n\nCensus\nPurpose: Pull demographic and housing data at the census tract level for Philadelphia from the 2023 5-year ACS. This data will provide predictors for neighborhood characteristics in our modeling.\nVariables Collected:\nMedian household income (B19013)\nPercentage of family households (B11001)\nEducation attainment: percent of population 25+ with a bachelor’s degree or higher (B15003)\nHousing vacancy rate (B25002)\nRacial composition: percent white (B02001)\n\nLoad Philly Census Data from Previously Retrieved Files\n\n\nCode\n# Relative to project root\ncensus_path &lt;- here(\"data\", \"Philly Census\")\n\ncensus_csv_path &lt;- file.path(census_path, \"philly_tract_metrics.csv\")\ncensus_shp_path &lt;- file.path(census_path, \"philly_tract.shp\")\n\n# Csv with census tract Geo IDs and metrics\nphilly_censustract &lt;- read_csv(census_csv_path, show_col_types = FALSE)\n\n# Shp File including geometry\nphilly_tract_sf &lt;- st_read(census_shp_path, quiet = TRUE)\n\n\n\n\nObserve Summary Statistics from target metrics.x\n\n\nCode\nnumeric_vars &lt;- c(\"median_income\", \"pct_white\", \"pct_bachelors\", \"pct_vacant\")\nphilly_censustract %&gt;%\n  select(all_of(numeric_vars)) %&gt;%\n  summary()\n\n\n median_income      pct_white      pct_bachelors      pct_vacant     \n Min.   : 13721   Min.   : 0.000   Min.   : 1.504   Min.   :  0.000  \n 1st Qu.: 42469   1st Qu.: 8.826   1st Qu.:16.094   1st Qu.:  5.366  \n Median : 60817   Median :34.740   Median :28.426   Median :  8.516  \n Mean   : 66877   Mean   :37.944   Mean   :36.325   Mean   :  9.844  \n 3rd Qu.: 85298   3rd Qu.:64.202   3rd Qu.:55.345   3rd Qu.: 12.801  \n Max.   :192727   Max.   :95.513   Max.   :96.632   Max.   :100.000  \n NA's   :27       NA's   :17       NA's   :17       NA's   :19       \n\n\nA quick check of the census variables reveals some missing values and lower than epected values in median income. We will note this information but retain the missing values for now to maintain the full pitcure of census blocks.\n\n\n\nCleaning Methodology (Census)\nMedian income: Selected only the estimate column and renamed it for clarity.\nHousehold composition: Pivoted ACS table to wide format, then calculated total households and family households.\nEducation: Pivoted to wide format, summed relevant categories to compute percent of population with a bachelor’s degree or higher.\nVacancy: Pivoted to wide format, calculated percent of homes vacant (vacant_units / total_units * 100).\nRacial composition: Pivoted to wide format, computed percent white.\nMerging: Combined all datasets by GEOID to create a single dataframe philly_blockgroup with all variables.\nGeometry: Pulled census tract shapefiles with ACS geometry and merged with philly_blockgroup to create philly_bg_map.\n\n\nNeighborhood (Polygon)\nReading in Philadelphia Neighborhoods as a shp object. This will allow us to aggregate data on neighborhoods to identify catagorical metrics.\n\n\nCode\nneighborhood_folder &lt;- here(\"data\", \"philadelphia-neighborhoods\")\nneighborhood_path   &lt;- file.path(neighborhood_folder, \"philadelphia-neighborhoods.shp\")\n\n# Read the shapefile\nphilly_neighborhoods &lt;- st_read(neighborhood_path, quiet = TRUE)\n\nhead(philly_neighborhoods)\n\n\nSimple feature collection with 6 features and 5 fields\nGeometry type: POLYGON\nDimension:     XY\nBounding box:  xmin: -75.23049 ymin: 39.98491 xmax: -75.0156 ymax: 40.11269\nGeodetic CRS:  WGS 84\n             NAME         LISTNAME         MAPNAME Shape_Leng Shape_Area\n1      BRIDESBURG       Bridesburg      Bridesburg   27814.55   44586264\n2       BUSTLETON        Bustleton       Bustleton   48868.46  114050424\n3      CEDARBROOK       Cedarbrook      Cedarbrook   20021.42   24871745\n4   CHESTNUT_HILL    Chestnut Hill   Chestnut Hill   56394.30   79664975\n5      EAST_FALLS       East Falls      East Falls   27400.78   40576888\n6 MOUNT_AIRY_EAST Mount Airy, East East Mount Airy   28845.55   43152470\n                        geometry\n1 POLYGON ((-75.06773 40.0054...\n2 POLYGON ((-75.0156 40.09487...\n3 POLYGON ((-75.18848 40.0727...\n4 POLYGON ((-75.21221 40.0860...\n5 POLYGON ((-75.18476 40.0282...\n6 POLYGON ((-75.18087 40.0432...\n\n\n\n\nCommercial and office points of interests (Amenities)(alternative in the next section if you do not want to download pbf data)\n\n\nCode\n# downloading osm data from geofabrik:https://download.geofabrik.de/north-america/us-northeast.html\n\n#input_pbf &lt;- \"the pdf file downloaded from the link above\"\n\n\n# get boundary of Philadelphia County\npa_counties &lt;- counties(state = \"PA\", year = 2023)\n\n# Filter to Philadelphia County\nphilly_boundary &lt;- subset(pa_counties, NAME == \"Philadelphia\")\n\n# read the full OSM PBF (you can select layer types like points, lines, polygons)\npoi &lt;- oe_read(input_pbf, \n                       boundary = philly_boundary, \n                       boundary_type = \"clipsrc\", \n                       layer = \"points\")  # or \"lines\" / \"multipolygons\"\n\n\nkeywords &lt;- c(\"shop\",\"amenity\",\"office\",\"historic\",\"tourism\",\"healthcare\",\n              \"building\",\"leisure\")\npattern &lt;- paste0(keywords, collapse = \"|\")\n\n# ==== Filter by 'other_tags' ====\nif (\"other_tags\" %in% names(poi)) {\n  poi$other_tags &lt;- iconv(as.character(poi$other_tags), from = \"\", to = \"UTF-8\", sub = \"\")\n  poi$other_tags[is.na(poi$other_tags)] &lt;- \"\"\n  \n  poi_filtered &lt;- poi %&gt;%\n    filter(grepl(pattern, other_tags, ignore.case = TRUE))\n  \n  cat(\"filtered POIs found:\", nrow(poi_filtered), \"of\", nrow(poi), \"\\n\")\n  \n} \n\n\n\n\nAlternative: filtered POI if you donot want to download osm data\n\n\nCode\npoi_path &lt;- here(\"data\", \"filtered poi\")\npoi_shp_path=file.path(poi_path, \"philadelphia_poi_filtered.shp\")\npoi=st_read(poi_shp_path)\n\n\nReading layer `philadelphia_poi_filtered' from data source \n  `/Users/ye/Documents/GitHub/portfolio-setup-yezhanggg/data/filtered poi/philadelphia_poi_filtered.shp' \n  using driver `ESRI Shapefile'\nSimple feature collection with 11161 features and 10 fields\nGeometry type: POINT\nDimension:     XY\nBounding box:  xmin: -75.27472 ymin: 39.87383 xmax: -74.95777 ymax: 40.13445\nGeodetic CRS:  WGS 84\n\n\n\n\nKernel Density Rasters (Economic activities density)\nInstead of using distance to CBD, we extracted commercial and office points of interests from OpenStreetMap (OSM), and we opreate a Kernel Density Estimation (KDE) with a bandwidth of 300 meters. By doing that, we manage to get a surface of density of economic activities across the whole city. The higher the KDE value is, the more economic activities it will be, implying a higher likelyhood of the area as a city centers.\nThere are several benefits using this approach compared to distance to CBD. First, with the development of suburbanization, even within the context of Philadelphia County, there is still a shift from monocentric model to polycentric model, meaning there multiple centers/subcenters. Using one CBD fail to capture these subcenters, which may also influence housing price. Second, CBD is an area rather than a point, distance method fail to capture this while the continuous surface computed by KDE would have a value of economic activities across the whole city.\n\n\nCode\n# get boundary of Philadelphia County\npa_counties &lt;- counties(state = \"PA\", year = 2023)\n\n# Filter to Philadelphia County\nphilly_boundary &lt;- subset(pa_counties, NAME == \"Philadelphia\")\n\nphilly_boundary &lt;- st_transform(philly_boundary, 3364)  \npoi &lt;- st_transform(poi, 3364)\n# ==== Prepare point pattern ====\n# Convert sf points to spatstat ppp object\nwin &lt;- as.owin(st_union(philly_boundary))  # window from county boundary\ncoords &lt;- st_coordinates(poi)\npp &lt;- ppp(x = coords[,1], y = coords[,2], window = win)\n\n\nWarning: data contain duplicated points\n\n\nCode\n# ==== Run Kernel Density Estimation ====\n# Sigma = bandwidth in map units (here, meters)\ndensity_map &lt;- density.ppp(pp, sigma = 300* 3.28084, edge = TRUE, at = \"pixels\",eps = c(100, 100))\n\n# ==== Convert to raster ====\nr_Economic &lt;- rast(density_map)\ncrs(r_Economic) &lt;- st_crs(philly_boundary)$proj4string\nr_Economic &lt;- mask(r_Economic, vect(philly_boundary))\n\n\n\n\nEducation\nWe used two datasets from OpenDataPhilly.com to identify schools geolocation and populated the metrics off Attendance percent and Withdrawal volumes from those schools.\n\n\nCode\n# Relative to project root\neducation_path &lt;- here(\"data\", \"Education\")\n\neducation_csv_path &lt;- file.path(education_path, \"philadelphia_schools.csv\")\neducation_shp_path &lt;- file.path(education_path, \"Schools Shape\", \"Schools.shp\")\n\n# Csv with School Names and metrics\nphilly_schools &lt;- read_csv(education_csv_path, show_col_types = FALSE)\n\n# Shp File including geometry\nphilly_schools_sf &lt;- st_read(education_shp_path, quiet = TRUE)\n\n\nWe joined the csv file containing the metrics with the shp file containing geoloaction.\n\n\nCode\n# Joining Schools csv metrics to shp file. Joined on 'location_i' (shp) and 'School_code (csv)\n# Keeping metrics for Attendance and Withdrawals\n\n# Select relevant metrics from CSV\nschool_metrics &lt;- philly_schools %&gt;%\n  select(School_code, Attendance, Withdrawals) %&gt;%\n  mutate(School_code = as.character(School_code))\n\nphilly_schools_sf &lt;- philly_schools_sf %&gt;%\n  left_join(school_metrics,\n            by = c(\"location_i\" = \"School_code\"))\n\n\n\n\nCode\nphilly_schools_sf_clean &lt;- philly_schools_sf %&gt;%\n  filter(!is.na(Attendance) & !is.na(Withdrawals))\n\n\n\n\nCode\nnames(philly_schools_sf_clean)\n\n\n [1] \"aun\"         \"school_num\"  \"location_i\"  \"school_nam\"  \"school_n_1\" \n [6] \"street_add\"  \"zip_code\"    \"phone_numb\"  \"grade_leve\"  \"grade_org\"  \n[11] \"enrollment\"  \"type\"        \"type_speci\"  \"objectid\"    \"Attendance\" \n[16] \"Withdrawals\" \"geometry\"   \n\n\nCode\nnrow(philly_schools_sf_clean)\n\n\n[1] 204\n\n\nOnce joined, we dropped rows that did not have values in Attendance and Withdrawal. This resulted in 204 public schools and their metrics located in Philadelphia City Limits.\n###Tree density Location of trees data was extracted from Opendata Philly. A Kernel Density Estimation was used to estimate the density of trees. The higher the value is, the more trees there will be in this (and surronding) cell\n\n\nCode\ntree_path &lt;- here(\"data\", \"ppr_tree_inventory_2024\")\n\ntree_shp_path &lt;- file.path(tree_path, \"ppr_tree_inventory_2024.shp\")\n\n\ntrees=st_read(tree_shp_path)\n\n\nReading layer `ppr_tree_inventory_2024' from data source \n  `/Users/ye/Documents/GitHub/portfolio-setup-yezhanggg/data/ppr_tree_inventory_2024/ppr_tree_inventory_2024.shp' \n  using driver `ESRI Shapefile'\nSimple feature collection with 151713 features and 6 fields\nGeometry type: POINT\nDimension:     XY\nBounding box:  xmin: -8380434 ymin: 4847791 xmax: -8344373 ymax: 4885938\nProjected CRS: WGS 84 / Pseudo-Mercator\n\n\nCode\ntrees=st_transform(trees,3364)\n\n# Convert sf points to spatstat ppp object\ncoords_trees &lt;- st_coordinates(trees)\npp_trees &lt;- ppp(x = coords_trees[,1], y = coords_trees[,2], window = win)\n\n\nWarning: 355 points were rejected as lying outside the specified window\n\n\nWarning: data contain duplicated points\n\n\nCode\n# ==== Run Kernel Density Estimation ====\n# Sigma = bandwidth in map units (here, meters)\ndensity_map_trees &lt;- density.ppp(pp_trees, edge = TRUE, at = \"pixels\",eps = c(100, 100))\n\n# ==== Convert to raster ====\nr_trees &lt;- rast(density_map_trees)\ncrs(r_trees) &lt;- st_crs(philly_boundary)$proj4string\nr_trees &lt;- mask(r_trees, vect(philly_boundary))  # mask to county boundary\n\n\n\n\nJoining data together\nFirst, since commercial and office POI and trees are point data, and the more they cluster the higher the housing price will be. As a result, we use Kernel Density method to estimate the density of them. Value of the cell was assign to the point (housing prices), if the point falls within it.\nSecond, as census tracts and neighborhood are polygon data, a st_within spatial join was used to join them with housing data. Values from polygon data will assign to the points when points fall within it.\n\n\nCode\n# Geometry - Cenus Data Merge\nphilly_tract_sf$GEOID=as.numeric(philly_tract_sf$GEOID)\nphilly_tract_map &lt;- philly_tract_sf %&gt;%\n  left_join(philly_censustract, by = \"GEOID\")\n\n#convert housing prices data into point data\nphl_sales_clean_sf = phl_sales_clean%&gt;%\n  mutate(geometry = st_as_sfc(shape)) %&gt;%   # parse WKT into geometry\n  st_as_sf(crs = 2272)  \n#convert and match the crs\nphilly_schools_sf_clean=philly_schools_sf_clean%&gt;%\n  st_transform(2272)\nphilly_neighborhoods=philly_neighborhoods%&gt;%\n  st_transform(2272)\nphilly_tract_map=philly_tract_map%&gt;%\n  st_transform(2272)\n#merge them together\nphl_sales_clean_sf_final=phl_sales_clean_sf%&gt;%\n  st_join(philly_tract_map,join=st_within)%&gt;%\n  st_join(philly_neighborhoods,join = st_within)\nphl_sales_clean_sf_final &lt;- st_transform(phl_sales_clean_sf_final, crs = st_crs(r_trees))\nphl_sales_clean_sf_final$EconKDE &lt;- raster::extract(r_Economic,phl_sales_clean_sf_final)\nphl_sales_clean_sf_final$TreeKDE &lt;- raster::extract(r_trees, phl_sales_clean_sf_final)  \nphl_sales_clean_sf_final=phl_sales_clean_sf_final%&gt;%\n  st_transform(2272)\n\n\n\n\nSummary table before and after dimensions\n\n\nCode\nbefore_after_summary &lt;- data.frame(\n  Stage = c(\"Raw Data\", \n            \"After Residential Filter (2023–24)\", \n            \"After Removing Errors & NAs\", \n            \"After Spatial Joins & Final Cleaning\"),\n  Rows = c(nrow(phl_sales),\n           nrow(phl_sales_res_23_24),\n           nrow(phl_sales_clean),\n           nrow(phl_sales_clean_sf_final)),\n  Columns = c(ncol(phl_sales),\n              ncol(phl_sales_res_23_24),\n              ncol(phl_sales_clean),\n              ncol(phl_sales_clean_sf_final))\n)\n\nknitr::kable(before_after_summary, caption = \"Data dimensions before and after cleaning\")\n\n\n\nData dimensions before and after cleaning\n\n\nStage\nRows\nColumns\n\n\n\n\nRaw Data\n583750\n79\n\n\nAfter Residential Filter (2023–24)\n34537\n79\n\n\nAfter Removing Errors & NAs\n22018\n81\n\n\nAfter Spatial Joins & Final Cleaning\n22018\n106"
  },
  {
    "objectID": "labs/Zhang_Ye_Assignment1.html",
    "href": "labs/Zhang_Ye_Assignment1.html",
    "title": "Assignment 1: Census Data Quality for Policy Decisions",
    "section": "",
    "text": "You are a data analyst for the [California] Department of Human Services. The department is considering implementing an algorithmic system to identify communities that should receive priority for social service funding and outreach programs. Your supervisor has asked you to evaluate the quality and reliability of available census data to inform this decision.\nDrawing on our Week 2 discussion of algorithmic bias, you need to assess not just what the data shows, but how reliable it is and what communities might be affected by data quality issues.\n\n\n\n\nApply dplyr functions to real census data for policy analysis\nEvaluate data quality using margins of error\nConnect technical analysis to algorithmic decision-making\nIdentify potential equity implications of data reliability issues\nCreate professional documentation for policy stakeholders\n\n\n\n\nSubmit by posting your updated portfolio link on Canvas. Your assignment should be accessible at your-portfolio-url/assignments/assignment_1/\nMake sure to update your _quarto.yml navigation to include this assignment under an “Assignments” menu."
  },
  {
    "objectID": "labs/Zhang_Ye_Assignment1.html#scenario",
    "href": "labs/Zhang_Ye_Assignment1.html#scenario",
    "title": "Assignment 1: Census Data Quality for Policy Decisions",
    "section": "",
    "text": "You are a data analyst for the [California] Department of Human Services. The department is considering implementing an algorithmic system to identify communities that should receive priority for social service funding and outreach programs. Your supervisor has asked you to evaluate the quality and reliability of available census data to inform this decision.\nDrawing on our Week 2 discussion of algorithmic bias, you need to assess not just what the data shows, but how reliable it is and what communities might be affected by data quality issues."
  },
  {
    "objectID": "labs/Zhang_Ye_Assignment1.html#learning-objectives",
    "href": "labs/Zhang_Ye_Assignment1.html#learning-objectives",
    "title": "Assignment 1: Census Data Quality for Policy Decisions",
    "section": "",
    "text": "Apply dplyr functions to real census data for policy analysis\nEvaluate data quality using margins of error\nConnect technical analysis to algorithmic decision-making\nIdentify potential equity implications of data reliability issues\nCreate professional documentation for policy stakeholders"
  },
  {
    "objectID": "labs/Zhang_Ye_Assignment1.html#submission-instructions",
    "href": "labs/Zhang_Ye_Assignment1.html#submission-instructions",
    "title": "Assignment 1: Census Data Quality for Policy Decisions",
    "section": "",
    "text": "Submit by posting your updated portfolio link on Canvas. Your assignment should be accessible at your-portfolio-url/assignments/assignment_1/\nMake sure to update your _quarto.yml navigation to include this assignment under an “Assignments” menu."
  },
  {
    "objectID": "labs/Zhang_Ye_Assignment1.html#data-retrieval",
    "href": "labs/Zhang_Ye_Assignment1.html#data-retrieval",
    "title": "Assignment 1: Census Data Quality for Policy Decisions",
    "section": "2.1 Data Retrieval",
    "text": "2.1 Data Retrieval\nYour Task: Use get_acs() to retrieve county-level data for your chosen state.\nRequirements: - Geography: county level - Variables: median household income (B19013_001) and total population (B01003_001)\n- Year: 2022 - Survey: acs5 - Output format: wide\nHint: Remember to give your variables descriptive names using the variables = c(name = \"code\") syntax.\n\n# Write your get_acs() code here\n\ncounty_data &lt;- get_acs(\n  geography = \"county\",\n  variables = c(\n    median_income = \"B19013_001\",  \n    total_pop = \"B01003_001\"    \n  ),\n  state = my_state,\n  year = 2022,\n  output = \"wide\"\n)\n\n# Clean the county names to remove state name and \"County\" \n# Hint: use mutate() with str_remove()\n\ncounty_data &lt;- county_data %&gt;%\n  mutate(NAME = str_remove(NAME, \" County, California\"))\n\n# Display the first few rows\nhead(county_data)\n\n# A tibble: 6 × 6\n  GEOID NAME      median_incomeE median_incomeM total_popE total_popM\n  &lt;chr&gt; &lt;chr&gt;              &lt;dbl&gt;          &lt;dbl&gt;      &lt;dbl&gt;      &lt;dbl&gt;\n1 06001 Alameda           122488           1231    1663823         NA\n2 06003 Alpine            101125          17442       1515        206\n3 06005 Amador             74853           6048      40577         NA\n4 06007 Butte              66085           2261     213605         NA\n5 06009 Calaveras          77526           3875      45674         NA\n6 06011 Colusa             69619           5745      21811         NA"
  },
  {
    "objectID": "labs/Zhang_Ye_Assignment1.html#data-quality-assessment",
    "href": "labs/Zhang_Ye_Assignment1.html#data-quality-assessment",
    "title": "Assignment 1: Census Data Quality for Policy Decisions",
    "section": "2.2 Data Quality Assessment",
    "text": "2.2 Data Quality Assessment\nYour Task: Calculate margin of error percentages and create reliability categories.\nRequirements: - Calculate MOE percentage: (margin of error / estimate) * 100 - Create reliability categories: - High Confidence: MOE &lt; 5% - Moderate Confidence: MOE 5-10%\n- Low Confidence: MOE &gt; 10% - Create a flag for unreliable estimates (MOE &gt; 10%)\nHint: Use mutate() with case_when() for the categories.\n\n# Calculate MOE percentage and reliability categories using mutate()\n\ncounty_data &lt;- county_data %&gt;%\n  mutate(\n    income_moe_pct = (median_incomeM / median_incomeE) * 100,\n     reliability_category = case_when(\n      income_moe_pct &lt; 5 ~ \"High Confidence\",\n      income_moe_pct &lt;= 10 ~ \"Moderate Confidence\",\n      TRUE ~ \"Low Confidence\"),\n        unreliable_flag = income_moe_pct &gt; 10)\n\n# Create a summary showing count of counties in each reliability category\n# Hint: use count() and mutate() to add percentages\n\nreliability_summary &lt;- county_data %&gt;%\n  count(reliability_category) %&gt;%\n  mutate(\n    percentage = round(n / sum(n) * 100, 1)\n  )"
  },
  {
    "objectID": "labs/Zhang_Ye_Assignment1.html#high-uncertainty-counties",
    "href": "labs/Zhang_Ye_Assignment1.html#high-uncertainty-counties",
    "title": "Assignment 1: Census Data Quality for Policy Decisions",
    "section": "2.3 High Uncertainty Counties",
    "text": "2.3 High Uncertainty Counties\nYour Task: Identify the 5 counties with the highest MOE percentages.\nRequirements: - Sort by MOE percentage (highest first) - Select the top 5 counties - Display: county name, median income, margin of error, MOE percentage, reliability category - Format as a professional table using kable()\nHint: Use arrange(), slice(), and select() functions.\n\n# Create table of top 5 counties by MOE percentage\n\nhigh_uncertainty_table &lt;- county_data %&gt;%\n  arrange(desc(income_moe_pct)) %&gt;%  \n  slice_head(n = 5) %&gt;% \n  select(NAME, median_incomeE, median_incomeM, income_moe_pct, reliability_category)\n\n# Format as table with kable() - include appropriate column names and caption\n\nlibrary(knitr)\nlibrary(kableExtra)\n\nhigh_uncertainty_table %&gt;%\n  kable(\n    col.names = c(\"County\", \"Median Income\", \"Margin of Error\", \"MOE (%)\", \"Reliability\"),\n    caption = \"Top 5 California Counties with Highest Income Uncertainty\",\n    format.args = list(big.mark = \",\"),\n    digits = c(0, 0, 0, 1, 0) \n  ) %&gt;%\n  kable_styling(bootstrap_options = c(\"striped\", \"hover\"))\n\n\nTop 5 California Counties with Highest Income Uncertainty\n\n\nCounty\nMedian Income\nMargin of Error\nMOE (%)\nReliability\n\n\n\n\nMono\n82,038\n15,388\n18.8\nLow Confidence\n\n\nAlpine\n101,125\n17,442\n17.2\nLow Confidence\n\n\nSierra\n61,108\n9,237\n15.1\nLow Confidence\n\n\nTrinity\n47,317\n5,890\n12.4\nLow Confidence\n\n\nPlumas\n67,885\n7,772\n11.4\nLow Confidence\n\n\n\n\n\nData Quality Commentary:\n[Counties that have higher margin of error will have unreliable income estimates and could lead to misleading policy plan and resource allocation. The high margin of error could because these are counties that have low population and the data could easily skewed.]"
  },
  {
    "objectID": "labs/Zhang_Ye_Assignment1.html#focus-area-selection",
    "href": "labs/Zhang_Ye_Assignment1.html#focus-area-selection",
    "title": "Assignment 1: Census Data Quality for Policy Decisions",
    "section": "3.1 Focus Area Selection",
    "text": "3.1 Focus Area Selection\nYour Task: Select 2-3 counties from your reliability analysis for detailed tract-level study.\nStrategy: Choose counties that represent different reliability levels (e.g., 1 high confidence, 1 moderate, 1 low confidence) to compare how data quality varies.\n\n# Use filter() to select 2-3 counties from your county_reliability data\n# Store the selected counties in a variable called selected_counties\n\nselected_counties &lt;- county_data %&gt;%\n  filter(NAME %in% c(\"Los Angeles\", \"Trinity\", \"Tehama\")) %&gt;%\n  select(NAME, median_incomeE, income_moe_pct, reliability_category)\n\n# Display the selected counties with their key characteristics\n# Show: county name, median income, MOE percentage, reliability category\n\nprint(selected_counties)\n\n# A tibble: 3 × 4\n  NAME        median_incomeE income_moe_pct reliability_category\n  &lt;chr&gt;                &lt;dbl&gt;          &lt;dbl&gt; &lt;chr&gt;               \n1 Los Angeles          83411          0.526 High Confidence     \n2 Tehama               59029          6.95  Moderate Confidence \n3 Trinity              47317         12.4   Low Confidence      \n\n\nComment on the output: [Los Angeles has a High Confidence category because of its extremely low income margin of error percent. This is because Los Angeles is one of the most populated county and that lead to low margin of error. On the another other hand, Tehama has a income margin of error percentage around 7 and Trinity is at 12. The data for Tehama and Trinity are less reliable compare to Los Angeles County.]"
  },
  {
    "objectID": "labs/Zhang_Ye_Assignment1.html#tract-level-demographics",
    "href": "labs/Zhang_Ye_Assignment1.html#tract-level-demographics",
    "title": "Assignment 1: Census Data Quality for Policy Decisions",
    "section": "3.2 Tract-Level Demographics",
    "text": "3.2 Tract-Level Demographics\nYour Task: Get demographic data for census tracts in your selected counties.\nRequirements: - Geography: tract level - Variables: white alone (B03002_003), Black/African American (B03002_004), Hispanic/Latino (B03002_012), total population (B03002_001) - Use the same state and year as before - Output format: wide - Challenge: You’ll need county codes, not names. Look at the GEOID patterns in your county data for hints.\n\n# Define your race/ethnicity variables with descriptive names\n\nrace_vars &lt;- c(\n  total_pop = \"B03002_001\",\n  white_alone = \"B03002_003\",\n  black_alone = \"B03002_004\",\n  hispanic = \"B03002_012\"\n)\n\n# Use get_acs() to retrieve tract-level data\n# Hint: You may need to specify county codes in the county parameter\n\ntract_demographics &lt;- get_acs(\n  geography = \"tract\",\n  variables = race_vars,\n  state = my_state,\n  county = c(\"037\", \"105\", \"103\"),\n  year = 2022,\n  output = \"wide\"\n)\n\n# Calculate percentage of each group using mutate()\n# Create percentages for white, Black, and Hispanic populations\n\ntract_demographics &lt;- tract_demographics %&gt;%\n  mutate(\n    white_pct = (white_aloneE / total_popE) * 100,\n    black_pct = (black_aloneE / total_popE) * 100,\n    hispanic_pct = (hispanicE / total_popE) * 100\n  )\n\n# Add readable tract and county name columns using str_extract() or similar\n\ntract_demographics &lt;- tract_demographics %&gt;%\n  mutate(\n    census_tract = str_remove(NAME, \";.*\"),\n    county = str_remove(NAME, \"Census Tract [0-9.]+; \") %&gt;%\n             str_remove(\"; California\") %&gt;%\n             str_remove(\" County\")\n  )"
  },
  {
    "objectID": "labs/Zhang_Ye_Assignment1.html#demographic-analysis",
    "href": "labs/Zhang_Ye_Assignment1.html#demographic-analysis",
    "title": "Assignment 1: Census Data Quality for Policy Decisions",
    "section": "3.3 Demographic Analysis",
    "text": "3.3 Demographic Analysis\nYour Task: Analyze the demographic patterns in your selected areas.\n\n# Find the tract with the highest percentage of Hispanic/Latino residents\n# Hint: use arrange() and slice() to get the top tract\n\nhighest_hispanic &lt;- tract_demographics %&gt;%\n  arrange(desc(hispanic_pct)) %&gt;%\n  slice_head(n = 1) %&gt;%\n  select(census_tract, county, hispanic_pct)\n\n# Calculate average demographics by county using group_by() and summarize()\n# Show: number of tracts, average percentage for each racial/ethnic group\n\ncounty_demographics &lt;- tract_demographics %&gt;%\n  group_by(county) %&gt;%\n  summarize(\n    num_tracts = n(),\n    avg_white_pct = mean(white_pct, na.rm = TRUE),\n    avg_black_pct = mean(black_pct, na.rm = TRUE),\n    avg_hispanic_pct = mean(hispanic_pct, na.rm = TRUE)\n  )\n\n# Create a nicely formatted table of your results using kable()\n\nlibrary(knitr)          \ncounty_demographics %&gt;%\n  kable(\n    col.names = c(\"County\", \"Number of Tracts\", \"Avg White %\", \"Avg Black %\", \"Avg Hispanic %\"), \n    caption = \"Average Demographics by County\",  \n    digits = 1           \n  )\n\n\nAverage Demographics by County\n\n\n\n\n\n\n\n\n\nCounty\nNumber of Tracts\nAvg White %\nAvg Black %\nAvg Hispanic %\n\n\n\n\nLos Angeles\n2498\n26.3\n7.6\n47.6\n\n\nTehama\n14\n65.9\n0.9\n26.0\n\n\nTrinity\n4\n79.2\n1.7\n7.0"
  },
  {
    "objectID": "labs/Zhang_Ye_Assignment1.html#moe-analysis-for-demographic-variables",
    "href": "labs/Zhang_Ye_Assignment1.html#moe-analysis-for-demographic-variables",
    "title": "Assignment 1: Census Data Quality for Policy Decisions",
    "section": "4.1 MOE Analysis for Demographic Variables",
    "text": "4.1 MOE Analysis for Demographic Variables\nYour Task: Examine margins of error for demographic variables to see if some communities have less reliable data.\nRequirements: - Calculate MOE percentages for each demographic variable - Flag tracts where any demographic variable has MOE &gt; 15% - Create summary statistics\n\n# Calculate MOE percentages for white, Black, and Hispanic variables\n# Hint: use the same formula as before (margin/estimate * 100)\n\ntract_demographics &lt;- tract_demographics %&gt;%\n  mutate(\n    white_moe_pct = (white_aloneM / white_aloneE) * 100,\n    black_moe_pct = (black_aloneM / black_aloneE) * 100,\n    hispanic_moe_pct = (hispanicM / hispanicE) * 100,\n    high_moe_flag = ifelse(white_moe_pct &gt; 15 | black_moe_pct &gt; 15 | hispanic_moe_pct &gt; 15, TRUE, FALSE)\n  )\n\n# Create a flag for tracts with high MOE on any demographic variable\n# Use logical operators (| for OR) in an ifelse() statement\n\n# Create summary statistics showing how many tracts have data quality issues\n\nmoe_summary &lt;- tract_demographics %&gt;%\n  summarize(\n    total_tracts = n(),\n    tracts_with_issues = sum(high_moe_flag, na.rm = TRUE),\n    pct_with_issues = round(sum(high_moe_flag, na.rm = TRUE) / n() * 100, 1)\n  )\n\nmoe_summary\n\n# A tibble: 1 × 3\n  total_tracts tracts_with_issues pct_with_issues\n         &lt;int&gt;              &lt;int&gt;           &lt;dbl&gt;\n1         2516               2515             100"
  },
  {
    "objectID": "labs/Zhang_Ye_Assignment1.html#pattern-analysis",
    "href": "labs/Zhang_Ye_Assignment1.html#pattern-analysis",
    "title": "Assignment 1: Census Data Quality for Policy Decisions",
    "section": "4.2 Pattern Analysis",
    "text": "4.2 Pattern Analysis\nYour Task: Investigate whether data quality problems are randomly distributed or concentrated in certain types of communities.\n\n# Group tracts by whether they have high MOE issues\n# Calculate average characteristics for each group:\n# - population size, demographic percentages\n\npattern_comparison &lt;- tract_demographics %&gt;%\n  group_by(high_moe_flag) %&gt;%\n  summarize(\n    num_tracts = n(),\n    avg_population = mean(total_popE, na.rm = TRUE),\n    avg_pct_white = mean(white_pct, na.rm = TRUE),\n    avg_pct_black = mean(black_pct, na.rm = TRUE),\n    avg_pct_hispanic = mean(hispanic_pct, na.rm = TRUE)\n  )\n\n# Use group_by() and summarize() to create this comparison\n# Create a professional table showing the patterns\n\nlibrary(knitr)\npattern_comparison %&gt;%\n  kable(\n    col.names = c(\"High MOE Issues\", \"Number of Tracts\", \"Avg Population\", \n                  \"Avg White% \", \"Avg Black %\", \"Avg Hispanic %\"),\n    caption = \"Comparison of Tract Characteristics by Data Quality\",\n    digits = c(0, 0, 0, 1, 1, 1)\n  )\n\n\nComparison of Tract Characteristics by Data Quality\n\n\n\n\n\n\n\n\n\n\nHigh MOE Issues\nNumber of Tracts\nAvg Population\nAvg White%\nAvg Black %\nAvg Hispanic %\n\n\n\n\nFALSE\n1\n8994\n16.8\n33.6\n41.4\n\n\nTRUE\n2515\n3980\n26.6\n7.5\n47.4\n\n\n\n\n\nPattern Analysis: [It is crazy that almost all of the data are not reliable. The estimate for racial group is not reliable on the census tract level through their high margin of error. I think this might further stress the point of how sampling population is critical in doing analysis. The average population for all the HIGH MOE Issues tracts is at 3980. The only one tract without HIGH MOE Issue has a population 0f 8994.]"
  },
  {
    "objectID": "labs/Zhang_Ye_Assignment1.html#analysis-integration-and-professional-summary",
    "href": "labs/Zhang_Ye_Assignment1.html#analysis-integration-and-professional-summary",
    "title": "Assignment 1: Census Data Quality for Policy Decisions",
    "section": "5.1 Analysis Integration and Professional Summary",
    "text": "5.1 Analysis Integration and Professional Summary\nYour Task: Write an executive summary that integrates findings from all four analyses.\nExecutive Summary Requirements: 1. Overall Pattern Identification: What are the systematic patterns across all your analyses? 2. Equity Assessment: Which communities face the greatest risk of algorithmic bias based on your findings? 3. Root Cause Analysis: What underlying factors drive both data quality issues and bias risk? 4. Strategic Recommendations: What should the Department implement to address these systematic issues?\nExecutive Summary:\n[I think the most important takeaway from this assignment is the important of sampling size. This situation has appear in multiple analysis and multple times. If a data has some reliability issue, one of the characteristic of that data is the small sampling size.\nThe community that faced the greatest risk of algorithmic bias is Trinity. Trinity only has 4 census tract and it such a small county that the data could highly likely to be skewed because of sampling error. It is also show as a “Low Confidence” in county level income data. The interpretation and result could be really not reliable due to data quality issue.\nThe root cause may be the survey design and also just how insufficient for the data to be reliable if we want to look at small geographic areas like census tracts. Rural places like Trinity and Tehama make reliable sampling really challenging.\nI think one of the most important is to apply strict MOE thresholds. Data that are not reliable especially on small sample and big geographic area it is really important to examine the data closely. Aggregate the data and looking at large scale is necessary if the public organization want to implement any policies. Careful decision making process is necessary and key for community success. ]"
  },
  {
    "objectID": "labs/Zhang_Ye_Assignment1.html#specific-recommendations",
    "href": "labs/Zhang_Ye_Assignment1.html#specific-recommendations",
    "title": "Assignment 1: Census Data Quality for Policy Decisions",
    "section": "6.3 Specific Recommendations",
    "text": "6.3 Specific Recommendations\nYour Task: Create a decision framework for algorithm implementation.\n\n# Create a summary table using your county reliability data\n# Include: county name, median income, MOE percentage, reliability category\n\ndecision_framework &lt;- county_data %&gt;%\n  select(NAME, median_incomeE, income_moe_pct, reliability_category) %&gt;%\n  mutate(\n    algorithm_recommendation = case_when(\n      reliability_category == \"High Confidence\" ~ \"Safe for algorithmic decisions\",\n      reliability_category == \"Moderate Confidence\" ~ \"Use with caution - monitor outcomes\",\n      reliability_category == \"Low Confidence\" ~ \"Requires manual review or additional data\"\n    )\n  )\n\n# Add a new column with algorithm recommendations using case_when():\n# - High Confidence: \"Safe for algorithmic decisions\"\n# - Moderate Confidence: \"Use with caution - monitor outcomes\"  \n# - Low Confidence: \"Requires manual review or additional data\"\n\n# Format as a professional table with kable()\n\nlibrary(knitr)\nlibrary(kableExtra)\n\ndecision_framework %&gt;%\n  kable(\n    col.names = c(\"County\", \"Median Income\", \"MOE %\", \"Reliability\", \"Algorithm Recommendation\"),\n    caption = \"Decision Framework for Algorithm Implementation by County\",\n    format.args = list(big.mark = \",\"),\n    digits = c(0, 0, 1, 0, 0)\n  ) %&gt;%\n  kable_styling(bootstrap_options = c(\"striped\", \"hover\"))\n\n\nDecision Framework for Algorithm Implementation by County\n\n\nCounty\nMedian Income\nMOE %\nReliability\nAlgorithm Recommendation\n\n\n\n\nAlameda\n122,488\n1.0\nHigh Confidence\nSafe for algorithmic decisions\n\n\nAlpine\n101,125\n17.2\nLow Confidence\nRequires manual review or additional data\n\n\nAmador\n74,853\n8.1\nModerate Confidence\nUse with caution - monitor outcomes\n\n\nButte\n66,085\n3.4\nHigh Confidence\nSafe for algorithmic decisions\n\n\nCalaveras\n77,526\n5.0\nHigh Confidence\nSafe for algorithmic decisions\n\n\nColusa\n69,619\n8.3\nModerate Confidence\nUse with caution - monitor outcomes\n\n\nContra Costa\n120,020\n1.2\nHigh Confidence\nSafe for algorithmic decisions\n\n\nDel Norte\n61,149\n7.2\nModerate Confidence\nUse with caution - monitor outcomes\n\n\nEl Dorado\n99,246\n3.4\nHigh Confidence\nSafe for algorithmic decisions\n\n\nFresno\n67,756\n1.4\nHigh Confidence\nSafe for algorithmic decisions\n\n\nGlenn\n64,033\n6.2\nModerate Confidence\nUse with caution - monitor outcomes\n\n\nHumboldt\n57,881\n3.7\nHigh Confidence\nSafe for algorithmic decisions\n\n\nImperial\n53,847\n4.1\nHigh Confidence\nSafe for algorithmic decisions\n\n\nInyo\n63,417\n8.6\nModerate Confidence\nUse with caution - monitor outcomes\n\n\nKern\n63,883\n2.1\nHigh Confidence\nSafe for algorithmic decisions\n\n\nKings\n68,540\n3.3\nHigh Confidence\nSafe for algorithmic decisions\n\n\nLake\n56,259\n4.3\nHigh Confidence\nSafe for algorithmic decisions\n\n\nLassen\n59,515\n6.0\nModerate Confidence\nUse with caution - monitor outcomes\n\n\nLos Angeles\n83,411\n0.5\nHigh Confidence\nSafe for algorithmic decisions\n\n\nMadera\n73,543\n3.9\nHigh Confidence\nSafe for algorithmic decisions\n\n\nMarin\n142,019\n2.9\nHigh Confidence\nSafe for algorithmic decisions\n\n\nMariposa\n60,021\n8.8\nModerate Confidence\nUse with caution - monitor outcomes\n\n\nMendocino\n61,335\n3.6\nHigh Confidence\nSafe for algorithmic decisions\n\n\nMerced\n64,772\n3.3\nHigh Confidence\nSafe for algorithmic decisions\n\n\nModoc\n54,962\n9.8\nModerate Confidence\nUse with caution - monitor outcomes\n\n\nMono\n82,038\n18.8\nLow Confidence\nRequires manual review or additional data\n\n\nMonterey\n91,043\n2.1\nHigh Confidence\nSafe for algorithmic decisions\n\n\nNapa\n105,809\n2.8\nHigh Confidence\nSafe for algorithmic decisions\n\n\nNevada\n79,395\n4.8\nHigh Confidence\nSafe for algorithmic decisions\n\n\nOrange\n109,361\n0.8\nHigh Confidence\nSafe for algorithmic decisions\n\n\nPlacer\n109,375\n1.7\nHigh Confidence\nSafe for algorithmic decisions\n\n\nPlumas\n67,885\n11.4\nLow Confidence\nRequires manual review or additional data\n\n\nRiverside\n84,505\n1.3\nHigh Confidence\nSafe for algorithmic decisions\n\n\nSacramento\n84,010\n1.0\nHigh Confidence\nSafe for algorithmic decisions\n\n\nSan Benito\n104,451\n5.2\nModerate Confidence\nUse with caution - monitor outcomes\n\n\nSan Bernardino\n77,423\n1.0\nHigh Confidence\nSafe for algorithmic decisions\n\n\nSan Diego\n96,974\n1.0\nHigh Confidence\nSafe for algorithmic decisions\n\n\nSan Francisco\n136,689\n1.4\nHigh Confidence\nSafe for algorithmic decisions\n\n\nSan Joaquin\n82,837\n1.7\nHigh Confidence\nSafe for algorithmic decisions\n\n\nSan Luis Obispo\n90,158\n2.6\nHigh Confidence\nSafe for algorithmic decisions\n\n\nSan Mateo\n149,907\n1.7\nHigh Confidence\nSafe for algorithmic decisions\n\n\nSanta Barbara\n92,332\n2.0\nHigh Confidence\nSafe for algorithmic decisions\n\n\nSanta Clara\n153,792\n1.0\nHigh Confidence\nSafe for algorithmic decisions\n\n\nSanta Cruz\n104,409\n3.0\nHigh Confidence\nSafe for algorithmic decisions\n\n\nShasta\n68,347\n3.6\nHigh Confidence\nSafe for algorithmic decisions\n\n\nSierra\n61,108\n15.1\nLow Confidence\nRequires manual review or additional data\n\n\nSiskiyou\n53,898\n4.9\nHigh Confidence\nSafe for algorithmic decisions\n\n\nSolano\n97,037\n1.8\nHigh Confidence\nSafe for algorithmic decisions\n\n\nSonoma\n99,266\n2.0\nHigh Confidence\nSafe for algorithmic decisions\n\n\nStanislaus\n74,872\n1.8\nHigh Confidence\nSafe for algorithmic decisions\n\n\nSutter\n72,654\n4.7\nHigh Confidence\nSafe for algorithmic decisions\n\n\nTehama\n59,029\n7.0\nModerate Confidence\nUse with caution - monitor outcomes\n\n\nTrinity\n47,317\n12.4\nLow Confidence\nRequires manual review or additional data\n\n\nTulare\n64,474\n2.3\nHigh Confidence\nSafe for algorithmic decisions\n\n\nTuolumne\n70,432\n6.7\nModerate Confidence\nUse with caution - monitor outcomes\n\n\nVentura\n102,141\n1.5\nHigh Confidence\nSafe for algorithmic decisions\n\n\nYolo\n85,097\n2.7\nHigh Confidence\nSafe for algorithmic decisions\n\n\nYuba\n66,693\n4.2\nHigh Confidence\nSafe for algorithmic decisions\n\n\n\n\n\nKey Recommendations:\nYour Task: Use your analysis results to provide specific guidance to the department.\n\nhigh_confidence &lt;- decision_framework %&gt;%\n  filter(reliability_category == \"High Confidence\") %&gt;%\n  select(NAME)\n\nmoderate_confidence &lt;- decision_framework %&gt;%\n  filter(reliability_category == \"Moderate Confidence\") %&gt;%\n  select(NAME)\n\nlow_confidence &lt;- decision_framework %&gt;%\n  filter(reliability_category == \"Low Confidence\") %&gt;%\n  select(NAME)\n\nprint(\"High Confidence Counties:\")\n\n[1] \"High Confidence Counties:\"\n\nprint(high_confidence)\n\n# A tibble: 42 × 1\n   NAME        \n   &lt;chr&gt;       \n 1 Alameda     \n 2 Butte       \n 3 Calaveras   \n 4 Contra Costa\n 5 El Dorado   \n 6 Fresno      \n 7 Humboldt    \n 8 Imperial    \n 9 Kern        \n10 Kings       \n# ℹ 32 more rows\n\nprint(\"Moderate Confidence Counties:\")\n\n[1] \"Moderate Confidence Counties:\"\n\nprint(moderate_confidence)\n\n# A tibble: 11 × 1\n   NAME      \n   &lt;chr&gt;     \n 1 Amador    \n 2 Colusa    \n 3 Del Norte \n 4 Glenn     \n 5 Inyo      \n 6 Lassen    \n 7 Mariposa  \n 8 Modoc     \n 9 San Benito\n10 Tehama    \n11 Tuolumne  \n\nprint(\"Low Confidence Counties:\")\n\n[1] \"Low Confidence Counties:\"\n\nprint(low_confidence)\n\n# A tibble: 5 × 1\n  NAME   \n  &lt;chr&gt;  \n1 Alpine \n2 Mono   \n3 Plumas \n4 Sierra \n5 Trinity\n\n\n\nCounties suitable for immediate algorithmic implementation: [The counties that are suitabale for immediate algorithmic implementation are Alameda, Butte, Calaveras, Contra Costa, El Dorado, Fresno, Humboldt, Imperial, Kern, Kings, Lake, Los Angeles, Madera, Marin, Mendocino, Merced, Monterey, Napa, Nevada, Orange, Placer, Riverside, Sacramento, San Bernardino, San Diego, San Francisco, San Joaquin, San Luis Obispo, San Mateo, Santa Barbara, Santa Clara, Santa Cruz, Shasta, Siskiyou, Solano, Sonoma, Stanislaus, Sutter, Tulare, Ventura, Yolo, Yuba. These county are in the high confidence category for the new decision making framework.]\nCounties requiring additional oversight: [Counties need additional oversight are Amador, Colusa, Del Norte, Glenn, Inyo, Lassen, Mariposa, Modoc, San Benito, Tehama, Tuolumne. These county are in the moderate confidence category for the new decision making framework.]\nCounties needing alternative approaches: [Counties that need alternative approaches are Alpine, Mono, Plumas, Sierra, Trinity because they are in the low confidence category for the new decision making framework.]"
  },
  {
    "objectID": "labs/Zhang_Ye_Assignment1.html#questions-for-further-investigation",
    "href": "labs/Zhang_Ye_Assignment1.html#questions-for-further-investigation",
    "title": "Assignment 1: Census Data Quality for Policy Decisions",
    "section": "Questions for Further Investigation",
    "text": "Questions for Further Investigation\n[- How has the reliability of ACS estimates changed over time for small counties like Trinity? - Do neighboring census tracts with high MOEs cluster together spatially?]"
  },
  {
    "objectID": "labs/Zhang_Ye_Assignment1.html#submission-checklist",
    "href": "labs/Zhang_Ye_Assignment1.html#submission-checklist",
    "title": "Assignment 1: Census Data Quality for Policy Decisions",
    "section": "Submission Checklist",
    "text": "Submission Checklist\nBefore submitting your portfolio link on Canvas:\n\nAll code chunks run without errors\nAll “[Fill this in]” prompts have been completed\nTables are properly formatted and readable\nExecutive summary addresses all four required components\nPortfolio navigation includes this assignment\nCensus API key is properly set\nDocument renders correctly to HTML\n\nRemember: Submit your portfolio URL on Canvas, not the file itself. Your assignment should be accessible at your-portfolio-url/assignments/assignment_1/your_file_name.html"
  },
  {
    "objectID": "weekly-notes/week-09-notes.html",
    "href": "weekly-notes/week-09-notes.html",
    "title": "Week 9 Notes",
    "section": "",
    "text": "##Key Concepts Learned\n\nWe learned about “dirty data” - data derived from or influenced by corrupt, biased, and unlawful practices including fabricated data, systematic bias, and missing data, not just traditional data quality issues\nPredictive policing creates feedback loops where algorithms learn from biased historical policing patterns, then direct police to the same neighborhoods, generating more arrests that “confirm” the algorithm’s predictions\nThe key distinction is that crime data doesn’t reveal actual crime - it reveals policing patterns, because crime data is always socially constructed, selectively enforced, and organizationally filtered\n\n##Coding Techniques\n\nCount-based crime models use Poisson regression initially, but we check for overdispersion using dispersion statistics and switch to Negative Binomial regression when variance exceeds the mean\nKernel Density Estimation (KDE) with density.ppp(sigma = 1000) creates a baseline prediction using only past crime locations, which we compare against feature-based models to assess whether complexity adds value\nThe validation workflow is: train on 2017 data → create risk predictions → test on 2018 hold-out data → calculate hit rates by risk quintile → compare model performance to KDE baseline\n\n##Questions & Challenges\n\nThe biggest challenge is the impossibility of “neutral” crime data - even if we exclude racially biased drug arrest data, aren’t property crime and assault enforcement also biased? Where do we draw the line?\nAnother challenge is distinguishing between “cleaning” data (removing technical errors) versus addressing fundamental problems where the data itself reflects systemic injustice and cannot be “fixed”\nUnderstanding when a statistically “good” model (that beats KDE and performs well on hold-out data) can still be socially harmful requires considering who benefits, who is harmed, and what feedback loops are created\n\n##Connections to Policy\n\nCase studies from Baltimore, NYPD, and other departments revealed extensive stat manipulation including 14,000+ serious assaults misrecorded as minor offenses and officers planting evidence to meet arrest quotas\nPredictive policing systems exclude white-collar crime, wage theft ($300B+ annually), and corporate fraud from predictions, focusing enforcement only on street crime despite its much lower economic impact\nConsent decrees attempt to address police misconduct but don’t prevent historically biased data from training current algorithms, meaning past injustice becomes embedded in “objective” predictions\n\n##Reflection\n\nThe most important lesson is asking “SHOULD we build this?” before “HOW do we build this?” - technical accuracy doesn’t equal social justice, and a statistically superior model can still be ethically problematic\nSame technical methods could be redirected toward predictive models for justice (predicting eviction risk, health crises, food insecurity) where predictions lead to help instead of punishment"
  },
  {
    "objectID": "weekly-notes/week-06-notes.html",
    "href": "weekly-notes/week-06-notes.html",
    "title": "Week 6 Notes",
    "section": "",
    "text": "###Key Concepts Learned\n\nWe learned how to expand regression models beyond simple continuous variables by incorporating categorical variables (like neighborhoods), interaction terms, and polynomial terms to capture complex relationships\nThe hedonic model framework was introduced, showing how housing prices are determined by structural features, spatial characteristics, and neighborhood effects bundled together\nWe explored Tobler’s First Law and spatial autocorrelation, understanding that nearby houses influence each other’s values, which requires us to add spatial features like buffer aggregations, k-nearest neighbors, and distance to amenities\n\n###Coding Techniques\n\nst_as_sf(coords = c(“Longitude”, “Latitude”), crs = 4326) converts dataframe to spatial object, and st_transform(‘ESRI:102286’) projects to appropriate coordinate system for distance calculations\nst_join(nhoods, join = st_intersects) performs spatial join to assign each house to its neighborhood polygon\nadd_count(name) followed by mutate(name_cv = if_else(n &lt; 10, “Small_Neighborhoods”, as.character(name))) groups sparse categories to prevent cross-validation errors\nst_buffer(dist = 500) creates circular buffers around points for spatial aggregation, and st_nn(k = 3) finds k-nearest neighbors for spatial lag calculations\ntrain(SalePrice ~ LivingArea + as.factor(name_cv), method = “lm”, trControl = trainControl(method = “cv”, number = 10)) runs 10-fold cross-validation with categorical fixed effects\n\n###Questions & Challenges\n\nThe biggest challenge is handling sparse categories in cross-validation - when neighborhoods have fewer than 10 observations, random splits can put all instances in one fold, causing the model to fail on unseen categories\nCreating meaningful spatial features requires domain knowledge about what distances matter (500ft vs 1 mile buffers for crime) and which amenities actually affect prices\nDeciding between grouping small neighborhoods into “Other” versus dropping them entirely involves trade-offs between model stability and potentially excluding marginalized communities from analysis\n\n###Connections to Policy\n\nFixed effects models can reveal systematic price disparities across neighborhoods that aren’t explained by structural features alone, potentially indicating discrimination or historical disinvestment patterns\nUnderstanding which spatial features (crime, transit access, parks) have the strongest effects on property values helps prioritize infrastructure investments and target interventions where they’ll have maximum impact\nThe sparse category problem highlights data equity issues - neighborhoods with few sales are often lower-income areas, and excluding them from analysis risks making policy recommendations that ignore communities most in need\n\nReflection\n\nThe progression from structural-only models (R² = 0.13) to models with spatial features and fixed effects (much lower RMSE) demonstrates how critical location is to housing values beyond just physical characteristics\nI need to always check category counts before running cross-validation with categorical variables to avoid the “factor has new levels” error\nThe insight that fixed effects bundle many unmeasured factors (schools, prestige, amenities) explains why they often provide the biggest prediction improvement, even though they’re less interpretable than specific spatial features"
  },
  {
    "objectID": "weekly-notes/week-04-notes.html",
    "href": "weekly-notes/week-04-notes.html",
    "title": "Week 4 Notes",
    "section": "",
    "text": "We learned about spatial data in R. We talked about vector data model and different type of spatial data to use in R. We also talked about some spatial data operation of how to filter and select the data that we need. We also mentioned some corrdinate reference system and how each is created."
  },
  {
    "objectID": "weekly-notes/week-04-notes.html#key-concepts-learned",
    "href": "weekly-notes/week-04-notes.html#key-concepts-learned",
    "title": "Week 4 Notes",
    "section": "",
    "text": "We learned about spatial data in R. We talked about vector data model and different type of spatial data to use in R. We also talked about some spatial data operation of how to filter and select the data that we need. We also mentioned some corrdinate reference system and how each is created."
  },
  {
    "objectID": "weekly-notes/week-04-notes.html#coding-techniques",
    "href": "weekly-notes/week-04-notes.html#coding-techniques",
    "title": "Week 4 Notes",
    "section": "Coding Techniques",
    "text": "Coding Techniques\n\nBelow are some of the core techniques that we covered in class and used in class. | st_filter() | Select features by spatial relationship | Find neighboring counties | | st_buffer() | Create zones around features | Hospital service areas | | st_intersects() | Test spatial overlap | Check access to services | | st_disjoint() | Test spatial separation | Find rural areas | | st_join() | Join by location | Add county info to tracts | | st_union() | Combine geometries | Merge overlapping buffers | | st_intersection() | Clip geometries | Calculate overlap areas | | st_transform() | Change CRS | Accurate distance/area calculations | | st_area() | Calculate areas | County sizes, coverage | | st_distance() | Calculate distances | Distance to facilities |"
  },
  {
    "objectID": "weekly-notes/week-04-notes.html#questions-challenges",
    "href": "weekly-notes/week-04-notes.html#questions-challenges",
    "title": "Week 4 Notes",
    "section": "Questions & Challenges",
    "text": "Questions & Challenges\n\nI think it is very important to understand what kind of spatial relationship you want to explore. That determines what types of function you need to explore relationships. ‘st_filter()’ and ‘st_intersection()’ might seem confusing at first but they are serving differe kinds of operation."
  },
  {
    "objectID": "weekly-notes/week-04-notes.html#connections-to-policy",
    "href": "weekly-notes/week-04-notes.html#connections-to-policy",
    "title": "Week 4 Notes",
    "section": "Connections to Policy",
    "text": "Connections to Policy\n\nThese operations are heavily related to Policy. Policy decision need spatial analysis to fully understand the root issues."
  },
  {
    "objectID": "weekly-notes/week-04-notes.html#reflection",
    "href": "weekly-notes/week-04-notes.html#reflection",
    "title": "Week 4 Notes",
    "section": "Reflection",
    "text": "Reflection\n\nBeing able to master the spatial analysis and GIS in R will unlock great potential in coming up with policy decision and more in-depth analysis for urban issues."
  },
  {
    "objectID": "weekly-notes/week-02-notes.html",
    "href": "weekly-notes/week-02-notes.html",
    "title": "Week 2 Notes",
    "section": "",
    "text": "I learned about topics on algorithmic decision making and what is census data.\nWe also went through a scenarios practices where we designed the ethical algorithms.\nThe importance of having algorithmic decision making.\nCensus data is really large but it has a lot of potential to do some great things."
  },
  {
    "objectID": "weekly-notes/week-02-notes.html#key-concepts-learned",
    "href": "weekly-notes/week-02-notes.html#key-concepts-learned",
    "title": "Week 2 Notes",
    "section": "",
    "text": "I learned about topics on algorithmic decision making and what is census data.\nWe also went through a scenarios practices where we designed the ethical algorithms.\nThe importance of having algorithmic decision making.\nCensus data is really large but it has a lot of potential to do some great things."
  },
  {
    "objectID": "weekly-notes/week-02-notes.html#coding-techniques",
    "href": "weekly-notes/week-02-notes.html#coding-techniques",
    "title": "Week 2 Notes",
    "section": "Coding Techniques",
    "text": "Coding Techniques\n\nFunctions that I learned include “str_remove(), str_extract(), str_replace(), case_when(),"
  },
  {
    "objectID": "weekly-notes/week-02-notes.html#questions-challenges",
    "href": "weekly-notes/week-02-notes.html#questions-challenges",
    "title": "Week 2 Notes",
    "section": "Questions & Challenges",
    "text": "Questions & Challenges\n\nI think it will take some time to fully understand how to best use the Census Tract. I think it will take a lot of trials and error. The more pract the better we will get at it."
  },
  {
    "objectID": "weekly-notes/week-02-notes.html#connections-to-policy",
    "href": "weekly-notes/week-02-notes.html#connections-to-policy",
    "title": "Week 2 Notes",
    "section": "Connections to Policy",
    "text": "Connections to Policy\n\nIt is important to understand why we want to do algorthimic decision making in policy.\nData analytics is a subjective process because it involves human decisions. So we need to know to be best clean the data and understand the data."
  },
  {
    "objectID": "weekly-notes/week-02-notes.html#reflection",
    "href": "weekly-notes/week-02-notes.html#reflection",
    "title": "Week 2 Notes",
    "section": "Reflection",
    "text": "Reflection\n\nLooking forward to explore the census data more and to see what insightful result we could get. Also really looking forward to the data update in the end of this year. To see how status have changed for people."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "MUSA 5080 Portfolio",
    "section": "",
    "text": "This portfolio documents my learning journey in Public Policy Analytics (MUSA 5080).\n\n\nAdvanced spatial analysis and data science for urban planning and public policy.\n\n\n\n\nWeekly Notes: My learning reflections and key concepts\nAssignment: Completed assignments and analyses\nFinal Project: Capstone modeling challenge\n\n\n\n\n\nMy name is Ye and I did my Bachelor in Urban Planning at USC ✌️✌️.\nMy email is yezhang1@upenn.edu\nMy Github user name is yezhanggg\nI think it is a requirement. Also I don’t have any prior experience with R and I think it is powerful tool that would be really helpful for me to know. It has so many potential and implications that I could use in multiple areas and projects.\n\n\n\n\n\nEmail: [yezhang1@design.upenn.edu]\nGitHub: [@yezhanggg]"
  },
  {
    "objectID": "index.html#about-this-course",
    "href": "index.html#about-this-course",
    "title": "MUSA 5080 Portfolio",
    "section": "",
    "text": "Advanced spatial analysis and data science for urban planning and public policy."
  },
  {
    "objectID": "index.html#portfolio-structure",
    "href": "index.html#portfolio-structure",
    "title": "MUSA 5080 Portfolio",
    "section": "",
    "text": "Weekly Notes: My learning reflections and key concepts\nAssignment: Completed assignments and analyses\nFinal Project: Capstone modeling challenge"
  },
  {
    "objectID": "index.html#about-me",
    "href": "index.html#about-me",
    "title": "MUSA 5080 Portfolio",
    "section": "",
    "text": "My name is Ye and I did my Bachelor in Urban Planning at USC ✌️✌️.\nMy email is yezhang1@upenn.edu\nMy Github user name is yezhanggg\nI think it is a requirement. Also I don’t have any prior experience with R and I think it is powerful tool that would be really helpful for me to know. It has so many potential and implications that I could use in multiple areas and projects."
  },
  {
    "objectID": "index.html#contact",
    "href": "index.html#contact",
    "title": "MUSA 5080 Portfolio",
    "section": "",
    "text": "Email: [yezhang1@design.upenn.edu]\nGitHub: [@yezhanggg]"
  },
  {
    "objectID": "weekly-notes/week-01-notes.html",
    "href": "weekly-notes/week-01-notes.html",
    "title": "Week 1 Notes - Course Introduction",
    "section": "",
    "text": "What is Git, Github, Quarto, and full term for YAML (YAML Ain’t Markup Language).\nSome basic R coding language and went over an example on CAR data."
  },
  {
    "objectID": "weekly-notes/week-01-notes.html#key-concepts-learned",
    "href": "weekly-notes/week-01-notes.html#key-concepts-learned",
    "title": "Week 1 Notes - Course Introduction",
    "section": "",
    "text": "What is Git, Github, Quarto, and full term for YAML (YAML Ain’t Markup Language).\nSome basic R coding language and went over an example on CAR data."
  },
  {
    "objectID": "weekly-notes/week-01-notes.html#coding-techniques",
    "href": "weekly-notes/week-01-notes.html#coding-techniques",
    "title": "Week 1 Notes - Course Introduction",
    "section": "Coding Techniques",
    "text": "Coding Techniques\n\nBold text\nItalic text\nBold and italic\ncode text\nStrikethrough\nselect() - choose columns\nfilter() - choose rows\nmutate() - create new variables\nsummarize() - calculate statistics\ngroup_by() - operate on groups]\ntitle: “My Analysis” author: “Your Name” date: today format: html"
  },
  {
    "objectID": "weekly-notes/week-01-notes.html#questions-challenges",
    "href": "weekly-notes/week-01-notes.html#questions-challenges",
    "title": "Week 1 Notes - Course Introduction",
    "section": "Questions & Challenges",
    "text": "Questions & Challenges\n\nI think it is hard to understand all the R code and notations.\nThis is my first time interact with R and Github. So I need to try everything first and I will find more challenges."
  },
  {
    "objectID": "weekly-notes/week-01-notes.html#connections-to-policy",
    "href": "weekly-notes/week-01-notes.html#connections-to-policy",
    "title": "Week 1 Notes - Course Introduction",
    "section": "Connections to Policy",
    "text": "Connections to Policy\n\nBoth the analysis and the data itself can be biased. The reason is that data were collected by someone who might be biased."
  },
  {
    "objectID": "weekly-notes/week-01-notes.html#reflection",
    "href": "weekly-notes/week-01-notes.html#reflection",
    "title": "Week 1 Notes - Course Introduction",
    "section": "Reflection",
    "text": "Reflection\n\nQuarto is really interesting. Looking forward to see more what Quarto can do.\nThrough our assignment. I think it will be a great practice for me."
  },
  {
    "objectID": "weekly-notes/week-03-notes.html",
    "href": "weekly-notes/week-03-notes.html",
    "title": "Week 3 Notes",
    "section": "",
    "text": "We learn some ways to do visualizations and the asethetic mapping. What is a ggplot. We also learned how to join data."
  },
  {
    "objectID": "weekly-notes/week-03-notes.html#key-concepts-learned",
    "href": "weekly-notes/week-03-notes.html#key-concepts-learned",
    "title": "Week 3 Notes",
    "section": "",
    "text": "We learn some ways to do visualizations and the asethetic mapping. What is a ggplot. We also learned how to join data."
  },
  {
    "objectID": "weekly-notes/week-03-notes.html#coding-techniques",
    "href": "weekly-notes/week-03-notes.html#coding-techniques",
    "title": "Week 3 Notes",
    "section": "Coding Techniques",
    "text": "Coding Techniques\n\nIn class, we learned the basics of ggplot2 and joins in dplyr. In ggplot2, aesthetic mappings like x, y, color, fill, size, shape, and alpha control visual elements such as position, color, and transparency. A typical plot follows the structure ggplot(data) + aes(x, y) + geom_something() + additional_layers(), and layers are added with +. We also covered data joins in dplyr: left_join() keeps all rows from the left dataset, right_join() keeps all from the right, inner_join() keeps only matching rows, and full_join() keeps all rows from both, with left_join() being the most common for adding columns to a main dataset."
  },
  {
    "objectID": "weekly-notes/week-03-notes.html#questions-challenges",
    "href": "weekly-notes/week-03-notes.html#questions-challenges",
    "title": "Week 3 Notes",
    "section": "Questions & Challenges",
    "text": "Questions & Challenges\n\nI think it is very important to follow all the steps correct to make everything work."
  },
  {
    "objectID": "weekly-notes/week-03-notes.html#connections-to-policy",
    "href": "weekly-notes/week-03-notes.html#connections-to-policy",
    "title": "Week 3 Notes",
    "section": "Connections to Policy",
    "text": "Connections to Policy\n\nLearning ggplot2 and dplyr helps us turn data into clear visuals and summaries that make policy trends easier to understand. This makes it simpler to spot patterns and share insights that can guide better, data-informed decisions."
  },
  {
    "objectID": "weekly-notes/week-03-notes.html#reflection",
    "href": "weekly-notes/week-03-notes.html#reflection",
    "title": "Week 3 Notes",
    "section": "Reflection",
    "text": "Reflection\n\nTo become more proficient in this process, it requires a lot of time. Even following the class excercise could be challenging. More practice!"
  },
  {
    "objectID": "weekly-notes/week-05-notes.html",
    "href": "weekly-notes/week-05-notes.html",
    "title": "Week 5 Notes",
    "section": "",
    "text": "We learned about linear regression and different estimator and their purposes. We talked about how to differentiate inference and prediction. We also built our first model to test statistical significance. We talked about the case of overfitting and how test what is considered as a good model."
  },
  {
    "objectID": "weekly-notes/week-05-notes.html#key-concepts-learned",
    "href": "weekly-notes/week-05-notes.html#key-concepts-learned",
    "title": "Week 5 Notes",
    "section": "",
    "text": "We learned about linear regression and different estimator and their purposes. We talked about how to differentiate inference and prediction. We also built our first model to test statistical significance. We talked about the case of overfitting and how test what is considered as a good model."
  },
  {
    "objectID": "weekly-notes/week-05-notes.html#coding-techniques",
    "href": "weekly-notes/week-05-notes.html#coding-techniques",
    "title": "Week 5 Notes",
    "section": "Coding Techniques",
    "text": "Coding Techniques\n\nBelow are some of the core techniques that we covered in class and used in class.\nlm(formula = attribute ~ attribute, data = pa_data) is how to built a model.\nsummary of the model will provide all the important statistics.\nset.seed(123) is to set random number generator\nsqrt(mean is to find RMSE\ntrain_control &lt;- trainControl(method = “cv”, number = 10) sets up the cross-validation technique"
  },
  {
    "objectID": "weekly-notes/week-05-notes.html#questions-challenges",
    "href": "weekly-notes/week-05-notes.html#questions-challenges",
    "title": "Week 5 Notes",
    "section": "Questions & Challenges",
    "text": "Questions & Challenges\n\nI think the hard question is how to find the best predictor for you model while also taking the consideration of different linear model assumption and the biases that we want to avoid. The in-class activity of finding home value was quite challeneging by finding the best predictors that are significant."
  },
  {
    "objectID": "weekly-notes/week-05-notes.html#connections-to-policy",
    "href": "weekly-notes/week-05-notes.html#connections-to-policy",
    "title": "Week 5 Notes",
    "section": "Connections to Policy",
    "text": "Connections to Policy\n\nThis is super useful when we trying to find the correlation or coenction between different inputs and results especially when we are designing public policy."
  },
  {
    "objectID": "weekly-notes/week-05-notes.html#reflection",
    "href": "weekly-notes/week-05-notes.html#reflection",
    "title": "Week 5 Notes",
    "section": "Reflection",
    "text": "Reflection\n\nIt is importan to remember the difference between inference and prediction. We need to carefully look at the model to prevent in-sample fit and overfitting. It is important to check assumption and plot things out to see."
  },
  {
    "objectID": "weekly-notes/week-07-notes.html",
    "href": "weekly-notes/week-07-notes.html",
    "title": "Week 7 Notes",
    "section": "",
    "text": "##Key Concepts Learned\n\nWe learned about spatial autocorrelation and Tobler’s First Law of Geography - near things are more related than distant things\nMoran’s I is a diagnostic tool that detects spatial patterns in model errors, revealing if we’re missing important spatial relationships\nThe key distinction is that spatial lag/error models are for inference about spillover effects, while spatial feature engineering is for prediction\n\n##Coding Techniques\n\nmoran.mc(errors, weights, nsim = 999) tests for spatial autocorrelation in model residuals using Monte Carlo simulation\nAdding progress = FALSE to get_acs(), get_decennial(), and tigris functions suppresses progress messages in rendered documents\nThe workflow is: fit model → extract errors → create spatial weights → test Moran’s I → add spatial features if needed\n\n##Questions & Challenges\n\nThe biggest challenge is deciding what to do when Moran’s I shows significant spatial autocorrelation - should we add more spatial features at different buffer distances or try neighborhood fixed effects?\nAnother challenge is avoiding the “simultaneity problem” where using neighbor prices to predict prices creates circular logic and data leakage\nFinding the right balance between capturing spatial relationships and avoiding overfitting requires both statistical testing and domain knowledge\n\n##Connections to Policy\n\nSpatially clustered model errors can reveal systematic bias in property assessments, leading to inequitable taxation in certain neighborhoods\nUnderstanding spatial autocorrelation helps policymakers identify spillover effects like how crime or transit investments affect surrounding property values\nThis diagnostic approach ensures predictive models don’t perpetuate existing geographic inequalities in housing markets\n\n##Reflection\n\nMoran’s I is a diagnostic tool, not a solution - it tells us our model needs improvement but not exactly how to fix it\nThe iterative process of checking diagnostics, adding spatial features, and re-testing is essential rather than just relying on R-squared\nMoving forward, I’ll always map residuals and test for spatial patterns as part of my standard model evaluation workflow"
  },
  {
    "objectID": "labs/Zhang_Ye_Assignment2.html",
    "href": "labs/Zhang_Ye_Assignment2.html",
    "title": "Assignment 2: Spatial Analysis and Visualization",
    "section": "",
    "text": "Learning Objectives: - Apply spatial operations to answer policy-relevant research questions - Integrate census demographic data with spatial analysis - Create publication-quality visualizations and maps - Work with spatial data from multiple sources - Communicate findings effectively for policy audiences"
  },
  {
    "objectID": "labs/Zhang_Ye_Assignment2.html#assignment-overview",
    "href": "labs/Zhang_Ye_Assignment2.html#assignment-overview",
    "title": "Assignment 2: Spatial Analysis and Visualization",
    "section": "",
    "text": "Learning Objectives: - Apply spatial operations to answer policy-relevant research questions - Integrate census demographic data with spatial analysis - Create publication-quality visualizations and maps - Work with spatial data from multiple sources - Communicate findings effectively for policy audiences"
  },
  {
    "objectID": "labs/Zhang_Ye_Assignment2.html#part-1-healthcare-access-for-vulnerable-populations",
    "href": "labs/Zhang_Ye_Assignment2.html#part-1-healthcare-access-for-vulnerable-populations",
    "title": "Assignment 2: Spatial Analysis and Visualization",
    "section": "Part 1: Healthcare Access for Vulnerable Populations",
    "text": "Part 1: Healthcare Access for Vulnerable Populations\n\nResearch Question\nWhich Pennsylvania counties have the highest proportion of vulnerable populations (elderly + low-income) living far from hospitals?\n\nStep 1: Data Collection (5 points)\n\n# Load required packages\nlibrary(sf)\nlibrary(tidyverse)\nlibrary(tigris)\nlibrary(tidycensus)\nlibrary(scales)\nlibrary(patchwork)\nlibrary(here)\n\noptions(tigris_use_cache = TRUE)\noptions(tigris_progress = FALSE)\n\ncensus_api_key(\"138db656646183a18ae51e4a7c1e1f5cd64d4b40\")\ninstall = TRUE\n\n# Load spatial data\n\npa_counties &lt;- st_read(here(\"~/Documents/GitHub/portfolio-setup-yezhanggg/labs/assignment_2_data/Pennsylvania_County_Boundaries.shp\"))\n\nReading layer `Pennsylvania_County_Boundaries' from data source \n  `/Users/ye/Documents/GitHub/portfolio-setup-yezhanggg/labs/assignment_2_data/Pennsylvania_County_Boundaries.shp' \n  using driver `ESRI Shapefile'\nSimple feature collection with 67 features and 19 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: -8963377 ymin: 4825316 xmax: -8314404 ymax: 5201413\nProjected CRS: WGS 84 / Pseudo-Mercator\n\nhospitals &lt;- st_read(here(\"~/Documents/GitHub/portfolio-setup-yezhanggg/labs/assignment_2_data/hospitals.geojson\"))\n\nReading layer `hospitals' from data source \n  `/Users/ye/Documents/GitHub/portfolio-setup-yezhanggg/labs/assignment_2_data/hospitals.geojson' \n  using driver `GeoJSON'\nSimple feature collection with 223 features and 11 fields\nGeometry type: POINT\nDimension:     XY\nBounding box:  xmin: -80.49621 ymin: 39.75163 xmax: -74.86704 ymax: 42.13403\nGeodetic CRS:  WGS 84\n\ncensus_tracts &lt;- tracts(state = \"PA\", cb = TRUE)\n\n# Check that all data loaded correctly\n\nst_crs(pa_counties)\n\nCoordinate Reference System:\n  User input: WGS 84 / Pseudo-Mercator \n  wkt:\nPROJCRS[\"WGS 84 / Pseudo-Mercator\",\n    BASEGEOGCRS[\"WGS 84\",\n        ENSEMBLE[\"World Geodetic System 1984 ensemble\",\n            MEMBER[\"World Geodetic System 1984 (Transit)\"],\n            MEMBER[\"World Geodetic System 1984 (G730)\"],\n            MEMBER[\"World Geodetic System 1984 (G873)\"],\n            MEMBER[\"World Geodetic System 1984 (G1150)\"],\n            MEMBER[\"World Geodetic System 1984 (G1674)\"],\n            MEMBER[\"World Geodetic System 1984 (G1762)\"],\n            MEMBER[\"World Geodetic System 1984 (G2139)\"],\n            MEMBER[\"World Geodetic System 1984 (G2296)\"],\n            ELLIPSOID[\"WGS 84\",6378137,298.257223563,\n                LENGTHUNIT[\"metre\",1]],\n            ENSEMBLEACCURACY[2.0]],\n        PRIMEM[\"Greenwich\",0,\n            ANGLEUNIT[\"degree\",0.0174532925199433]],\n        ID[\"EPSG\",4326]],\n    CONVERSION[\"Popular Visualisation Pseudo-Mercator\",\n        METHOD[\"Popular Visualisation Pseudo Mercator\",\n            ID[\"EPSG\",1024]],\n        PARAMETER[\"Latitude of natural origin\",0,\n            ANGLEUNIT[\"degree\",0.0174532925199433],\n            ID[\"EPSG\",8801]],\n        PARAMETER[\"Longitude of natural origin\",0,\n            ANGLEUNIT[\"degree\",0.0174532925199433],\n            ID[\"EPSG\",8802]],\n        PARAMETER[\"False easting\",0,\n            LENGTHUNIT[\"metre\",1],\n            ID[\"EPSG\",8806]],\n        PARAMETER[\"False northing\",0,\n            LENGTHUNIT[\"metre\",1],\n            ID[\"EPSG\",8807]]],\n    CS[Cartesian,2],\n        AXIS[\"easting (X)\",east,\n            ORDER[1],\n            LENGTHUNIT[\"metre\",1]],\n        AXIS[\"northing (Y)\",north,\n            ORDER[2],\n            LENGTHUNIT[\"metre\",1]],\n    USAGE[\n        SCOPE[\"Web mapping and visualisation.\"],\n        AREA[\"World between 85.06°S and 85.06°N.\"],\n        BBOX[-85.06,-180,85.06,180]],\n    ID[\"EPSG\",3857]]\n\nst_crs(hospitals)\n\nCoordinate Reference System:\n  User input: WGS 84 \n  wkt:\nGEOGCRS[\"WGS 84\",\n    DATUM[\"World Geodetic System 1984\",\n        ELLIPSOID[\"WGS 84\",6378137,298.257223563,\n            LENGTHUNIT[\"metre\",1]]],\n    PRIMEM[\"Greenwich\",0,\n        ANGLEUNIT[\"degree\",0.0174532925199433]],\n    CS[ellipsoidal,2],\n        AXIS[\"geodetic latitude (Lat)\",north,\n            ORDER[1],\n            ANGLEUNIT[\"degree\",0.0174532925199433]],\n        AXIS[\"geodetic longitude (Lon)\",east,\n            ORDER[2],\n            ANGLEUNIT[\"degree\",0.0174532925199433]],\n    ID[\"EPSG\",4326]]\n\nst_crs(census_tracts)\n\nCoordinate Reference System:\n  User input: NAD83 \n  wkt:\nGEOGCRS[\"NAD83\",\n    DATUM[\"North American Datum 1983\",\n        ELLIPSOID[\"GRS 1980\",6378137,298.257222101,\n            LENGTHUNIT[\"metre\",1]]],\n    PRIMEM[\"Greenwich\",0,\n        ANGLEUNIT[\"degree\",0.0174532925199433]],\n    CS[ellipsoidal,2],\n        AXIS[\"latitude\",north,\n            ORDER[1],\n            ANGLEUNIT[\"degree\",0.0174532925199433]],\n        AXIS[\"longitude\",east,\n            ORDER[2],\n            ANGLEUNIT[\"degree\",0.0174532925199433]],\n    ID[\"EPSG\",4269]]\n\n\nQuestions to answer: - How many hospitals are in your dataset? There aer 223 hosptials in my dataset. - How many census tracts? There are 3445 census tracts. - What coordinate reference system is each dataset in? PA Counties is in WGS 84 Pseudo Mercator, Hospitals is in WGS 84 and Census Tracts is in NAD83.\n\n\n\nStep 2: Get Demographic Data\n\n# Get demographic data from ACS\nacs_vars_2022 &lt;- load_variables(2022, \"acs5\", cache = TRUE)\n\n# Get demographic data from ACS\n\npa_demographics &lt;- get_acs(\n  geography = \"tract\",\n  state = \"PA\",\n  variables = c(\n    total_pop = \"B01003_001\",     \n    median_income = \"B19013_001\",   \n    male_65_66 = \"B01001_020\",\n    male_67_69 = \"B01001_021\",\n    male_70_74 = \"B01001_022\",\n    male_75_79 = \"B01001_023\",\n    male_80_84 = \"B01001_024\",\n    male_85_over = \"B01001_025\",\n    female_65_66 = \"B01001_044\",\n    female_67_69 = \"B01001_045\",\n    female_70_74 = \"B01001_046\",\n    female_75_79 = \"B01001_047\",\n    female_80_84 = \"B01001_048\",\n    female_85_over = \"B01001_049\"\n  ),\n  year = 2022,\n  survey = \"acs5\",\n  output = \"wide\",\n  geometry = FALSE\n)\n\npa_demographics &lt;- pa_demographics %&gt;%\n  mutate(\n    pop_65_over = male_65_66E + male_67_69E + male_70_74E + male_75_79E + \n                  male_80_84E + male_85_overE + \n                  female_65_66E + female_67_69E + female_70_74E + \n                  female_75_79E + female_80_84E + female_85_overE,\n    total_pop = total_popE,\n    median_income = median_incomeE\n  ) %&gt;%\n select(GEOID, total_pop, median_income, pop_65_over)\n\n# Join to tract boundaries\n\ncensus_tracts &lt;- census_tracts %&gt;%\n  left_join(pa_demographics, by = \"GEOID\")\n\nQuestions to answer: - What year of ACS data are you using? I am using ACS 2022 data. - How many tracts have missing income data? There are 62 tracts missing income data. - What is the median income across all PA census tracts? The median household income is $70,188 for all PA Census Tracts.\n\n\n\nStep 3: Define Vulnerable Populations\n\n# Filter for vulnerable tracts based on your criteria\n\nvulnerable_tracts &lt;- census_tracts %&gt;%\n  mutate(pct_elderly = (pop_65_over / total_pop) * 100) %&gt;%\n  filter(median_income &lt; 40000 & pct_elderly &gt; 21)\n\nQuestions to answer: - What income threshold did you choose and why? The income threshold that I chose is 40000. Through research, the poverty guideline in 2024 for a family of four is 31,200.The median household income in PA is $76,081 in 2023. Older population might be more vulnerable and it would be reasonable to adjust threshold higher. It would be reasonable to use this as the guideline to set for the low median household income threshold. - What elderly population threshold did you choose and why? The elderly population threhold that I choose is 21%. Because after reviewing the acs data, there are 14 different age groups and there are three age groups that include age over 65. - How many tracts meet your vulnerability criteria? There are 59 vulnerable tracts that meet my standard of below 40,000 income and over 21% are elderly people. - What percentage of PA census tracts are considered vulnerable by your definition? 1.7% of the PA census tracts are considered as vulnerable by my definition.\n\n\n\nStep 4: Calculate Distance to Hospitals\n\n# Transform to appropriate projected CRS\npa_counties &lt;- pa_counties %&gt;% st_transform(3365)\nhospitals &lt;- hospitals %&gt;% st_transform(3365)\ncensus_tracts &lt;- census_tracts %&gt;% st_transform(3365)\nvulnerable_tracts &lt;- vulnerable_tracts %&gt;% st_transform(3365)\n\n# Calculate distance from each tract centroid to nearest hospital\ntract_centers &lt;- st_centroid(vulnerable_tracts) \ndistance_matrix &lt;- st_distance(tract_centers, hospitals)\n\nmin_distances &lt;- apply(distance_matrix, 1, min)\n\nvulnerable_tracts &lt;- vulnerable_tracts %&gt;%\n  mutate(\n    distance_to_nearest_hospital_m = as.numeric(min_distances),\n    distance_to_nearest_hospital_miles = distance_to_nearest_hospital_m / 1609.34\n  )\n\nQuestions to answer: - What is the average distance to the nearest hospital for vulnerable tracts? The average distance is 7.4093 miles. - What is the maximum distance? The max distance is 61.2651 miles - How many vulnerable tracts are more than 15 miles from the nearest hospital? Based on my standards. There are 4 vulnerable tracts that are more than 15 miles from the nearest hospital.\n\nst_crs(vulnerable_tracts)$units\n\n[1] \"us-ft\"\n\nsummary(vulnerable_tracts)\n\n   STATEFP            COUNTYFP           TRACTCE            GEOIDFQ         \n Length:59          Length:59          Length:59          Length:59         \n Class :character   Class :character   Class :character   Class :character  \n Mode  :character   Mode  :character   Mode  :character   Mode  :character  \n                                                                            \n                                                                            \n                                                                            \n    GEOID               NAME             NAMELSAD            STUSPS         \n Length:59          Length:59          Length:59          Length:59         \n Class :character   Class :character   Class :character   Class :character  \n Mode  :character   Mode  :character   Mode  :character   Mode  :character  \n                                                                            \n                                                                            \n                                                                            \n  NAMELSADCO         STATE_NAME            LSAD               ALAND        \n Length:59          Length:59          Length:59          Min.   : 262958  \n Class :character   Class :character   Class :character   1st Qu.: 692938  \n Mode  :character   Mode  :character   Mode  :character   Median :1153118  \n                                                          Mean   :1782766  \n                                                          3rd Qu.:1827787  \n                                                          Max.   :9908475  \n     AWATER         total_pop    median_income    pop_65_over    \n Min.   :     0   Min.   :  19   Min.   :12336   Min.   :   9.0  \n 1st Qu.:     0   1st Qu.:1600   1st Qu.:24081   1st Qu.: 458.5  \n Median :     0   Median :2373   Median :33771   Median : 599.0  \n Mean   : 51871   Mean   :2456   Mean   :30485   Mean   : 658.7  \n 3rd Qu.: 36576   3rd Qu.:2922   3rd Qu.:37470   3rd Qu.: 787.0  \n Max.   :476215   Max.   :5197   Max.   :39891   Max.   :1649.0  \n          geometry   pct_elderly    distance_to_nearest_hospital_m\n MULTIPOLYGON :59   Min.   :21.04   Min.   :  312.1               \n epsg:3365    : 0   1st Qu.:22.80   1st Qu.: 4124.4               \n +proj=lcc ...: 0   Median :25.50   Median : 7340.2               \n                    Mean   :27.47   Mean   :11924.0               \n                    3rd Qu.:31.17   3rd Qu.:14265.6               \n                    Max.   :47.37   Max.   :98596.3               \n distance_to_nearest_hospital_miles\n Min.   : 0.1939                   \n 1st Qu.: 2.5628                   \n Median : 4.5610                   \n Mean   : 7.4093                   \n 3rd Qu.: 8.8643                   \n Max.   :61.2651                   \n\nsum(vulnerable_tracts$distance_to_nearest_hospital_miles &gt; 15)\n\n[1] 4\n\n\n\n\n\nStep 5: Identify Underserved Areas\n\n# Create underserved variable\n\nvulnerable_tracts &lt;- vulnerable_tracts %&gt;%\n  mutate(underserved = distance_to_nearest_hospital_miles &gt; 15)\n\nQuestions to answer: - How many tracts are underserved? There are 4 tracts that are underserved. - What percentage of vulnerable tracts are underserved? The percentage for vulnerable tracts are underserved is 6.78%. - Does this surprise you? Why or why not? This percentage is little bit lower than I expected. I was expected the vulnerable tracts that are underserved might takes up a larger percentage. It might be how I designed what is considered as a vulnerable tracts.\n\n\n\nStep 6: Aggregate to County Level\n\n# Spatial join tracts to counties\n\ntracts_with_counties &lt;- vulnerable_tracts %&gt;%\n  st_join(pa_counties %&gt;% select(COUNTY_NAM))\n\n# Aggregate statistics by county\n\nvulnerable_by_county &lt;- tracts_with_counties %&gt;%\n  st_drop_geometry() %&gt;%\n  group_by(COUNTY_NAM) %&gt;%\n  summarize(\n    n_vulnerable_tracts = n(),\n    n_underserved_tracts = sum(underserved == TRUE, na.rm = TRUE),\n    pct_vulnerable_underserved = (sum(underserved == TRUE, na.rm = TRUE) / n()) * 100,\n    avg_distance_to_hospital = mean(distance_to_nearest_hospital_miles, na.rm = TRUE),\n    total_vulnerable_pop = sum(total_pop, na.rm = TRUE)\n  ) %&gt;%\n  arrange(desc(n_vulnerable_tracts))\n\n\nvulnerable_by_county %&gt;%\n  arrange(desc(pct_vulnerable_underserved)) %&gt;%\n  head(5)\n\n# A tibble: 5 × 6\n  COUNTY_NAM     n_vulnerable_tracts n_underserved_tracts pct_vulnerable_under…¹\n  &lt;chr&gt;                        &lt;int&gt;                &lt;int&gt;                  &lt;dbl&gt;\n1 CAMERON                          1                    1                 100   \n2 NORTHUMBERLAND                   1                    1                 100   \n3 CAMBRIA                          4                    1                  25   \n4 ALLEGHENY                       18                    1                   5.56\n5 PHILADELPHIA                    10                    0                   0   \n# ℹ abbreviated name: ¹​pct_vulnerable_underserved\n# ℹ 2 more variables: avg_distance_to_hospital &lt;dbl&gt;,\n#   total_vulnerable_pop &lt;dbl&gt;\n\nvulnerable_by_county %&gt;%\n  filter(n_underserved_tracts &gt; 0) %&gt;%\n  arrange(desc(total_vulnerable_pop)) %&gt;%\n  head(10)\n\n# A tibble: 4 × 6\n  COUNTY_NAM     n_vulnerable_tracts n_underserved_tracts pct_vulnerable_under…¹\n  &lt;chr&gt;                        &lt;int&gt;                &lt;int&gt;                  &lt;dbl&gt;\n1 ALLEGHENY                       18                    1                   5.56\n2 CAMBRIA                          4                    1                  25   \n3 NORTHUMBERLAND                   1                    1                 100   \n4 CAMERON                          1                    1                 100   \n# ℹ abbreviated name: ¹​pct_vulnerable_underserved\n# ℹ 2 more variables: avg_distance_to_hospital &lt;dbl&gt;,\n#   total_vulnerable_pop &lt;dbl&gt;\n\n\nQuestions to answer: - Which 5 counties have the highest percentage of underserved vulnerable tracts? Based on my threshold, the top 4 counties with highest percentage of underserved vulnerable tracts. - Which counties have the most vulnerable people living far from hospitals? Allegheny has the most vulnerable people living far from hospitals. - Are there any patterns in where underserved counties are located? Underserved counties tends to also have higher vulnerable tracts and higher average distance to hospital.\n\n\n\nStep 7: Create Summary Table\n\n# Create and format priority counties table\n\nlibrary(knitr)\nlibrary(kableExtra)\n\npriority_counties &lt;- vulnerable_by_county %&gt;%\n  arrange(desc(pct_vulnerable_underserved)) %&gt;%\n  head(10) %&gt;%\n  select(\n    COUNTY_NAM,\n    n_vulnerable_tracts,\n    n_underserved_tracts,\n    pct_vulnerable_underserved,\n    avg_distance_to_hospital,\n    total_vulnerable_pop\n  )\n\npriority_counties %&gt;%\n  kable(\n    col.names = c(\n      \"County\",\n      \"Vulnerable Tracts\",\n      \"Underserved Tracts\",\n      \"% Underserved\",\n      \"Avg Distance (miles)\",\n      \"Vulnerable Population\"\n    ),\n    caption = \"Top 10 Priority Counties for Healthcare Investment: Highest Percentage of Underserved Vulnerable Tracts\",\n    digits = c(0, 0, 0, 1, 1, 0),\n    format.args = list(big.mark = \",\")\n  ) %&gt;%\n  kable_styling(bootstrap_options = c(\"striped\", \"hover\"))\n\n\nTop 10 Priority Counties for Healthcare Investment: Highest Percentage of Underserved Vulnerable Tracts\n\n\nCounty\nVulnerable Tracts\nUnderserved Tracts\n% Underserved\nAvg Distance (miles)\nVulnerable Population\n\n\n\n\nCAMERON\n1\n1\n100.0\n61.3\n1,988\n\n\nNORTHUMBERLAND\n1\n1\n100.0\n31.2\n2,350\n\n\nCAMBRIA\n4\n1\n25.0\n11.6\n8,094\n\n\nALLEGHENY\n18\n1\n5.6\n8.0\n37,378\n\n\nPHILADELPHIA\n10\n0\n0.0\n2.9\n36,700\n\n\nWESTMORELAND\n5\n0\n0.0\n8.1\n8,145\n\n\nERIE\n3\n0\n0.0\n1.4\n5,320\n\n\nLUZERNE\n3\n0\n0.0\n5.9\n8,633\n\n\nMERCER\n3\n0\n0.0\n4.6\n6,560\n\n\nBEAVER\n2\n0\n0.0\n13.8\n3,570"
  },
  {
    "objectID": "labs/Zhang_Ye_Assignment2.html#part-2-comprehensive-visualization",
    "href": "labs/Zhang_Ye_Assignment2.html#part-2-comprehensive-visualization",
    "title": "Assignment 2: Spatial Analysis and Visualization",
    "section": "Part 2: Comprehensive Visualization",
    "text": "Part 2: Comprehensive Visualization\n\nMap 1: County-Level Choropleth\n\n# Create county-level access map\n\nlibrary(scales)\n\ncounties_with_stats &lt;- pa_counties %&gt;%\n  left_join(vulnerable_by_county, by = \"COUNTY_NAM\")\n\nggplot() +\n  geom_sf(data = counties_with_stats, \n          aes(fill = pct_vulnerable_underserved), \n          color = \"white\", \n          size = 0.5) +\n  geom_sf(data = hospitals, \n          color = \"red\", \n          size = 1.5, \n          alpha = 0.6) +\n  scale_fill_viridis_c(\n    name = \"% Vulnerable\\nTracts\\nUnderserved\",\n    labels = comma,\n    option = \"plasma\",\n    na.value = \"lightgray\"\n  ) +\n  labs(\n    title = \"Healthcare Access Challenges in Pennsylvania Counties\",\n    subtitle = \"Percentage of vulnerable tracts located far from hospitals\",\n    caption = \"Source: ACS 2022 | Red points = Hospital locations\"\n  ) +\n  theme_void() +\n  theme(\n    plot.title = element_text(face = \"bold\", size = 14),\n    plot.subtitle = element_text(size = 11),\n    legend.position = \"right\"\n  )\n\n\n\n\n\n\n\n\n\n\n\nMap 2: Detailed Vulnerability Map\n\n# Create detailed tract-level map\n\nallegheny &lt;- pa_counties %&gt;%\n  filter(COUNTY_NAM == \"ALLEGHENY\")\n\nallegheny_tracts &lt;- vulnerable_tracts %&gt;%\n  st_filter(allegheny)\n\nallegheny_hospitals &lt;- hospitals %&gt;%\n  st_filter(allegheny)\n\nggplot() +\n  geom_sf(data = allegheny, \n          fill = \"white\", \n          color = \"black\", \n          size = 1) +\n  geom_sf(data = allegheny_tracts %&gt;% filter(underserved == FALSE),\n          fill = \"blue\", \n          color = \"gray80\",\n          size = 0.3,\n          alpha = 0.6) +\n  geom_sf(data = allegheny_tracts %&gt;% filter(underserved == TRUE),\n          fill = \"red\", \n          color = \"gray80\",\n          size = 0.3,\n          alpha = 0.7) +\n  geom_sf(data = allegheny_hospitals, \n          color = \"lightgreen\", \n          size = 2, \n          shape = 17) +\n  labs(\n    title = \"Underserved Vulnerable Populations in Allegheny County\",\n    subtitle = \"Red = Underserved tracts | Blue = Vulnerable tracts | Triangles = Hospitals\"\n  ) +\n  theme_void()\n\n\n\n\n\n\n\n\n\n\n\nChart: Distribution Analysis of distances to hospitals for vulnerable populations.\n\n# Create distribution visualization\n\nggplot(vulnerable_tracts, aes(x = distance_to_nearest_hospital_miles)) +\n  geom_histogram(bins = 20, fill = \"steelblue\", alpha = 0.7, color = \"white\") +\n  geom_vline(xintercept = 15, color = \"red\", linetype = \"dashed\", size = 1) +\n  labs(\n    title = \"Distribution of Distance to Nearest Hospital\",\n    subtitle = \"For vulnerable census tracts in Pennsylvania\",\n    x = \"Distance to Nearest Hospital (miles)\",\n    y = \"Number of Vulnerable Tracts\",\n    caption = \"Red dashed line = 15-mile threshold for underserved status. \n    Most vulnerable tracts are within 15 miles of a hospital, but a concerning number remain beyond this threshold.\"\n  ) +\n  theme_minimal() +\n  scale_x_continuous(labels = comma)"
  },
  {
    "objectID": "labs/Zhang_Ye_Assignment2.html#part-3-bring-your-own-data-analysis",
    "href": "labs/Zhang_Ye_Assignment2.html#part-3-bring-your-own-data-analysis",
    "title": "Assignment 2: Spatial Analysis and Visualization",
    "section": "Part 3: Bring Your Own Data Analysis",
    "text": "Part 3: Bring Your Own Data Analysis\n\nYour Analysis\nOption B: School Safety Zones - Data: Schools, Crime Incidents, Bike Network - Question: “Are school zones safe for walking/biking, or are they crime hotspots?” - Operations: Buffer schools (500m safety zone), spatial join with crime incidents, assess bike infrastructure coverage - Policy relevance: Safe Routes to School programs, crossing guard placement\nYour Task:\n\nFind and load additional data\n\n\n# Load your additional dataset\n\n\nschools &lt;- st_read(here(\"~/Documents/GitHub/portfolio-setup-yezhanggg/labs/assignment_2_data/Challenege/Schools/Schools.shp\"))\n\nReading layer `Schools' from data source \n  `/Users/ye/Documents/GitHub/portfolio-setup-yezhanggg/labs/assignment_2_data/Challenege/Schools/Schools.shp' \n  using driver `ESRI Shapefile'\nSimple feature collection with 495 features and 14 fields\nGeometry type: POINT\nDimension:     XY\nBounding box:  xmin: -8378628 ymin: 4852555 xmax: -8345686 ymax: 4884813\nProjected CRS: WGS 84 / Pseudo-Mercator\n\ncrime &lt;- st_read(here(\"~/Documents/GitHub/portfolio-setup-yezhanggg/labs/assignment_2_data/Challenege/incidents_part1_part2/incidents_part1_part2.shp\"))\n\nReading layer `incidents_part1_part2' from data source \n  `/Users/ye/Documents/GitHub/portfolio-setup-yezhanggg/labs/assignment_2_data/Challenege/incidents_part1_part2/incidents_part1_part2.shp' \n  using driver `ESRI Shapefile'\nreplacing null geometries with empty geometries\nSimple feature collection with 169017 features and 13 fields (with 6985 geometries empty)\nGeometry type: POINT\nDimension:     XY\nBounding box:  xmin: -75.2753 ymin: 39.87465 xmax: -74.95761 ymax: 40.13762\nGeodetic CRS:  WGS 84\n\nbike &lt;- st_read(here(\"~/Documents/GitHub/portfolio-setup-yezhanggg/labs/assignment_2_data/Challenege/Bike_Network/Bike_Network.shp\"))\n\nReading layer `Bike_Network' from data source \n  `/Users/ye/Documents/GitHub/portfolio-setup-yezhanggg/labs/assignment_2_data/Challenege/Bike_Network/Bike_Network.shp' \n  using driver `ESRI Shapefile'\nSimple feature collection with 5225 features and 8 fields\nGeometry type: LINESTRING\nDimension:     XY\nBounding box:  xmin: -8378937 ymin: 4847835 xmax: -8345146 ymax: 4883978\nProjected CRS: WGS 84 / Pseudo-Mercator\n\n\n\nst_crs(schools)\n\nCoordinate Reference System:\n  User input: WGS 84 / Pseudo-Mercator \n  wkt:\nPROJCRS[\"WGS 84 / Pseudo-Mercator\",\n    BASEGEOGCRS[\"WGS 84\",\n        ENSEMBLE[\"World Geodetic System 1984 ensemble\",\n            MEMBER[\"World Geodetic System 1984 (Transit)\"],\n            MEMBER[\"World Geodetic System 1984 (G730)\"],\n            MEMBER[\"World Geodetic System 1984 (G873)\"],\n            MEMBER[\"World Geodetic System 1984 (G1150)\"],\n            MEMBER[\"World Geodetic System 1984 (G1674)\"],\n            MEMBER[\"World Geodetic System 1984 (G1762)\"],\n            MEMBER[\"World Geodetic System 1984 (G2139)\"],\n            MEMBER[\"World Geodetic System 1984 (G2296)\"],\n            ELLIPSOID[\"WGS 84\",6378137,298.257223563,\n                LENGTHUNIT[\"metre\",1]],\n            ENSEMBLEACCURACY[2.0]],\n        PRIMEM[\"Greenwich\",0,\n            ANGLEUNIT[\"degree\",0.0174532925199433]],\n        ID[\"EPSG\",4326]],\n    CONVERSION[\"Popular Visualisation Pseudo-Mercator\",\n        METHOD[\"Popular Visualisation Pseudo Mercator\",\n            ID[\"EPSG\",1024]],\n        PARAMETER[\"Latitude of natural origin\",0,\n            ANGLEUNIT[\"degree\",0.0174532925199433],\n            ID[\"EPSG\",8801]],\n        PARAMETER[\"Longitude of natural origin\",0,\n            ANGLEUNIT[\"degree\",0.0174532925199433],\n            ID[\"EPSG\",8802]],\n        PARAMETER[\"False easting\",0,\n            LENGTHUNIT[\"metre\",1],\n            ID[\"EPSG\",8806]],\n        PARAMETER[\"False northing\",0,\n            LENGTHUNIT[\"metre\",1],\n            ID[\"EPSG\",8807]]],\n    CS[Cartesian,2],\n        AXIS[\"easting (X)\",east,\n            ORDER[1],\n            LENGTHUNIT[\"metre\",1]],\n        AXIS[\"northing (Y)\",north,\n            ORDER[2],\n            LENGTHUNIT[\"metre\",1]],\n    USAGE[\n        SCOPE[\"Web mapping and visualisation.\"],\n        AREA[\"World between 85.06°S and 85.06°N.\"],\n        BBOX[-85.06,-180,85.06,180]],\n    ID[\"EPSG\",3857]]\n\nst_crs(crime)\n\nCoordinate Reference System:\n  User input: WGS 84 \n  wkt:\nGEOGCRS[\"WGS 84\",\n    DATUM[\"World Geodetic System 1984\",\n        ELLIPSOID[\"WGS 84\",6378137,298.257223563,\n            LENGTHUNIT[\"metre\",1]]],\n    PRIMEM[\"Greenwich\",0,\n        ANGLEUNIT[\"degree\",0.0174532925199433]],\n    CS[ellipsoidal,2],\n        AXIS[\"latitude\",north,\n            ORDER[1],\n            ANGLEUNIT[\"degree\",0.0174532925199433]],\n        AXIS[\"longitude\",east,\n            ORDER[2],\n            ANGLEUNIT[\"degree\",0.0174532925199433]],\n    ID[\"EPSG\",4326]]\n\nst_crs(bike)\n\nCoordinate Reference System:\n  User input: WGS 84 / Pseudo-Mercator \n  wkt:\nPROJCRS[\"WGS 84 / Pseudo-Mercator\",\n    BASEGEOGCRS[\"WGS 84\",\n        ENSEMBLE[\"World Geodetic System 1984 ensemble\",\n            MEMBER[\"World Geodetic System 1984 (Transit)\"],\n            MEMBER[\"World Geodetic System 1984 (G730)\"],\n            MEMBER[\"World Geodetic System 1984 (G873)\"],\n            MEMBER[\"World Geodetic System 1984 (G1150)\"],\n            MEMBER[\"World Geodetic System 1984 (G1674)\"],\n            MEMBER[\"World Geodetic System 1984 (G1762)\"],\n            MEMBER[\"World Geodetic System 1984 (G2139)\"],\n            MEMBER[\"World Geodetic System 1984 (G2296)\"],\n            ELLIPSOID[\"WGS 84\",6378137,298.257223563,\n                LENGTHUNIT[\"metre\",1]],\n            ENSEMBLEACCURACY[2.0]],\n        PRIMEM[\"Greenwich\",0,\n            ANGLEUNIT[\"degree\",0.0174532925199433]],\n        ID[\"EPSG\",4326]],\n    CONVERSION[\"Popular Visualisation Pseudo-Mercator\",\n        METHOD[\"Popular Visualisation Pseudo Mercator\",\n            ID[\"EPSG\",1024]],\n        PARAMETER[\"Latitude of natural origin\",0,\n            ANGLEUNIT[\"degree\",0.0174532925199433],\n            ID[\"EPSG\",8801]],\n        PARAMETER[\"Longitude of natural origin\",0,\n            ANGLEUNIT[\"degree\",0.0174532925199433],\n            ID[\"EPSG\",8802]],\n        PARAMETER[\"False easting\",0,\n            LENGTHUNIT[\"metre\",1],\n            ID[\"EPSG\",8806]],\n        PARAMETER[\"False northing\",0,\n            LENGTHUNIT[\"metre\",1],\n            ID[\"EPSG\",8807]]],\n    CS[Cartesian,2],\n        AXIS[\"easting (X)\",east,\n            ORDER[1],\n            LENGTHUNIT[\"metre\",1]],\n        AXIS[\"northing (Y)\",north,\n            ORDER[2],\n            LENGTHUNIT[\"metre\",1]],\n    USAGE[\n        SCOPE[\"Web mapping and visualisation.\"],\n        AREA[\"World between 85.06°S and 85.06°N.\"],\n        BBOX[-85.06,-180,85.06,180]],\n    ID[\"EPSG\",3857]]\n\nschools &lt;- st_transform(schools, 3365)\ncrime &lt;- st_transform(crime, 3365)\nbike &lt;- st_transform(bike, 3365)\n\nnrow(schools)\n\n[1] 495\n\nnrow(crime)\n\n[1] 169017\n\nnrow(bike)\n\n[1] 5225\n\nplot(st_geometry(schools))\n\n\n\n\n\n\n\nplot(st_geometry(crime))\n\n\n\n\n\n\n\nplot(st_geometry(bike))\n\n\n\n\n\n\n\n\nQuestions to answer: - What dataset did you choose and why? I choose schools, bike network and crime incidents to look school safety topics. - What is the data source and date? All of the data are downloaded from OpenDataPhilly. Crime incidents data is 2025 and bike network and schools don’t have their date included. - How many features does it contain? There are 15 features for schoos, 14 features for crime incidents, and 9 features for bike network. - What CRS is it in? Did you need to transform it? I transformed all of the data into EPSG:3365 for their CRS because it is an optimal option for Philadelphia. —\n\nPose a research question\n\nAre school zones safe for walking/biking, or are they crime hotspots?\n\n\nConduct spatial analysis\n\nUse at least TWO spatial operations to answer your research question.\n\n# Your spatial analysis\n\n# Operation 1: BUFFERS\n\n#find the buffers around school\nschools_buffer &lt;- schools %&gt;%\n  st_buffer(dist = 500)\n\n# Count crime incidents near each school\nschools_with_crime &lt;- schools %&gt;%\n  mutate(\n    crime_within_500m = lengths(st_intersects(schools_buffer, crime))\n  )\n\n# Summary statistics\ncat(\"Crime incidents near schools:\\n\")\n\nCrime incidents near schools:\n\ncat(\"Mean crimes per school:\", mean(schools_with_crime$crime_within_500m), \"\\n\")\n\nMean crimes per school: 60.67677 \n\ncat(\"Max crimes near one school:\", max(schools_with_crime$crime_within_500m), \"\\n\")\n\nMax crimes near one school: 490 \n\ncat(\"Schools with 10+ crimes nearby:\", \n    sum(schools_with_crime$crime_within_500m &gt;= 10), \"\\n\\n\")\n\nSchools with 10+ crimes nearby: 439 \n\ncat(\"% Schools with 10+ crimes nearby:\",\n    (sum(schools_with_crime$crime_within_500m &gt;= 10))/495, \"\\n\\n\")\n\n% Schools with 10+ crimes nearby: 0.8868687 \n\n# Operation 2: SPATIAL JOIN\n\n# Find schools within 200m of bike infrastructure\nschools_near_bikes &lt;- schools %&gt;%\n  st_filter(st_buffer(bike, 200), .predicate = st_intersects) %&gt;%\n  mutate(has_bike_access = TRUE)\n# Calculate percentage\npct_bike_access &lt;- (nrow(schools_near_bikes) / nrow(schools)) * 100\ncat(\"Schools with bike access:\", nrow(schools_near_bikes), \n    \"out of\", nrow(schools), \n    \"(\", round(pct_bike_access, 1), \"%)\")\n\nSchools with bike access: 120 out of 495 ( 24.2 %)\n\n# Create map\nggplot() +\n  geom_sf(data = bike, color = \"green\", size = 0.5, alpha = 1) +\n  geom_sf(data = crime, color = \"red\", size = 0.000000001, alpha = 0.01) +\n  geom_sf(data = schools_buffer, fill = \"yellow\", alpha = 0.8, color = NA) +\n  geom_sf(data = schools_with_crime, \n          aes(color = crime_within_500m), \n          size = 1) +\n  scale_color_viridis_c(\n    name = \"Crimes within 500m\",\n    option = \"plasma\"\n  ) +\n  labs(\n    title = \"School Safety Analysis: Crime Incidents and Bike Access\",\n    subtitle = \"Yellow zones = 500m school buffers | Green lines = Bike network\",\n    caption = \"Red points = Crime incidents\"\n  ) +\n  theme_void()\n\n\n\n\n\n\n\n\nYour interpretation:\nThe mean crime per school is around 60 and the max crime around school is 490 which is super high. There are 495 schools in total and 439 school has crimes over 10+ nearby. 88.7% of all school has crime nearby. 24.2% of the school has access to bike route. With the high percentage of crime present around school is not safe to bike around schools. There might be only a limited number of schools has the level of safety to allow students to bike around schools."
  },
  {
    "objectID": "labs/Zhang_Ye_Assignment2.html#finally---a-few-comments-about-your-incorporation-of-feedback",
    "href": "labs/Zhang_Ye_Assignment2.html#finally---a-few-comments-about-your-incorporation-of-feedback",
    "title": "Assignment 2: Spatial Analysis and Visualization",
    "section": "Finally - A few comments about your incorporation of feedback!",
    "text": "Finally - A few comments about your incorporation of feedback!\nRemove extra instruction text and only leave what is important and present the assignment like a presentation. No need to keep the instructions. Be sure to review everything."
  },
  {
    "objectID": "labs/Zhang_Ye_Assignment4.html",
    "href": "labs/Zhang_Ye_Assignment4.html",
    "title": "Lab Assignment 4: Spatial Predictive Analysis",
    "section": "",
    "text": "In this analysis, I will build a spatial predictive model for vacant and abandoned buildings in Chicago. Abandoned buildings are a critical urban issue that affects neighborhood stability, property values, public safety, and community well-being. By analyzing the spatial patterns of reported vacant and abandoned buildings, we can better understand where these issues concentrate and potentially improve city intervention strategies."
  },
  {
    "objectID": "labs/Zhang_Ye_Assignment4.html#introduction",
    "href": "labs/Zhang_Ye_Assignment4.html#introduction",
    "title": "Lab Assignment 4: Spatial Predictive Analysis",
    "section": "",
    "text": "In this analysis, I will build a spatial predictive model for vacant and abandoned buildings in Chicago. Abandoned buildings are a critical urban issue that affects neighborhood stability, property values, public safety, and community well-being. By analyzing the spatial patterns of reported vacant and abandoned buildings, we can better understand where these issues concentrate and potentially improve city intervention strategies."
  },
  {
    "objectID": "labs/Zhang_Ye_Assignment4.html#load-chicago-spatial-boundaries",
    "href": "labs/Zhang_Ye_Assignment4.html#load-chicago-spatial-boundaries",
    "title": "Lab Assignment 4: Spatial Predictive Analysis",
    "section": "Load Chicago Spatial Boundaries",
    "text": "Load Chicago Spatial Boundaries\n\n\nCode\n# Load police districts - we'll use these for cross-validation later\npoliceDistricts &lt;- \n  st_read(\"https://data.cityofchicago.org/api/geospatial/24zt-jpfn?method=export&format=GeoJSON\") %&gt;%\n  st_transform('ESRI:102271') %&gt;%\n  dplyr::select(District = dist_num)\n\n\nReading layer `OGRGeoJSON' from data source \n  `https://data.cityofchicago.org/api/geospatial/24zt-jpfn?method=export&format=GeoJSON' \n  using driver `GeoJSON'\nSimple feature collection with 25 features and 2 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: -87.94011 ymin: 41.64455 xmax: -87.52414 ymax: 42.02303\nGeodetic CRS:  WGS 84\n\n\nCode\n# Load police beats - smaller units within districts\npoliceBeats &lt;- \n  st_read(\"https://data.cityofchicago.org/api/geospatial/n9it-hstw?method=export&format=GeoJSON\") %&gt;%\n  st_transform('ESRI:102271') %&gt;%\n  dplyr::select(Beat = beat_num)\n\n\nReading layer `OGRGeoJSON' from data source \n  `https://data.cityofchicago.org/api/geospatial/n9it-hstw?method=export&format=GeoJSON' \n  using driver `GeoJSON'\nSimple feature collection with 277 features and 4 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: -87.94011 ymin: 41.64455 xmax: -87.52414 ymax: 42.02303\nGeodetic CRS:  WGS 84\n\n\nCode\n# Load Chicago boundary - the city outline\nchicagoBoundary &lt;- \n  st_read(\"https://raw.githubusercontent.com/urbanSpatial/Public-Policy-Analytics-Landing/master/DATA/Chapter5/chicagoBoundary.geojson\") %&gt;%\n  st_transform('ESRI:102271')\n\n\nReading layer `chicagoBoundary' from data source \n  `https://raw.githubusercontent.com/urbanSpatial/Public-Policy-Analytics-Landing/master/DATA/Chapter5/chicagoBoundary.geojson' \n  using driver `GeoJSON'\nSimple feature collection with 1 feature and 1 field\nGeometry type: POLYGON\nDimension:     XY\nBounding box:  xmin: -87.8367 ymin: 41.64454 xmax: -87.52414 ymax: 42.02304\nGeodetic CRS:  WGS 84\n\n\nCode\ncat(\"✓ Loaded spatial boundaries\\n\")\n\n\n✓ Loaded spatial boundaries\n\n\nCode\ncat(\"  - Police districts:\", nrow(policeDistricts), \"\\n\")\n\n\n  - Police districts: 25 \n\n\nCode\ncat(\"  - Police beats:\", nrow(policeBeats), \"\\n\")\n\n\n  - Police beats: 277"
  },
  {
    "objectID": "labs/Zhang_Ye_Assignment4.html#load-vacant-and-abandoned-buildings-data",
    "href": "labs/Zhang_Ye_Assignment4.html#load-vacant-and-abandoned-buildings-data",
    "title": "Lab Assignment 4: Spatial Predictive Analysis",
    "section": "Load Vacant and Abandoned Buildings Data",
    "text": "Load Vacant and Abandoned Buildings Data\n\n\nCode\nbuilding_raw &lt;- read_csv(here(\"data\", \"311_Service_Requests_-_Vacant_and_Abandoned_Buildings_Reported_-_Historical_20251116.csv\"))\n\n\ncat(\"Column names in dataset:\\n\")\n\n\nColumn names in dataset:\n\n\nCode\nprint(names(building_raw))\n\n\n [1] \"SERVICE REQUEST TYPE\"                                                 \n [2] \"SERVICE REQUEST NUMBER\"                                               \n [3] \"DATE SERVICE REQUEST WAS RECEIVED\"                                    \n [4] \"LOCATION OF BUILDING ON THE LOT (IF GARAGE, CHANGE TYPE CODE TO BGD).\"\n [5] \"IS THE BUILDING DANGEROUS OR HAZARDOUS?\"                              \n [6] \"IS BUILDING OPEN OR BOARDED?\"                                         \n [7] \"IF THE BUILDING IS OPEN, WHERE IS THE ENTRY POINT?\"                   \n [8] \"IS THE BUILDING CURRENTLY VACANT OR OCCUPIED?\"                        \n [9] \"IS THE BUILDING VACANT DUE TO FIRE?\"                                  \n[10] \"ANY PEOPLE USING PROPERTY? (HOMELESS, CHILDEN, GANGS)\"                \n[11] \"ADDRESS STREET NUMBER\"                                                \n[12] \"ADDRESS STREET DIRECTION\"                                             \n[13] \"ADDRESS STREET NAME\"                                                  \n[14] \"ADDRESS STREET SUFFIX\"                                                \n[15] \"ZIP CODE\"                                                             \n[16] \"X COORDINATE\"                                                         \n[17] \"Y COORDINATE\"                                                         \n[18] \"Ward\"                                                                 \n[19] \"Police District\"                                                      \n[20] \"Community Area\"                                                       \n[21] \"LATITUDE\"                                                             \n[22] \"LONGITUDE\"                                                            \n[23] \"Location\"                                                             \n\n\nCode\nbuildings &lt;- building_raw %&gt;%\n  # Remove rows without coordinates\n  filter(!is.na(`X COORDINATE`), !is.na(`Y COORDINATE`)) %&gt;%\n  # Convert to sf object - coordinates are in State Plane\n  st_as_sf(coords = c(\"X COORDINATE\", \"Y COORDINATE\"), crs = 3435) %&gt;%  \n  # Transform to the CRS used in the exercise (ESRI:102271)\n  st_transform('ESRI:102271')\n\n\ncat(\"\\n✓ Loaded abandoned buildings data\\n\")\n\n\n\n✓ Loaded abandoned buildings data\n\n\nCode\ncat(\"  - Number of building reports:\", nrow(buildings), \"\\n\")\n\n\n  - Number of building reports: 65082 \n\n\nCode\ncat(\"  - CRS:\", st_crs(buildings)$input, \"\\n\")\n\n\n  - CRS: ESRI:102271 \n\n\nLoading the 311 service requests for vacant and abandoned buildings and converting them to a spatial format that matches our other Chicago data."
  },
  {
    "objectID": "labs/Zhang_Ye_Assignment4.html#visualize-building-distribution",
    "href": "labs/Zhang_Ye_Assignment4.html#visualize-building-distribution",
    "title": "Lab Assignment 4: Spatial Predictive Analysis",
    "section": "Visualize Building Distribution",
    "text": "Visualize Building Distribution\n\n\nCode\np1 &lt;- ggplot() + \n  geom_sf(data = chicagoBoundary, fill = \"gray95\", color = \"gray60\") +\n  geom_sf(data = buildings, color = \"#d62828\", size = 0.1, alpha = 0.4) +\n  labs(\n    title = \"Vacant & Abandoned Building Locations\",\n    subtitle = paste0(\"Chicago, n = \", format(nrow(buildings), big.mark = \",\"))\n  )\n\n\nbuildings_coords_plot &lt;- as.data.frame(st_coordinates(buildings))\nnames(buildings_coords_plot) &lt;- c(\"X\", \"Y\")\n\np2 &lt;- ggplot() + \n  geom_sf(data = chicagoBoundary, fill = \"gray95\", color = \"gray60\") +\n  geom_density_2d_filled(\n    data = buildings_coords_plot,\n    aes(x = X, y = Y),\n    alpha = 0.7,\n    bins = 8\n  ) +\n  scale_fill_viridis_d(\n    option = \"plasma\",\n    direction = -1,\n    guide = \"none\"\n  ) +\n  labs(\n    title = \"Density Surface\",\n    subtitle = \"Kernel density estimation\"\n  )\n\n\np1 + p2 + \n  plot_annotation(\n    title = \"Spatial Distribution of Vacant & Abandoned Buildings in Chicago\",\n    tag_levels = 'A'\n  )\n\n\n\n\n\n\n\n\n\nVacant and abandoned buildings are not evenly distributed across Chicago. The density surface reveals distinct concentration patterns with hot spots appearing in specific neighborhoods. There are clear clusters in the central and also south part of the city. High cluster in the south part of the city."
  },
  {
    "objectID": "labs/Zhang_Ye_Assignment4.html#create-the-grid",
    "href": "labs/Zhang_Ye_Assignment4.html#create-the-grid",
    "title": "Lab Assignment 4: Spatial Predictive Analysis",
    "section": "Create the Grid",
    "text": "Create the Grid\n\n\nCode\nfishnet &lt;- st_make_grid(\n  chicagoBoundary,\n  cellsize = 500,  \n  square = TRUE\n) %&gt;%\n  st_sf() %&gt;%\n  mutate(uniqueID = row_number())\n\n\nfishnet &lt;- fishnet[chicagoBoundary, ]\n\ncat(\"✓ Created fishnet grid\\n\")\n\n\n✓ Created fishnet grid\n\n\nCode\ncat(\"  - Number of cells:\", nrow(fishnet), \"\\n\")\n\n\n  - Number of cells: 2458 \n\n\nCode\ncat(\"  - Cell size: 500 x 500 meters\\n\")\n\n\n  - Cell size: 500 x 500 meters\n\n\nCreating a grid of 500m x 500m squares that covers Chicago. This converts our point data into a regular grid that’s easier to analyze and model."
  },
  {
    "objectID": "labs/Zhang_Ye_Assignment4.html#aggregate-buildings-to-grid-cells",
    "href": "labs/Zhang_Ye_Assignment4.html#aggregate-buildings-to-grid-cells",
    "title": "Lab Assignment 4: Spatial Predictive Analysis",
    "section": "Aggregate Buildings to Grid Cells",
    "text": "Aggregate Buildings to Grid Cells\n\n\nCode\nbuildings_fishnet &lt;- st_join(buildings, fishnet, join = st_within) %&gt;%\n  st_drop_geometry() %&gt;%\n  group_by(uniqueID) %&gt;%\n  summarize(countBuildings = n())\n\n\nfishnet &lt;- fishnet %&gt;%\n  left_join(buildings_fishnet, by = \"uniqueID\") %&gt;%\n  mutate(countBuildings = replace_na(countBuildings, 0))\n\n\ncat(\"\\nBuilding count distribution:\\n\")\n\n\n\nBuilding count distribution:\n\n\nCode\nsummary(fishnet$countBuildings)\n\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n   0.00    1.00    7.00   26.47   28.00  378.00 \n\n\nCode\ncat(\"\\nCells with zero buildings:\", \n    sum(fishnet$countBuildings == 0), \n    \"/\", nrow(fishnet),\n    \"(\", round(100 * sum(fishnet$countBuildings == 0) / nrow(fishnet), 1), \"%)\\n\")\n\n\n\nCells with zero buildings: 542 / 2458 ( 22.1 %)\n\n\nCounting how many abandoned building reports occurred in each grid cell."
  },
  {
    "objectID": "labs/Zhang_Ye_Assignment4.html#visualize-the-grid",
    "href": "labs/Zhang_Ye_Assignment4.html#visualize-the-grid",
    "title": "Lab Assignment 4: Spatial Predictive Analysis",
    "section": "Visualize the Grid",
    "text": "Visualize the Grid\n\n\nCode\n# Map the counts in each grid cell\nggplot() +\n  geom_sf(data = fishnet, aes(fill = countBuildings), color = NA) +\n  scale_fill_viridis_c(\n    name = \"Abandoned\\nBuildings\",\n    option = \"plasma\",\n    trans = \"sqrt\"  \n  ) +\n  labs(\n    title = \"Vacant & Abandoned Buildings by 500m Grid Cell\",\n    subtitle = \"Chicago\"\n  ) +\n  theme_buildings()\n\n\n\n\n\n\n\n\n\nThe fishnet grid reveals clear spatial concentration of abandoned building reports. The highest counts cluster in south part of the city. Many cells have zero reports, while some hot spot cells have significantly elevated counts, indicating neighborhoods with persistent vacancy and abandonment issues."
  },
  {
    "objectID": "labs/Zhang_Ye_Assignment4.html#calculate-k-nearest-neighbors",
    "href": "labs/Zhang_Ye_Assignment4.html#calculate-k-nearest-neighbors",
    "title": "Lab Assignment 4: Spatial Predictive Analysis",
    "section": "Calculate k-Nearest Neighbors",
    "text": "Calculate k-Nearest Neighbors\n\n\nCode\nbuildings_coords &lt;- st_coordinates(buildings)\n\n\nfishnet_coords &lt;- st_coordinates(st_centroid(fishnet))\n\n\nnn_distances &lt;- get.knnx(\n  data = buildings_coords,  \n  query = fishnet_coords,   \n  k = 5                    \n)\n\n\nfishnet &lt;- fishnet %&gt;%\n  mutate(\n    buildings.nn = rowMeans(nn_distances$nn.dist)\n  )\n\ncat(\"✓ Calculated k-nearest neighbors\\n\")\n\n\n✓ Calculated k-nearest neighbors\n\n\nCode\ncat(\"  - k = 5 nearest buildings\\n\")\n\n\n  - k = 5 nearest buildings\n\n\nCode\ncat(\"  - Mean distance:\", round(mean(fishnet$buildings.nn), 1), \"meters\\n\")\n\n\n  - Mean distance: 291.5 meters\n\n\nFor each grid cell, we find the average distance to the 5 nearest abandoned building reports. This measures how close each cell is to existing abandoned buildings."
  },
  {
    "objectID": "labs/Zhang_Ye_Assignment4.html#visualize-k-nn-feature",
    "href": "labs/Zhang_Ye_Assignment4.html#visualize-k-nn-feature",
    "title": "Lab Assignment 4: Spatial Predictive Analysis",
    "section": "Visualize k-NN Feature",
    "text": "Visualize k-NN Feature\n\n\nCode\nggplot() +\n  geom_sf(data = fishnet, aes(fill = buildings.nn), color = NA) +\n  scale_fill_viridis_c(\n    name = \"Mean Distance\\nto 5 Nearest\\nBuildings (m)\",\n    option = \"viridis\",\n    direction = -1\n  ) +\n  labs(\n    title = \"Average Distance to Nearest Abandoned Buildings\",\n    subtitle = \"Lower values = closer to existing abandoned buildings\"\n  ) +\n  theme_buildings()\n\n\n\n\n\n\n\n\n\nThe k-NN map shows the consistent yellow coloring across most of the city suggests that abandoned buildings are a pervasive issue affecting nearly all neighborhoods rather than being isolated to just a few problem areas."
  },
  {
    "objectID": "labs/Zhang_Ye_Assignment4.html#calculate-local-morans-i",
    "href": "labs/Zhang_Ye_Assignment4.html#calculate-local-morans-i",
    "title": "Lab Assignment 4: Spatial Predictive Analysis",
    "section": "Calculate Local Moran’s I",
    "text": "Calculate Local Moran’s I\n\n\nCode\ncoords &lt;- st_centroid(fishnet) %&gt;% st_coordinates()\nweight_matrix &lt;- knn2nb(knearneigh(coords, k = 4))\nweights &lt;- nb2listw(weight_matrix, style = \"W\", zero.policy = TRUE)\n\n\nfishnet &lt;- fishnet %&gt;%\n  mutate(\n    # Calculate local Moran's I statistic\n    localMorans = localmoran(countBuildings, weights, zero.policy = TRUE)[,1],\n    # Get p-values\n    localMorans_pval = localmoran(countBuildings, weights, zero.policy = TRUE)[,5]\n  )\n\nfishnet &lt;- fishnet %&gt;%\n  mutate(\n    cluster = case_when(\n      localMorans_pval &gt; 0.05 ~ \"Not Significant\",\n      localMorans &gt; 0 & countBuildings &gt; median(countBuildings) ~ \"High-High (Hot Spot)\",\n      localMorans &gt; 0 & countBuildings &lt;= median(countBuildings) ~ \"Low-Low (Cold Spot)\",\n      localMorans &lt; 0 & countBuildings &gt; median(countBuildings) ~ \"High-Low (Outlier)\",\n      localMorans &lt; 0 & countBuildings &lt;= median(countBuildings) ~ \"Low-High (Outlier)\",\n      TRUE ~ \"Not Significant\"\n    )\n  )\n\n# Summary\ncat(\"Cluster distribution:\\n\")\n\n\nCluster distribution:\n\n\nCode\ntable(fishnet$cluster)\n\n\n\nHigh-High (Hot Spot)   High-Low (Outlier)   Low-High (Outlier) \n                 270                    6                    2 \n     Not Significant \n                2180 \n\n\nLocal Moran’s I identifies clusters of similar values. Hot spots are high-abandonment areas surrounded by high-abandonment neighbors. Cold spots are low-abandonment areas surrounded by low-abandonment neighbors."
  },
  {
    "objectID": "labs/Zhang_Ye_Assignment4.html#visualize-hot-spots",
    "href": "labs/Zhang_Ye_Assignment4.html#visualize-hot-spots",
    "title": "Lab Assignment 4: Spatial Predictive Analysis",
    "section": "Visualize Hot Spots",
    "text": "Visualize Hot Spots\n\n\nCode\n# Map 1: Cluster types\np1 &lt;- ggplot() +\n  geom_sf(data = fishnet, aes(fill = cluster), color = NA) +\n  scale_fill_manual(\n    name = \"Cluster Type\",\n    values = c(\n      \"High-High (Hot Spot)\" = \"#d7191c\",\n      \"Low-Low (Cold Spot)\" = \"#2c7bb6\",\n      \"High-Low (Outlier)\" = \"#fdae61\",\n      \"Low-High (Outlier)\" = \"#abd9e9\",\n      \"Not Significant\" = \"gray90\"\n    )\n  ) +\n  labs(\n    title = \"Local Moran's I Clusters\",\n    subtitle = \"Spatial autocorrelation of vacant & abandoned buildings\"\n  ) +\n  theme_buildings()\n\n# Map 2: Local Moran's I values\np2 &lt;- ggplot() +\n  geom_sf(data = fishnet, aes(fill = localMorans), color = NA) +\n  scale_fill_gradient2(\n    name = \"Local\\nMoran's I\",\n    low = \"#2c7bb6\",\n    mid = \"white\",\n    high = \"#d7191c\",\n    midpoint = 0\n  ) +\n  labs(\n    title = \"Local Moran's I Values\",\n    subtitle = \"Positive = similar neighbors, Negative = dissimilar neighbors\"\n  ) +\n  theme_buildings()\n\np1 + p2\n\n\n\n\n\n\n\n\n\nThe hot spot analysis identifies concentrated high-abandonment clusters in south Chicago neighborhoods and some in the north central. Cold spots appear to scatter around and there are very few of them. The spatial clustering is statistically significant, confirming that building abandonment is not randomly distributed but concentrates in neighborhoods that might experiencing disinvestment."
  },
  {
    "objectID": "labs/Zhang_Ye_Assignment4.html#calculate-distance-to-hot-spots",
    "href": "labs/Zhang_Ye_Assignment4.html#calculate-distance-to-hot-spots",
    "title": "Lab Assignment 4: Spatial Predictive Analysis",
    "section": "Calculate Distance to Hot Spots",
    "text": "Calculate Distance to Hot Spots\n\n\nCode\nhotspots &lt;- fishnet %&gt;%\n  filter(cluster == \"High-High (Hot Spot)\")\n\ncat(\"Number of hot spots identified:\", nrow(hotspots), \"\\n\")\n\n\nNumber of hot spots identified: 270 \n\n\nCode\nif(nrow(hotspots) &gt; 0) {\n  # Get coordinates\n  hotspot_coords &lt;- st_coordinates(st_centroid(hotspots))\n  fishnet_coords &lt;- st_coordinates(st_centroid(fishnet))\n  \n\n  nn_to_hotspot &lt;- get.knnx(\n    data = hotspot_coords,\n    query = fishnet_coords,\n    k = 1\n  )\n  \n\n  fishnet &lt;- fishnet %&gt;%\n    mutate(\n      dist_to_hotspot = nn_to_hotspot$nn.dist[,1]\n    )\n  \n  cat(\"✓ Calculated distance to nearest hot spot\\n\")\n  cat(\"  - Number of hot spots:\", nrow(hotspots), \"\\n\")\n  cat(\"  - Mean distance to hot spot:\", round(mean(fishnet$dist_to_hotspot), 1), \"meters\\n\")\n} else {\n  # If no hot spots, create a placeholder distance variable\n  fishnet &lt;- fishnet %&gt;%\n    mutate(dist_to_hotspot = 0)\n  cat(\"⚠ No hot spots identified - using placeholder distance variable\\n\")\n}\n\n\n✓ Calculated distance to nearest hot spot\n  - Number of hot spots: 270 \n  - Mean distance to hot spot: 3873.7 meters\n\n\nMeasuring how far each grid cell is from the nearest identified hot spot of building abandonment."
  },
  {
    "objectID": "labs/Zhang_Ye_Assignment4.html#visualize-distance-to-hot-spots",
    "href": "labs/Zhang_Ye_Assignment4.html#visualize-distance-to-hot-spots",
    "title": "Lab Assignment 4: Spatial Predictive Analysis",
    "section": "Visualize Distance to Hot Spots",
    "text": "Visualize Distance to Hot Spots\n\n\nCode\nif(nrow(hotspots) &gt; 0) {\n  ggplot() +\n    geom_sf(data = fishnet, aes(fill = dist_to_hotspot), color = NA) +\n    scale_fill_viridis_c(\n      name = \"Distance to\\nNearest Hot Spot\\n(meters)\",\n      option = \"magma\",\n      direction = -1\n    ) +\n    labs(\n      title = \"Distance to Nearest Abandonment Hot Spot\",\n      subtitle = \"Lower values = closer to hot spots\"\n    ) +\n    theme_buildings()\n} else {\n  cat(\"No hot spots to visualize\\n\")\n}\n\n\n\n\n\n\n\n\n\nThe map shows clear concentric rings radiating outward from a central hot spot in Chicago, where the lightest (near-zero distance) areas indicate the core abandonment cluster. The gradient pattern reveals that most of the northern of the city is relatively close to this hot spot, while the far north and some southern areas are more isolated from the main abandonment concentration, suggesting these neighborhoods may be less affected by the spillover effects of concentrated building vacancy."
  },
  {
    "objectID": "labs/Zhang_Ye_Assignment4.html#prepare-modeling-data",
    "href": "labs/Zhang_Ye_Assignment4.html#prepare-modeling-data",
    "title": "Lab Assignment 4: Spatial Predictive Analysis",
    "section": "Prepare Modeling Data",
    "text": "Prepare Modeling Data\n\n\nCode\nfishnet_model &lt;- fishnet %&gt;%\n  st_join(policeDistricts, join = st_intersects, left = TRUE) %&gt;%\n  st_drop_geometry() %&gt;%\n  filter(!is.na(District))  # Remove cells outside districts\n\nfishnet_model &lt;- fishnet_model %&gt;%\n  mutate(\n    buildings.nn_scaled = scale(buildings.nn)[,1],\n    dist_to_hotspot_scaled = scale(dist_to_hotspot)[,1]\n  )\n\ncat(\"✓ Prepared modeling dataset\\n\")\n\n\n✓ Prepared modeling dataset\n\n\nCode\ncat(\"  - Number of cells:\", nrow(fishnet_model), \"\\n\")\n\n\n  - Number of cells: 2891 \n\n\nCode\ncat(\"  - Number of districts:\", n_distinct(fishnet_model$District), \"\\n\")\n\n\n  - Number of districts: 23 \n\n\nPreparing our data for modeling by joining it with police districts (for cross-validation) and creating scaled versions of our predictor variables."
  },
  {
    "objectID": "labs/Zhang_Ye_Assignment4.html#fit-poisson-regression",
    "href": "labs/Zhang_Ye_Assignment4.html#fit-poisson-regression",
    "title": "Lab Assignment 4: Spatial Predictive Analysis",
    "section": "Fit Poisson Regression",
    "text": "Fit Poisson Regression\n\n\nCode\nmodel_poisson &lt;- glm(\n  countBuildings ~ buildings.nn_scaled + dist_to_hotspot_scaled,\n  data = fishnet_model,\n  family = poisson()\n)\n\n\nsummary(model_poisson)\n\n\n\nCall:\nglm(formula = countBuildings ~ buildings.nn_scaled + dist_to_hotspot_scaled, \n    family = poisson(), data = fishnet_model)\n\nCoefficients:\n                       Estimate Std. Error z value            Pr(&gt;|z|)    \n(Intercept)             1.06854    0.01458    73.3 &lt;0.0000000000000002 ***\nbuildings.nn_scaled    -3.36737    0.02359  -142.7 &lt;0.0000000000000002 ***\ndist_to_hotspot_scaled -0.92218    0.00901  -102.4 &lt;0.0000000000000002 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for poisson family taken to be 1)\n\n    Null deviance: 151530  on 2890  degrees of freedom\nResidual deviance:  23630  on 2888  degrees of freedom\nAIC: 33755\n\nNumber of Fisher Scoring iterations: 5\n\n\nCode\ncat(\"\\nPoisson Model AIC:\", AIC(model_poisson), \"\\n\")\n\n\n\nPoisson Model AIC: 33755.26 \n\n\nFitting a Poisson regression model that predicts building counts using distance to nearest buildings and distance to hot spots."
  },
  {
    "objectID": "labs/Zhang_Ye_Assignment4.html#fit-negative-binomial-regression",
    "href": "labs/Zhang_Ye_Assignment4.html#fit-negative-binomial-regression",
    "title": "Lab Assignment 4: Spatial Predictive Analysis",
    "section": "Fit Negative Binomial Regression",
    "text": "Fit Negative Binomial Regression\n\n\nCode\ncat(\"Data diagnostics:\\n\")\n\n\nData diagnostics:\n\n\nCode\ncat(\"Building count range:\", min(fishnet_model$countBuildings), \"to\", max(fishnet_model$countBuildings), \"\\n\")\n\n\nBuilding count range: 0 to 378 \n\n\nCode\ncat(\"Mean:\", round(mean(fishnet_model$countBuildings), 2), \"\\n\")\n\n\nMean: 26.82 \n\n\nCode\ncat(\"Variance:\", round(var(fishnet_model$countBuildings), 2), \"\\n\")\n\n\nVariance: 2084.37 \n\n\nCode\ncat(\"Variance/Mean ratio:\", round(var(fishnet_model$countBuildings)/mean(fishnet_model$countBuildings), 2), \"\\n\\n\")\n\n\nVariance/Mean ratio: 77.72 \n\n\nCode\nmodel_nb &lt;- tryCatch({\n  cat(\"Trying NB model with scaled predictors...\\n\")\n  glm.nb(\n    countBuildings ~ buildings.nn_scaled + dist_to_hotspot_scaled,\n    data = fishnet_model,\n    control = glm.control(maxit = 100)\n  )\n}, error = function(e1) {\n  cat(\"Direct fit failed. Trying with Poisson starting values...\\n\")\n  \n  model_poisson_scaled &lt;- glm(\n    countBuildings ~ buildings.nn_scaled + dist_to_hotspot_scaled,\n    data = fishnet_model,\n    family = poisson()\n  )\n  \n  tryCatch({\n    glm.nb(\n      countBuildings ~ buildings.nn_scaled + dist_to_hotspot_scaled,\n      data = fishnet_model,\n      start = coef(model_poisson_scaled),\n      init.theta = 1,\n      control = glm.control(maxit = 200, epsilon = 1e-4)\n    )\n  }, error = function(e2) {\n    cat(\"Still failing. Trying with subset of data to estimate theta...\\n\")\n    \n    fishnet_subset &lt;- fishnet_model %&gt;%\n      filter(countBuildings &lt; quantile(countBuildings, 0.95))\n    \n    model_subset &lt;- glm.nb(\n      countBuildings ~ buildings.nn_scaled + dist_to_hotspot_scaled,\n      data = fishnet_subset\n    )\n    \n    glm.nb(\n      countBuildings ~ buildings.nn_scaled + dist_to_hotspot_scaled,\n      data = fishnet_model,\n      start = coef(model_subset),\n      init.theta = model_subset$theta,\n      control = glm.control(maxit = 200, epsilon = 1e-4)\n    )\n  })\n})\n\n\nTrying NB model with scaled predictors...\n\n\nCode\nsummary(model_nb)\n\n\n\nCall:\nglm.nb(formula = countBuildings ~ buildings.nn_scaled + dist_to_hotspot_scaled, \n    data = fishnet_model, control = glm.control(maxit = 100), \n    init.theta = 3.469696575, link = log)\n\nCoefficients:\n                       Estimate Std. Error z value            Pr(&gt;|z|)    \n(Intercept)             1.08864    0.02663   40.88 &lt;0.0000000000000002 ***\nbuildings.nn_scaled    -3.79635    0.05148  -73.75 &lt;0.0000000000000002 ***\ndist_to_hotspot_scaled -0.48137    0.01711  -28.13 &lt;0.0000000000000002 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for Negative Binomial(3.4697) family taken to be 1)\n\n    Null deviance: 21692.6  on 2890  degrees of freedom\nResidual deviance:  2658.7  on 2888  degrees of freedom\nAIC: 16847\n\nNumber of Fisher Scoring iterations: 1\n\n              Theta:  3.470 \n          Std. Err.:  0.124 \n\n 2 x log-likelihood:  -16839.458 \n\n\nCode\n# Compare AIC (lower is better)\ncat(\"\\nModel Comparison:\\n\")\n\n\n\nModel Comparison:\n\n\nCode\ncat(\"Poisson AIC:\", round(AIC(model_poisson), 1), \"\\n\")\n\n\nPoisson AIC: 33755.3 \n\n\nCode\ncat(\"Negative Binomial AIC:\", round(AIC(model_nb), 1), \"\\n\")\n\n\nNegative Binomial AIC: 16847.5 \n\n\nCode\ncat(\"\\n✓ Model fitted successfully\\n\")\n\n\n\n✓ Model fitted successfully\n\n\nThe lower AIC tells us which model fits better. Negative Binomial has a better fit through having a lower AIC. This large difference indicates that the data exhibits severe overdispersion - meaning the variance in abandoned building counts is much greater than the mean. The Poisson model’s assumption that mean equals variance is strongly violated, making the Negative Binomial model the clearly superior choice for modeling this data. This overdispersion is typical in urban abandonment data where most areas have few or no abandoned buildings, but some hot spot areas have very high concentrations."
  },
  {
    "objectID": "labs/Zhang_Ye_Assignment4.html#implement-leave-one-group-out-cv",
    "href": "labs/Zhang_Ye_Assignment4.html#implement-leave-one-group-out-cv",
    "title": "Lab Assignment 4: Spatial Predictive Analysis",
    "section": "Implement Leave-One-Group-Out CV",
    "text": "Implement Leave-One-Group-Out CV\n\n\nCode\ndistricts &lt;- unique(fishnet_model$District)\n\n\ncv_results &lt;- tibble()\n\ncat(\"Running Leave-One-Group-Out Cross-Validation...\\n\")\n\n\nRunning Leave-One-Group-Out Cross-Validation...\n\n\nCode\ncat(\"(\", length(districts), \"folds - one per district)\\n\\n\")\n\n\n( 23 folds - one per district)\n\n\nCode\nfor(i in 1:length(districts)) {\n  test_district &lt;- districts[i]\n  \n\n  train_data &lt;- fishnet_model %&gt;% filter(District != test_district)\n  test_data &lt;- fishnet_model %&gt;% filter(District == test_district)\n  \n  \n  model_cv &lt;- tryCatch({\n    glm.nb(\n      countBuildings ~ buildings.nn_scaled + dist_to_hotspot_scaled,\n      data = train_data,\n      control = glm.control(maxit = 100)\n    )\n  }, error = function(e) {\n\n    model_poisson_cv &lt;- glm(\n      countBuildings ~ buildings.nn_scaled + dist_to_hotspot_scaled,\n      data = train_data,\n      family = poisson()\n    )\n    \n    tryCatch({\n      glm.nb(\n        countBuildings ~ buildings.nn_scaled + dist_to_hotspot_scaled,\n        data = train_data,\n        start = coef(model_poisson_cv),\n        init.theta = 1,\n        control = glm.control(maxit = 100)\n      )\n    }, error = function(e2) {\n  \n      train_subset &lt;- train_data %&gt;%\n        filter(countBuildings &lt; quantile(countBuildings, 0.95))\n      \n      model_subset &lt;- glm.nb(\n        countBuildings ~ buildings.nn_scaled + dist_to_hotspot_scaled,\n        data = train_subset\n      )\n      \n      glm.nb(\n        countBuildings ~ buildings.nn_scaled + dist_to_hotspot_scaled,\n        data = train_data,\n        start = coef(model_subset),\n        init.theta = model_subset$theta,\n        control = glm.control(maxit = 100)\n      )\n    })\n  })\n  \n\n  test_data &lt;- test_data %&gt;%\n    mutate(\n      prediction = predict(model_cv, test_data, type = \"response\")\n    )\n  \n\n  mae &lt;- mean(abs(test_data$countBuildings - test_data$prediction))\n  rmse &lt;- sqrt(mean((test_data$countBuildings - test_data$prediction)^2))\n  \n\n  cv_results &lt;- bind_rows(\n    cv_results,\n    tibble(\n      fold = i,\n      test_district = test_district,\n      n_test = nrow(test_data),\n      mae = mae,\n      rmse = rmse\n    )\n  )\n  \n  cat(\"  Fold\", i, \"/\", length(districts), \"- District\", test_district, \n      \"- MAE:\", round(mae, 2), \"\\n\")\n}\n\n\n  Fold 1 / 23 - District 5 - MAE: 17.77 \n  Fold 2 / 23 - District 4 - MAE: 9.2 \n  Fold 3 / 23 - District 22 - MAE: 12.62 \n  Fold 4 / 23 - District 31 - MAE: 1.08 \n  Fold 5 / 23 - District 6 - MAE: 22.23 \n  Fold 6 / 23 - District 8 - MAE: 10.16 \n  Fold 7 / 23 - District 7 - MAE: 70.75 \n  Fold 8 / 23 - District 3 - MAE: 22.32 \n  Fold 9 / 23 - District 2 - MAE: 9.29 \n  Fold 10 / 23 - District 9 - MAE: 15 \n  Fold 11 / 23 - District 10 - MAE: 13.82 \n  Fold 12 / 23 - District 1 - MAE: 1.76 \n  Fold 13 / 23 - District 12 - MAE: 9.04 \n  Fold 14 / 23 - District 15 - MAE: 21.81 \n  Fold 15 / 23 - District 11 - MAE: 32.54 \n  Fold 16 / 23 - District 18 - MAE: 1.42 \n  Fold 17 / 23 - District 25 - MAE: 14.23 \n  Fold 18 / 23 - District 14 - MAE: 12.21 \n  Fold 19 / 23 - District 19 - MAE: 3.51 \n  Fold 20 / 23 - District 16 - MAE: 2.12 \n  Fold 21 / 23 - District 17 - MAE: 2.93 \n  Fold 22 / 23 - District 20 - MAE: 1.98 \n  Fold 23 / 23 - District 24 - MAE: 2.6 \n\n\nCode\ncat(\"\\n✓ Cross-Validation Complete\\n\")\n\n\n\n✓ Cross-Validation Complete\n\n\nCode\ncat(\"Mean MAE:\", round(mean(cv_results$mae), 2), \"\\n\")\n\n\nMean MAE: 13.49 \n\n\nCode\ncat(\"Mean RMSE:\", round(mean(cv_results$rmse), 2), \"\\n\")\n\n\nMean RMSE: 21.48 \n\n\nTesting our model by holding out one police district at a time, training on the rest, and seeing how well we predict the held-out district."
  },
  {
    "objectID": "labs/Zhang_Ye_Assignment4.html#cross-validation-results",
    "href": "labs/Zhang_Ye_Assignment4.html#cross-validation-results",
    "title": "Lab Assignment 4: Spatial Predictive Analysis",
    "section": "Cross-Validation Results",
    "text": "Cross-Validation Results\n\n\nCode\ncv_results %&gt;%\n  arrange(desc(mae)) %&gt;%\n  kable(\n    digits = 2,\n    caption = \"LOGO CV Results by District\",\n    col.names = c(\"Fold\", \"District\", \"N Test Cells\", \"MAE\", \"RMSE\")\n  ) %&gt;%\n  kable_styling(bootstrap_options = c(\"striped\", \"hover\"))\n\n\n\nLOGO CV Results by District\n\n\nFold\nDistrict\nN Test Cells\nMAE\nRMSE\n\n\n\n\n7\n7\n86\n70.75\n96.35\n\n\n15\n11\n79\n32.54\n48.31\n\n\n8\n3\n86\n22.32\n34.01\n\n\n5\n6\n109\n22.23\n30.62\n\n\n14\n15\n60\n21.81\n34.21\n\n\n1\n5\n170\n17.77\n33.61\n\n\n10\n9\n167\n15.00\n27.44\n\n\n17\n25\n135\n14.23\n20.45\n\n\n11\n10\n111\n13.82\n23.45\n\n\n3\n22\n187\n12.62\n22.26\n\n\n18\n14\n84\n12.21\n19.07\n\n\n6\n8\n281\n10.16\n18.74\n\n\n9\n2\n99\n9.29\n15.71\n\n\n2\n4\n325\n9.20\n22.08\n\n\n13\n12\n127\n9.04\n15.44\n\n\n19\n19\n116\n3.51\n7.12\n\n\n21\n17\n126\n2.93\n4.94\n\n\n23\n24\n76\n2.60\n3.67\n\n\n20\n16\n224\n2.12\n3.70\n\n\n22\n20\n61\n1.98\n3.28\n\n\n12\n1\n71\n1.76\n4.14\n\n\n16\n18\n74\n1.42\n3.00\n\n\n4\n31\n37\n1.08\n2.55\n\n\n\n\n\nSpatial cross-validation (LOGO CV) is more appropriate than random CV because of spatial autocorrelation - nearby locations tend to have similar characteristics. District 7 had the highest MAE and RMSE, indicating it has unique abandonment patterns not well-captured by the model’s spatial features or differs significantly from other districts in the training data."
  },
  {
    "objectID": "labs/Zhang_Ye_Assignment4.html#generate-final-predictions",
    "href": "labs/Zhang_Ye_Assignment4.html#generate-final-predictions",
    "title": "Lab Assignment 4: Spatial Predictive Analysis",
    "section": "Generate Final Predictions",
    "text": "Generate Final Predictions\n\n\nCode\nfishnet &lt;- fishnet %&gt;%\n  mutate(\n    buildings.nn_scaled = (buildings.nn - mean(fishnet_model$buildings.nn, na.rm = TRUE)) / \n                         sd(fishnet_model$buildings.nn, na.rm = TRUE),\n    dist_to_hotspot_scaled = (dist_to_hotspot - mean(fishnet_model$dist_to_hotspot, na.rm = TRUE)) / \n                              sd(fishnet_model$dist_to_hotspot, na.rm = TRUE)\n  )\n\nfinal_model &lt;- tryCatch({\n  cat(\"Fitting final NB model...\\n\")\n  glm.nb(\n    countBuildings ~ buildings.nn_scaled + dist_to_hotspot_scaled,\n    data = fishnet_model,\n    control = glm.control(maxit = 100)\n  )\n}, error = function(e) {\n  cat(\"Using Poisson starting values for final model...\\n\")\n  \n  model_poisson_final &lt;- glm(\n    countBuildings ~ buildings.nn_scaled + dist_to_hotspot_scaled,\n    data = fishnet_model,\n    family = poisson()\n  )\n  \n  tryCatch({\n    glm.nb(\n      countBuildings ~ buildings.nn_scaled + dist_to_hotspot_scaled,\n      data = fishnet_model,\n      start = coef(model_poisson_final),\n      init.theta = 1,\n      control = glm.control(maxit = 200)\n    )\n  }, error = function(e2) {\n    # Last resort\n    fishnet_subset &lt;- fishnet_model %&gt;%\n      filter(countBuildings &lt; quantile(countBuildings, 0.95))\n    \n    model_subset &lt;- glm.nb(\n      countBuildings ~ buildings.nn_scaled + dist_to_hotspot_scaled,\n      data = fishnet_subset\n    )\n    \n    glm.nb(\n      countBuildings ~ buildings.nn_scaled + dist_to_hotspot_scaled,\n      data = fishnet_model,\n      start = coef(model_subset),\n      init.theta = model_subset$theta,\n      control = glm.control(maxit = 200)\n    )\n  })\n})\n\n\nFitting final NB model...\n\n\nCode\npred_data &lt;- fishnet %&gt;%\n  st_drop_geometry() %&gt;%\n  dplyr::select(uniqueID, buildings.nn_scaled, dist_to_hotspot_scaled)\n\nfishnet &lt;- fishnet %&gt;%\n  mutate(\n    prediction_nb = predict(final_model, pred_data, type = \"response\")\n  )\n\ncat(\"✓ Final predictions generated\\n\")\n\n\n✓ Final predictions generated"
  },
  {
    "objectID": "labs/Zhang_Ye_Assignment4.html#compare-to-kde-baseline",
    "href": "labs/Zhang_Ye_Assignment4.html#compare-to-kde-baseline",
    "title": "Lab Assignment 4: Spatial Predictive Analysis",
    "section": "Compare to KDE Baseline",
    "text": "Compare to KDE Baseline\n\n\nCode\nbuildings_ppp &lt;- as.ppp(st_coordinates(buildings), W = as.owin(chicagoBoundary))\n\n\nkde_buildings &lt;- density.ppp(buildings_ppp, sigma = 1000)\n\n\nkde_raster &lt;- rast(kde_buildings)\nfishnet &lt;- fishnet %&gt;%\n  mutate(\n    kde_value = terra::extract(kde_raster, st_centroid(fishnet))[,2]\n  )\n\n\nkde_sum &lt;- sum(fishnet$kde_value, na.rm = TRUE)\ncount_sum &lt;- sum(fishnet$countBuildings, na.rm = TRUE)\nfishnet &lt;- fishnet %&gt;%\n  mutate(\n    prediction_kde = (kde_value / kde_sum) * count_sum\n  )"
  },
  {
    "objectID": "labs/Zhang_Ye_Assignment4.html#visualize-predictions",
    "href": "labs/Zhang_Ye_Assignment4.html#visualize-predictions",
    "title": "Lab Assignment 4: Spatial Predictive Analysis",
    "section": "Visualize Predictions",
    "text": "Visualize Predictions\n\n\nCode\n# Create three maps\np1 &lt;- ggplot() +\n  geom_sf(data = fishnet, aes(fill = countBuildings), color = NA) +\n  scale_fill_viridis_c(name = \"Count\", option = \"plasma\", limits = c(0, 50)) +\n  labs(title = \"Actual Building Reports\") +\n  theme_buildings()\n\np2 &lt;- ggplot() +\n  geom_sf(data = fishnet, aes(fill = prediction_nb), color = NA) +\n  scale_fill_viridis_c(name = \"Predicted\", option = \"plasma\", limits = c(0, 50)) +\n  labs(title = \"Model Predictions (Neg. Binomial)\") +\n  theme_buildings()\n\np3 &lt;- ggplot() +\n  geom_sf(data = fishnet, aes(fill = prediction_kde), color = NA) +\n  scale_fill_viridis_c(name = \"Predicted\", option = \"plasma\", limits = c(0, 50)) +\n  labs(title = \"KDE Baseline Predictions\") +\n  theme_buildings()\n\np1 + p2 + p3 +\n  plot_annotation(\n    title = \"Actual vs. Predicted Vacant & Abandoned Buildings\"\n  )"
  },
  {
    "objectID": "labs/Zhang_Ye_Assignment4.html#model-performance-comparison",
    "href": "labs/Zhang_Ye_Assignment4.html#model-performance-comparison",
    "title": "Lab Assignment 4: Spatial Predictive Analysis",
    "section": "Model Performance Comparison",
    "text": "Model Performance Comparison\n\n\nCode\n# Calculate performance metrics\ncomparison &lt;- fishnet %&gt;%\n  st_drop_geometry() %&gt;%\n  filter(!is.na(prediction_nb), !is.na(prediction_kde)) %&gt;%\n  summarize(\n    model_mae = mean(abs(countBuildings - prediction_nb)),\n    model_rmse = sqrt(mean((countBuildings - prediction_nb)^2)),\n    kde_mae = mean(abs(countBuildings - prediction_kde)),\n    kde_rmse = sqrt(mean((countBuildings - prediction_kde)^2))\n  )\n\ncomparison %&gt;%\n  pivot_longer(everything(), names_to = \"metric\", values_to = \"value\") %&gt;%\n  separate(metric, into = c(\"approach\", \"metric\"), sep = \"_\") %&gt;%\n  pivot_wider(names_from = metric, values_from = value) %&gt;%\n  kable(\n    digits = 2,\n    caption = \"Model Performance Comparison\",\n    col.names = c(\"Approach\", \"MAE\", \"RMSE\")\n  ) %&gt;%\n  kable_styling(bootstrap_options = c(\"striped\", \"hover\"))\n\n\n\nModel Performance Comparison\n\n\nApproach\nMAE\nRMSE\n\n\n\n\nmodel\n13.50\n28.35\n\n\nkde\n15.66\n27.79\n\n\n\n\n\nThe Negative Binomial model slightly beats the KDE baseline with a MAE of 13.50 versus 15.66, meaning it’s off by about 2 fewer buildings per cell - a modest 14% improvement. While the performance gain isn’t huge, the model provides insight into why certain areas are at risk by showing that proximity to abandoned buildings and hot spots matters for prediction. Both approaches struggle in the same places, particularly underpredicting the most intense abandonment areas in the central west corridor and overpredicting in moderate zones."
  },
  {
    "objectID": "labs/Zhang_Ye_Assignment4.html#map-prediction-errors",
    "href": "labs/Zhang_Ye_Assignment4.html#map-prediction-errors",
    "title": "Lab Assignment 4: Spatial Predictive Analysis",
    "section": "Map Prediction Errors",
    "text": "Map Prediction Errors\n\n\nCode\nfishnet &lt;- fishnet %&gt;%\n  mutate(\n    error_nb = countBuildings - prediction_nb,\n    abs_error_nb = abs(error_nb)\n  )\n\n\np1 &lt;- ggplot() +\n  geom_sf(data = fishnet, aes(fill = error_nb), color = NA) +\n  scale_fill_gradient2(\n    name = \"Error\",\n    low = \"#2166ac\", mid = \"white\", high = \"#b2182b\",\n    midpoint = 0,\n    limits = c(-20, 20)\n  ) +\n  labs(title = \"Model Errors (Actual - Predicted)\",\n       subtitle = \"Red = underpredicted, Blue = overpredicted\") +\n  theme_buildings()\n\n\np2 &lt;- ggplot() +\n  geom_sf(data = fishnet, aes(fill = abs_error_nb), color = NA) +\n  scale_fill_viridis_c(name = \"Abs. Error\", option = \"magma\") +\n  labs(title = \"Absolute Model Errors\") +\n  theme_buildings()\n\np1 + p2\n\n\n\n\n\n\n\n\n\nThe error maps show a fairly balanced mix of underprediction (red) and overprediction (blue) scattered throughout the city, with no strong systematic spatial bias in the signed errors. However, the absolute error map reveals that the largest mistakes concentrate in specific hotspot areas in the central and western parts of Chicago, indicating the model struggles most with neighborhoods experiencing the most severe abandonment where counts are highest and most variable."
  },
  {
    "objectID": "labs/Tim_Ye_Assignment5.html",
    "href": "labs/Tim_Ye_Assignment5.html",
    "title": "Space-Time Prediction of Bike Share Demand: Philadelphia Indego",
    "section": "",
    "text": "Code\n# Core tidyverse\nlibrary(tidyverse)\nlibrary(lubridate)\n\n# Spatial data\nlibrary(sf)\nlibrary(tigris)\n\n# Census data\nlibrary(tidycensus)\n\n# Weather data\nlibrary(riem)  # For Philadelphia weather from ASOS stations\n\n# Visualization\nlibrary(viridis)\nlibrary(gridExtra)\nlibrary(knitr)\nlibrary(kableExtra)\n\n# here!\nlibrary(here)\n\n#Knearest distance\nlibrary(FNN)\n# Get rid of scientific notation. We gotta look good!\noptions(scipen = 999)\nSys.setlocale(\"LC_TIME\", \"English_United States\")\n\n\n[1] \"\"\n\n\n\n\nCode\nplotTheme &lt;- theme(\n  plot.title = element_text(size = 14, face = \"bold\"),\n  plot.subtitle = element_text(size = 10),\n  plot.caption = element_text(size = 8),\n  axis.text.x = element_text(size = 10, angle = 45, hjust = 1),\n  axis.text.y = element_text(size = 10),\n  axis.title = element_text(size = 11, face = \"bold\"),\n  panel.background = element_blank(),\n  panel.grid.major = element_line(colour = \"#D0D0D0\", size = 0.2),\n  panel.grid.minor = element_blank(),\n  axis.ticks = element_blank(),\n  legend.position = \"right\"\n)\n\nmapTheme &lt;- theme(\n  plot.title = element_text(size = 14, face = \"bold\"),\n  plot.subtitle = element_text(size = 10),\n  plot.caption = element_text(size = 8),\n  axis.line = element_blank(),\n  axis.text = element_blank(),\n  axis.ticks = element_blank(),\n  axis.title = element_blank(),\n  panel.background = element_blank(),\n  panel.border = element_blank(),\n  panel.grid.major = element_line(colour = 'transparent'),\n  panel.grid.minor = element_blank(),\n  legend.position = \"right\",\n  plot.margin = margin(1, 1, 1, 1, 'cm'),\n  legend.key.height = unit(1, \"cm\"),\n  legend.key.width = unit(0.2, \"cm\")\n)\n\npalette5 &lt;- c(\"#eff3ff\", \"#bdd7e7\", \"#6baed6\", \"#3182bd\", \"#08519c\")\n\n\n\n\nCode\n# Read Q4 2024 data\nindego &lt;- read_csv(here(\"data/indego-trips-2024-q4.csv\"))\n\n# Quick look at the data\nglimpse(indego)\n\n\nRows: 299,121\nColumns: 15\n$ trip_id             &lt;dbl&gt; 1050296434, 1050293063, 1050296229, 1050276817, 10…\n$ duration            &lt;dbl&gt; 22, 8, 12, 1, 5, 9, 5, 6, 16, 16, 16, 39, 6, 40, 1…\n$ start_time          &lt;chr&gt; \"10/1/2024 0:00\", \"10/1/2024 0:06\", \"10/1/2024 0:0…\n$ end_time            &lt;chr&gt; \"10/1/2024 0:22\", \"10/1/2024 0:14\", \"10/1/2024 0:1…\n$ start_station       &lt;dbl&gt; 3322, 3166, 3007, 3166, 3075, 3166, 3030, 3010, 33…\n$ start_lat           &lt;dbl&gt; 39.93638, 39.97195, 39.94517, 39.97195, 39.96718, …\n$ start_lon           &lt;dbl&gt; -75.15526, -75.13445, -75.15993, -75.13445, -75.16…\n$ end_station         &lt;dbl&gt; 3375, 3017, 3244, 3166, 3102, 3008, 3203, 3163, 33…\n$ end_lat             &lt;dbl&gt; 39.96036, 39.98003, 39.93865, 39.97195, 39.96759, …\n$ end_lon             &lt;dbl&gt; -75.14020, -75.14371, -75.16674, -75.13445, -75.17…\n$ bike_id             &lt;chr&gt; \"03580\", \"05386\", \"18082\", \"02729\", \"18519\", \"0272…\n$ plan_duration       &lt;dbl&gt; 365, 365, 365, 1, 30, 1, 365, 365, 30, 30, 30, 30,…\n$ trip_route_category &lt;chr&gt; \"One Way\", \"One Way\", \"One Way\", \"Round Trip\", \"On…\n$ passholder_type     &lt;chr&gt; \"Indego365\", \"Indego365\", \"Indego365\", \"Walk-up\", …\n$ bike_type           &lt;chr&gt; \"standard\", \"standard\", \"electric\", \"standard\", \"e…\n\n\n\n\nCode\nindego &lt;- indego %&gt;%\n  mutate(\n    # Parse datetime\n    start_datetime = mdy_hm(start_time),\n    end_datetime = mdy_hm(end_time),\n    \n    # Create hourly bins\n    interval60 = floor_date(start_datetime, unit = \"hour\"),\n    \n    # Extract time features\n    week = week(interval60),\n    month = month(interval60, label = TRUE),\n    dotw = wday(interval60, label = TRUE),\n    hour = hour(interval60),\n    date = as.Date(interval60),\n    \n    # Create useful indicators\n    weekend = ifelse(dotw %in% c(\"Sat\", \"Sun\"), 1, 0),\n    rush_hour = ifelse(hour %in% c(7, 8, 9, 16, 17, 18), 1, 0)\n  )\n\n# Look at temporal features\nhead(indego %&gt;% select(start_datetime, interval60, week, dotw, hour, weekend))\n\n\n# A tibble: 6 × 6\n  start_datetime      interval60           week dotw   hour weekend\n  &lt;dttm&gt;              &lt;dttm&gt;              &lt;dbl&gt; &lt;ord&gt; &lt;int&gt;   &lt;dbl&gt;\n1 2024-10-01 00:00:00 2024-10-01 00:00:00    40 Tue       0       0\n2 2024-10-01 00:06:00 2024-10-01 00:00:00    40 Tue       0       0\n3 2024-10-01 00:06:00 2024-10-01 00:00:00    40 Tue       0       0\n4 2024-10-01 00:06:00 2024-10-01 00:00:00    40 Tue       0       0\n5 2024-10-01 00:07:00 2024-10-01 00:00:00    40 Tue       0       0\n6 2024-10-01 00:08:00 2024-10-01 00:00:00    40 Tue       0       0\n\n\nGet Weather Data\n\n\nCode\n# Get weather from Philadelphia International Airport (KPHL)\n# This covers Q4 2024: October 1 - December 31\nweather_data &lt;- riem_measures(\n  station = \"PHL\",  # Philadelphia International Airport\n  date_start = \"2024-10-01\",\n  date_end = \"2024-12-31\"\n)\n\n# Process weather data\nweather_processed &lt;- weather_data %&gt;%\n  mutate(\n    interval60 = floor_date(valid, unit = \"hour\"),\n    Temperature = tmpf,  # Temperature in Fahrenheit\n    Precipitation = ifelse(is.na(p01i), 0, p01i),  # Hourly precip in inches\n    Wind_Speed = sknt  # Wind speed in knots\n  ) %&gt;%\n  select(interval60, Temperature, Precipitation, Wind_Speed) %&gt;%\n  distinct()\n\n# Check for missing hours and interpolate if needed\nweather_complete &lt;- weather_processed %&gt;%\n  complete(interval60 = seq(min(interval60), max(interval60), by = \"hour\")) %&gt;%\n  fill(Temperature, Precipitation, Wind_Speed, .direction = \"down\")\n\n# Look at the weather\nsummary(weather_complete %&gt;% select(Temperature, Precipitation, Wind_Speed))\n\n\n  Temperature    Precipitation        Wind_Speed    \n Min.   :12.00   Min.   :0.000000   Min.   : 0.000  \n 1st Qu.:41.00   1st Qu.:0.000000   1st Qu.: 4.000  \n Median :51.00   Median :0.000000   Median : 7.000  \n Mean   :50.80   Mean   :0.004177   Mean   : 7.663  \n 3rd Qu.:60.95   3rd Qu.:0.000000   3rd Qu.:11.000  \n Max.   :83.00   Max.   :0.520000   Max.   :30.000  \n\n\nCensus data download\n\n\nCode\n# Get Philadelphia census tracts\nphilly_census &lt;- get_acs(\n  geography = \"tract\",\n  variables = c(\n    \"B01003_001\",  # Total population\n    \"B19013_001\",  # Median household income\n    \"B08301_001\",  # Total commuters\n    \"B08301_010\",  # Commute by transit\n    \"B02001_002\",  # White alone\n    \"B25077_001\"   # Median home value\n  ),\n  state = \"PA\",\n  county = \"Philadelphia\",\n  year = 2022,\n  geometry = TRUE,\n  output = \"wide\"\n) %&gt;%\n  rename(\n    Total_Pop = B01003_001E,\n    Med_Inc = B19013_001E,\n    Total_Commuters = B08301_001E,\n    Transit_Commuters = B08301_010E,\n    White_Pop = B02001_002E,\n    Med_Home_Value = B25077_001E\n  ) %&gt;%\n  mutate(\n    Percent_Taking_Transit = (Transit_Commuters / Total_Commuters) * 100,\n    Percent_White = (White_Pop / Total_Pop) * 100\n  ) %&gt;%\n  st_transform(crs = 4326)  # WGS84 for lat/lon matching\n\n\n\n  |                                                                            \n  |                                                                      |   0%\n  |                                                                            \n  |=                                                                     |   1%\n  |                                                                            \n  |=                                                                     |   2%\n  |                                                                            \n  |==                                                                    |   2%\n  |                                                                            \n  |==                                                                    |   3%\n  |                                                                            \n  |===                                                                   |   4%\n  |                                                                            \n  |===                                                                   |   5%\n  |                                                                            \n  |====                                                                  |   6%\n  |                                                                            \n  |=====                                                                 |   6%\n  |                                                                            \n  |=====                                                                 |   7%\n  |                                                                            \n  |======                                                                |   8%\n  |                                                                            \n  |======                                                                |   9%\n  |                                                                            \n  |=======                                                               |   9%\n  |                                                                            \n  |=======                                                               |  11%\n  |                                                                            \n  |========                                                              |  12%\n  |                                                                            \n  |=========                                                             |  12%\n  |                                                                            \n  |=========                                                             |  13%\n  |                                                                            \n  |==========                                                            |  14%\n  |                                                                            \n  |===========                                                           |  15%\n  |                                                                            \n  |===========                                                           |  16%\n  |                                                                            \n  |============                                                          |  17%\n  |                                                                            \n  |=============                                                         |  18%\n  |                                                                            \n  |=============                                                         |  19%\n  |                                                                            \n  |==============                                                        |  20%\n  |                                                                            \n  |===============                                                       |  21%\n  |                                                                            \n  |===============                                                       |  22%\n  |                                                                            \n  |================                                                      |  23%\n  |                                                                            \n  |=================                                                     |  24%\n  |                                                                            \n  |==================                                                    |  25%\n  |                                                                            \n  |==================                                                    |  26%\n  |                                                                            \n  |===================                                                   |  27%\n  |                                                                            \n  |====================                                                  |  28%\n  |                                                                            \n  |====================                                                  |  29%\n  |                                                                            \n  |=====================                                                 |  30%\n  |                                                                            \n  |======================                                                |  31%\n  |                                                                            \n  |======================                                                |  32%\n  |                                                                            \n  |=======================                                               |  33%\n  |                                                                            \n  |========================                                              |  34%\n  |                                                                            \n  |========================                                              |  35%\n  |                                                                            \n  |=========================                                             |  36%\n  |                                                                            \n  |==========================                                            |  37%\n  |                                                                            \n  |==========================                                            |  38%\n  |                                                                            \n  |===========================                                           |  39%\n  |                                                                            \n  |============================                                          |  40%\n  |                                                                            \n  |=============================                                         |  41%\n  |                                                                            \n  |=============================                                         |  42%\n  |                                                                            \n  |==============================                                        |  43%\n  |                                                                            \n  |===============================                                       |  44%\n  |                                                                            \n  |===============================                                       |  45%\n  |                                                                            \n  |================================                                      |  46%\n  |                                                                            \n  |=================================                                     |  47%\n  |                                                                            \n  |=================================                                     |  48%\n  |                                                                            \n  |==================================                                    |  49%\n  |                                                                            \n  |===================================                                   |  50%\n  |                                                                            \n  |===================================                                   |  51%\n  |                                                                            \n  |====================================                                  |  52%\n  |                                                                            \n  |=====================================                                 |  53%\n  |                                                                            \n  |=====================================                                 |  54%\n  |                                                                            \n  |======================================                                |  54%\n  |                                                                            \n  |=======================================                               |  55%\n  |                                                                            \n  |========================================                              |  56%\n  |                                                                            \n  |========================================                              |  57%\n  |                                                                            \n  |=========================================                             |  58%\n  |                                                                            \n  |==========================================                            |  59%\n  |                                                                            \n  |==========================================                            |  60%\n  |                                                                            \n  |===========================================                           |  61%\n  |                                                                            \n  |============================================                          |  62%\n  |                                                                            \n  |============================================                          |  63%\n  |                                                                            \n  |=============================================                         |  64%\n  |                                                                            \n  |==============================================                        |  65%\n  |                                                                            \n  |==============================================                        |  66%\n  |                                                                            \n  |===============================================                       |  67%\n  |                                                                            \n  |================================================                      |  68%\n  |                                                                            \n  |================================================                      |  69%\n  |                                                                            \n  |=================================================                     |  70%\n  |                                                                            \n  |==================================================                    |  71%\n  |                                                                            \n  |===================================================                   |  72%\n  |                                                                            \n  |===================================================                   |  73%\n  |                                                                            \n  |====================================================                  |  74%\n  |                                                                            \n  |=====================================================                 |  75%\n  |                                                                            \n  |=====================================================                 |  76%\n  |                                                                            \n  |======================================================                |  77%\n  |                                                                            \n  |=======================================================               |  78%\n  |                                                                            \n  |=======================================================               |  79%\n  |                                                                            \n  |========================================================              |  80%\n  |                                                                            \n  |=========================================================             |  81%\n  |                                                                            \n  |=========================================================             |  82%\n  |                                                                            \n  |==========================================================            |  83%\n  |                                                                            \n  |===========================================================           |  84%\n  |                                                                            \n  |===========================================================           |  85%\n  |                                                                            \n  |============================================================          |  86%\n  |                                                                            \n  |=============================================================         |  87%\n  |                                                                            \n  |=============================================================         |  88%\n  |                                                                            \n  |==============================================================        |  89%\n  |                                                                            \n  |===============================================================       |  90%\n  |                                                                            \n  |================================================================      |  91%\n  |                                                                            \n  |================================================================      |  92%\n  |                                                                            \n  |=================================================================     |  93%\n  |                                                                            \n  |==================================================================    |  94%\n  |                                                                            \n  |==================================================================    |  95%\n  |                                                                            \n  |===================================================================   |  96%\n  |                                                                            \n  |====================================================================  |  97%\n  |                                                                            \n  |====================================================================  |  98%\n  |                                                                            \n  |===================================================================== |  99%\n  |                                                                            \n  |======================================================================| 100%\n\n\nCode\n# Check the data\nglimpse(philly_census)\n\n\nRows: 408\nColumns: 17\n$ GEOID                  &lt;chr&gt; \"42101001500\", \"42101001800\", \"42101002802\", \"4…\n$ NAME                   &lt;chr&gt; \"Census Tract 15; Philadelphia County; Pennsylv…\n$ Total_Pop              &lt;dbl&gt; 3251, 3300, 5720, 4029, 4415, 1815, 3374, 2729,…\n$ B01003_001M            &lt;dbl&gt; 677, 369, 796, 437, 853, 210, 480, 734, 763, 11…\n$ Med_Inc                &lt;dbl&gt; 110859, 114063, 78871, 61583, 32347, 48581, 597…\n$ B19013_001M            &lt;dbl&gt; 24975, 30714, 20396, 22293, 4840, 13812, 6278, …\n$ Total_Commuters        &lt;dbl&gt; 2073, 2255, 3032, 2326, 1980, 969, 2427, 708, 2…\n$ B08301_001M            &lt;dbl&gt; 387, 308, 478, 383, 456, 189, 380, 281, 456, 68…\n$ Transit_Commuters      &lt;dbl&gt; 429, 123, 685, 506, 534, 192, 658, 218, 438, 51…\n$ B08301_010M            &lt;dbl&gt; 188, 66, 219, 144, 285, 71, 278, 184, 176, 235,…\n$ White_Pop              &lt;dbl&gt; 2185, 2494, 3691, 3223, 182, 984, 2111, 231, 35…\n$ B02001_002M            &lt;dbl&gt; 268, 381, 592, 380, 88, 190, 463, 112, 238, 778…\n$ Med_Home_Value         &lt;dbl&gt; 568300, 605000, 350600, 296400, 76600, 289700, …\n$ B25077_001M            &lt;dbl&gt; 58894, 34876, 12572, 22333, 10843, 118720, 1506…\n$ geometry               &lt;MULTIPOLYGON [°]&gt; MULTIPOLYGON (((-75.16558 3..., MU…\n$ Percent_Taking_Transit &lt;dbl&gt; 20.694645, 5.454545, 22.592348, 21.754084, 26.9…\n$ Percent_White          &lt;dbl&gt; 67.2100892, 75.5757576, 64.5279720, 79.9950360,…\n\n\nData Structure Check\n\n\nCode\n# How many trips?\ncat(\"Total trips in Q4 2024:\", nrow(indego), \"\\n\")\n\n\nTotal trips in Q4 2024: 299121 \n\n\nCode\n# Date range\ncat(\"Date range:\", \n    min(mdy_hm(indego$start_time)), \"to\", \n    max(mdy_hm(indego$start_time)), \"\\n\")\n\n\nDate range: 1727740800 to 1735689360 \n\n\nCode\n# How many unique stations?\ncat(\"Unique start stations:\", length(unique(indego$start_station)), \"\\n\")\n\n\nUnique start stations: 256 \n\n\nCode\n# Trip types\ntable(indego$trip_route_category)\n\n\n\n   One Way Round Trip \n    282675      16446 \n\n\nCode\n# Passholder types\ntable(indego$passholder_type)\n\n\n\n  Day Pass   Indego30  Indego365 IndegoFlex       NULL    Walk-up \n     10711     159895     112690          1       1165      14659 \n\n\nCode\n# Bike types\ntable(indego$bike_type)\n\n\n\nelectric standard \n  175503   123618 \n\n\nCreate Space-Time Panel\n\n\nCode\n# Create sf object for stations\nstations_sf &lt;- indego %&gt;%\n  distinct(start_station, start_lat, start_lon) %&gt;%\n  filter(!is.na(start_lat), !is.na(start_lon)) %&gt;%\n  st_as_sf(coords = c(\"start_lon\", \"start_lat\"), crs = 4326)\n\n# Spatial join to get census tract for each station\nstations_census &lt;- st_join(stations_sf, philly_census, left = TRUE) %&gt;%\n  st_drop_geometry()\n\n\nstations_for_map &lt;- indego %&gt;%\n  distinct(start_station, start_lat, start_lon) %&gt;%\n  filter(!is.na(start_lat), !is.na(start_lon)) %&gt;%\n  left_join(\n    stations_census %&gt;% select(start_station, Med_Inc),\n    by = \"start_station\"\n  ) %&gt;%\n  mutate(has_census = !is.na(Med_Inc))\n\n# Add back to trip data\nindego_census &lt;- indego %&gt;%\n  left_join(\n    stations_census %&gt;% \n      select(start_station, Med_Inc, Percent_Taking_Transit, \n             Percent_White, Total_Pop),\n    by = \"start_station\"\n  )\n# Identify which stations to keep\nvalid_stations &lt;- stations_census %&gt;%\n  filter(!is.na(Med_Inc)) %&gt;%\n  pull(start_station)\n\n# Filter trip data to valid stations only\nindego_census &lt;- indego %&gt;%\n  filter(start_station %in% valid_stations) %&gt;%\n  left_join(\n    stations_census %&gt;% \n      select(start_station, Med_Inc, Percent_Taking_Transit, \n             Percent_White, Total_Pop),\n    by = \"start_station\"\n  )\n\n\n\n\nCode\n# Count trips by station-hour\ntrips_panel &lt;- indego_census %&gt;%\n  group_by(interval60, start_station, start_lat, start_lon,\n           Med_Inc, Percent_Taking_Transit, Percent_White, Total_Pop) %&gt;%\n  summarize(Trip_Count = n()) %&gt;%\n  ungroup()\n\n# Calculate expected panel size\nn_stations &lt;- length(unique(trips_panel$start_station))\nn_hours &lt;- length(unique(trips_panel$interval60))\nexpected_rows &lt;- n_stations * n_hours\n\ncat(\"Expected panel rows:\", format(expected_rows, big.mark = \",\"), \"\\n\")\n\n\nExpected panel rows: 523,296 \n\n\nCode\ncat(\"Current rows:\", format(nrow(trips_panel), big.mark = \",\"), \"\\n\")\n\n\nCurrent rows: 150,972 \n\n\nCode\ncat(\"Missing rows:\", format(expected_rows - nrow(trips_panel), big.mark = \",\"), \"\\n\")\n\n\nMissing rows: 372,324 \n\n\nCode\n# Create complete panel\nstudy_panel &lt;- expand.grid(\n  interval60 = unique(trips_panel$interval60),\n  start_station = unique(trips_panel$start_station)\n) %&gt;%\n  # Join trip counts\n  left_join(trips_panel, by = c(\"interval60\", \"start_station\")) %&gt;%\n  # Replace NA trip counts with 0\n  mutate(Trip_Count = replace_na(Trip_Count, 0))\n\n# Fill in station attributes (they're the same for all hours)\nstation_attributes &lt;- trips_panel %&gt;%\n  group_by(start_station) %&gt;%\n  summarize(\n    start_lat = first(start_lat),\n    start_lon = first(start_lon),\n    Med_Inc = first(Med_Inc),\n    Percent_Taking_Transit = first(Percent_Taking_Transit),\n    Percent_White = first(Percent_White),\n    Total_Pop = first(Total_Pop)\n  )\n\nstudy_panel &lt;- study_panel %&gt;%\n  left_join(station_attributes, by = \"start_station\")\n\n\n\n\nCode\nstudy_panel &lt;- study_panel %&gt;%\n  mutate(\n    week = week(interval60),\n    month = month(interval60, label = TRUE),\n    dotw = wday(interval60, label = TRUE),\n    hour = hour(interval60),\n    date = as.Date(interval60),\n    weekend = ifelse(dotw %in% c(\"Sat\", \"Sun\"), 1, 0),\n    rush_hour = ifelse(hour %in% c(7, 8, 9, 16, 17, 18), 1, 0)\n  )\nstudy_panel &lt;- study_panel %&gt;%\n  left_join(weather_complete, by = \"interval60\")\n\n\nCreate Temporal Lag\n\n\nCode\n# Sort by station and time\nstudy_panel &lt;- study_panel %&gt;%\n  arrange(start_station, interval60)\n\n# Create lag variables WITHIN each station\nstudy_panel &lt;- study_panel %&gt;%\n  group_by(start_station) %&gt;%\n  mutate(\n    lag1Hour = lag(Trip_Count, 1),\n    lag2Hours = lag(Trip_Count, 2),\n    lag3Hours = lag(Trip_Count, 3),\n    lag12Hours = lag(Trip_Count, 12),\n    lag1day = lag(Trip_Count, 24)\n  ) %&gt;%\n  ungroup()\n\n# Remove rows with NA lags (first 24 hours for each station)\nstudy_panel_complete &lt;- study_panel %&gt;%\n  filter(!is.na(lag1day))\n\n\nTemporal Train/Test Split\n\n\nCode\n# Split by week\n\n# Which stations have trips in BOTH early and late periods?\nearly_stations &lt;- study_panel_complete %&gt;%\n  filter(week &lt; 50) %&gt;%\n  filter(Trip_Count &gt; 0) %&gt;%\n  distinct(start_station) %&gt;%\n  pull(start_station)\n\nlate_stations &lt;- study_panel_complete %&gt;%\n  filter(week &gt;= 50) %&gt;%\n  filter(Trip_Count &gt; 0) %&gt;%\n  distinct(start_station) %&gt;%\n  pull(start_station)\n\n# Keep only stations that appear in BOTH periods\ncommon_stations &lt;- intersect(early_stations, late_stations)\n\n\n# Filter panel to only common stations\nstudy_panel_complete &lt;- study_panel_complete %&gt;%\n  filter(start_station %in% common_stations)\n\n# NOW create train/test split\ntrain &lt;- study_panel_complete %&gt;%\n  filter(week &lt; 50)\n\ntest &lt;- study_panel_complete %&gt;%\n  filter(week &gt;= 50)\n\ncat(\"Training observations:\", format(nrow(train), big.mark = \",\"), \"\\n\")\n\n\nTraining observations: 410,628 \n\n\nCode\ncat(\"Testing observations:\", format(nrow(test), big.mark = \",\"), \"\\n\")\n\n\nTesting observations: 171,684 \n\n\nCode\ncat(\"Training date range:\", min(train$date), \"to\", max(train$date), \"\\n\")\n\n\nTraining date range: 19997 to 20065 \n\n\nCode\ncat(\"Testing date range:\", min(test$date), \"to\", max(test$date), \"\\n\")\n\n\nTesting date range: 20066 to 20088 \n\n\nHourly Patterns\n\n\nCode\n# Average trips by hour and day type\nhourly_patterns &lt;- indego %&gt;%\n  group_by(hour, weekend) %&gt;%\n  summarize(avg_trips = n() / n_distinct(date)) %&gt;%\n  mutate(day_type = ifelse(weekend == 1, \"Weekend\", \"Weekday\"))\n\nggplot(hourly_patterns, aes(x = hour, y = avg_trips, color = day_type)) +\n  geom_line(linewidth = 1.2) +\n  scale_color_manual(values = c(\"Weekday\" = \"#08519c\", \"Weekend\" = \"#6baed6\")) +\n  labs(\n    title = \"Average Hourly Ridership Patterns - Q4 2024\",\n    subtitle = \"Clear commute patterns on weekdays\",\n    x = \"Hour of Day\",\n    y = \"Average Trips per Hour\",\n    color = \"Day Type\"\n  ) +\n  plotTheme\n\n\n\n\n\n\n\n\n\nTrips over time\n\n\nCode\n# Daily trip counts\ndaily_trips &lt;- indego %&gt;%\n  group_by(date) %&gt;%\n  summarize(trips = n())\n\nggplot(daily_trips, aes(x = date, y = trips)) +\n  geom_line(color = \"#3182bd\", linewidth = 1) +\n  geom_smooth(se = FALSE, color = \"red\", linetype = \"dashed\") +\n  labs(\n    title = \"Indego Daily Ridership - Q4 2024\",\n    subtitle = \"Winter demand patterns in Philadelphia\",\n    x = \"Date\",\n    y = \"Daily Trips\",\n    caption = \"Source: Indego bike share\"\n  ) +\n  plotTheme\n\n\n\n\n\n\n\n\n\nModel 1\n\n\nCode\n# Create day of week factor with treatment (dummy) coding\n\ntrain &lt;- train %&gt;%\n  mutate(dotw_simple = factor(dotw, levels = c(\"Mon\", \"Tue\", \"Wed\", \"Thu\", \"Fri\", \"Sat\", \"Sun\")))\n\n# Set contrasts to treatment coding (dummy variables)\ncontrasts(train$dotw_simple) &lt;- contr.treatment(7)\n\n# Now run the model\nmodel1 &lt;- lm(\n  Trip_Count ~ as.factor(hour) + dotw_simple + Temperature + Precipitation,\n  data = train\n)\nsummary(model1)\n\n\n\nCall:\nlm(formula = Trip_Count ~ as.factor(hour) + dotw_simple + Temperature + \n    Precipitation, data = train)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-1.7089 -0.6783 -0.2159  0.1882 27.2727 \n\nCoefficients:\n                   Estimate Std. Error t value             Pr(&gt;|t|)    \n(Intercept)       -0.489156   0.013731 -35.624 &lt; 0.0000000000000002 ***\nas.factor(hour)1  -0.055267   0.012906  -4.282   0.0000184994686084 ***\nas.factor(hour)2  -0.078435   0.012690  -6.181   0.0000000006378151 ***\nas.factor(hour)3  -0.110234   0.012655  -8.711 &lt; 0.0000000000000002 ***\nas.factor(hour)4  -0.074204   0.012566  -5.905   0.0000000035263476 ***\nas.factor(hour)5   0.038930   0.012623   3.084             0.002042 ** \nas.factor(hour)6   0.296063   0.012639  23.424 &lt; 0.0000000000000002 ***\nas.factor(hour)7   0.562184   0.012853  43.738 &lt; 0.0000000000000002 ***\nas.factor(hour)8   0.940524   0.012603  74.629 &lt; 0.0000000000000002 ***\nas.factor(hour)9   0.681527   0.012691  53.700 &lt; 0.0000000000000002 ***\nas.factor(hour)10  0.562441   0.012526  44.900 &lt; 0.0000000000000002 ***\nas.factor(hour)11  0.602993   0.012538  48.094 &lt; 0.0000000000000002 ***\nas.factor(hour)12  0.698148   0.012435  56.145 &lt; 0.0000000000000002 ***\nas.factor(hour)13  0.667308   0.012338  54.085 &lt; 0.0000000000000002 ***\nas.factor(hour)14  0.685610   0.012386  55.355 &lt; 0.0000000000000002 ***\nas.factor(hour)15  0.791576   0.012780  61.940 &lt; 0.0000000000000002 ***\nas.factor(hour)16  0.906925   0.012549  72.273 &lt; 0.0000000000000002 ***\nas.factor(hour)17  1.110205   0.012639  87.842 &lt; 0.0000000000000002 ***\nas.factor(hour)18  0.831297   0.012728  65.314 &lt; 0.0000000000000002 ***\nas.factor(hour)19  0.541516   0.012768  42.412 &lt; 0.0000000000000002 ***\nas.factor(hour)20  0.341348   0.012858  26.547 &lt; 0.0000000000000002 ***\nas.factor(hour)21  0.233603   0.012881  18.135 &lt; 0.0000000000000002 ***\nas.factor(hour)22  0.175194   0.012820  13.666 &lt; 0.0000000000000002 ***\nas.factor(hour)23  0.076033   0.012906   5.891   0.0000000038337890 ***\ndotw_simple2       0.065588   0.006869   9.549 &lt; 0.0000000000000002 ***\ndotw_simple3       0.062920   0.006857   9.177 &lt; 0.0000000000000002 ***\ndotw_simple4      -0.050061   0.006571  -7.619   0.0000000000000257 ***\ndotw_simple5      -0.012223   0.006833  -1.789             0.073612 .  \ndotw_simple6      -0.024444   0.006806  -3.591             0.000329 ***\ndotw_simple7      -0.050983   0.006924  -7.363   0.0000000000001797 ***\nTemperature        0.012467   0.000161  77.432 &lt; 0.0000000000000002 ***\nPrecipitation     -1.109233   0.081793 -13.562 &lt; 0.0000000000000002 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.144 on 410596 degrees of freedom\nMultiple R-squared:  0.1141,    Adjusted R-squared:  0.114 \nF-statistic:  1705 on 31 and 410596 DF,  p-value: &lt; 0.00000000000000022\n\n\nModel 2\n\n\nCode\nmodel2 &lt;- lm(\n  Trip_Count ~ as.factor(hour) + dotw_simple + Temperature + Precipitation +\n    lag1Hour + lag3Hours + lag1day,\n  data = train\n)\n\nsummary(model2)\n\n\n\nCall:\nlm(formula = Trip_Count ~ as.factor(hour) + dotw_simple + Temperature + \n    Precipitation + lag1Hour + lag3Hours + lag1day, data = train)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-12.8428  -0.4697  -0.1265   0.1225  25.5313 \n\nCoefficients:\n                    Estimate Std. Error t value             Pr(&gt;|t|)    \n(Intercept)       -0.1971724  0.0120171 -16.408 &lt; 0.0000000000000002 ***\nas.factor(hour)1  -0.0175731  0.0112639  -1.560              0.11873    \nas.factor(hour)2  -0.0093400  0.0110785  -0.843              0.39918    \nas.factor(hour)3  -0.0323163  0.0110523  -2.924              0.00346 ** \nas.factor(hour)4  -0.0064036  0.0109793  -0.583              0.55973    \nas.factor(hour)5   0.0625618  0.0110386   5.668  0.00000001449553616 ***\nas.factor(hour)6   0.2581886  0.0110639  23.336 &lt; 0.0000000000000002 ***\nas.factor(hour)7   0.4087297  0.0112659  36.280 &lt; 0.0000000000000002 ***\nas.factor(hour)8   0.6475387  0.0110725  58.482 &lt; 0.0000000000000002 ***\nas.factor(hour)9   0.2687374  0.0111533  24.095 &lt; 0.0000000000000002 ***\nas.factor(hour)10  0.2122365  0.0109778  19.333 &lt; 0.0000000000000002 ***\nas.factor(hour)11  0.2554385  0.0109944  23.234 &lt; 0.0000000000000002 ***\nas.factor(hour)12  0.3583675  0.0108988  32.881 &lt; 0.0000000000000002 ***\nas.factor(hour)13  0.3239304  0.0108163  29.948 &lt; 0.0000000000000002 ***\nas.factor(hour)14  0.3531805  0.0108536  32.540 &lt; 0.0000000000000002 ***\nas.factor(hour)15  0.4322898  0.0112024  38.589 &lt; 0.0000000000000002 ***\nas.factor(hour)16  0.5127136  0.0110123  46.558 &lt; 0.0000000000000002 ***\nas.factor(hour)17  0.6404994  0.0111130  57.635 &lt; 0.0000000000000002 ***\nas.factor(hour)18  0.3266069  0.0112005  29.160 &lt; 0.0000000000000002 ***\nas.factor(hour)19  0.1612724  0.0111995  14.400 &lt; 0.0000000000000002 ***\nas.factor(hour)20  0.0449273  0.0112804   3.983  0.00006812693842296 ***\nas.factor(hour)21  0.0488508  0.0112677   4.335  0.00001454699054146 ***\nas.factor(hour)22  0.0606778  0.0111954   5.420  0.00000005967221309 ***\nas.factor(hour)23  0.0183651  0.0112646   1.630              0.10303    \ndotw_simple2       0.0070021  0.0059975   1.167              0.24301    \ndotw_simple3      -0.0128484  0.0059925  -2.144              0.03203 *  \ndotw_simple4      -0.0448243  0.0057357  -7.815  0.00000000000000551 ***\ndotw_simple5      -0.0422563  0.0059684  -7.080  0.00000000000144379 ***\ndotw_simple6      -0.0367596  0.0059417  -6.187  0.00000000061504630 ***\ndotw_simple7      -0.0593011  0.0060464  -9.808 &lt; 0.0000000000000002 ***\nTemperature        0.0040641  0.0001426  28.494 &lt; 0.0000000000000002 ***\nPrecipitation     -0.9676346  0.0714492 -13.543 &lt; 0.0000000000000002 ***\nlag1Hour           0.3238000  0.0014825 218.412 &lt; 0.0000000000000002 ***\nlag3Hours          0.1180469  0.0014504  81.387 &lt; 0.0000000000000002 ***\nlag1day            0.2027487  0.0013895 145.912 &lt; 0.0000000000000002 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.9988 on 410593 degrees of freedom\nMultiple R-squared:  0.3252,    Adjusted R-squared:  0.3252 \nF-statistic:  5821 on 34 and 410593 DF,  p-value: &lt; 0.00000000000000022\n\n\nModel 3\n\n\nCode\nmodel3 &lt;- lm(\n  Trip_Count ~ as.factor(hour) + dotw_simple + Temperature + Precipitation +\n    lag1Hour + lag3Hours + lag1day +\n    Med_Inc.x + Percent_Taking_Transit.y + Percent_White.y,\n  data = train\n)\n\nsummary(model3)\n\n\n\nCall:\nlm(formula = Trip_Count ~ as.factor(hour) + dotw_simple + Temperature + \n    Precipitation + lag1Hour + lag3Hours + lag1day + Med_Inc.x + \n    Percent_Taking_Transit.y + Percent_White.y, data = train)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-10.2603  -0.7198  -0.2719   0.4456  24.6278 \n\nCoefficients:\n                              Estimate    Std. Error t value\n(Intercept)               0.7009115282  0.0386608126  18.130\nas.factor(hour)1         -0.0339064631  0.0443755122  -0.764\nas.factor(hour)2         -0.0733390073  0.0472207425  -1.553\nas.factor(hour)3         -0.1736198797  0.0563179188  -3.083\nas.factor(hour)4         -0.1602016516  0.0526556216  -3.042\nas.factor(hour)5         -0.0842198175  0.0392042217  -2.148\nas.factor(hour)6          0.1812725910  0.0339389718   5.341\nas.factor(hour)7          0.3659932453  0.0327798040  11.165\nas.factor(hour)8          0.6419351722  0.0315874296  20.322\nas.factor(hour)9          0.0689540443  0.0318174207   2.167\nas.factor(hour)10         0.0455849373  0.0318601406   1.431\nas.factor(hour)11         0.0951277607  0.0317831322   2.993\nas.factor(hour)12         0.1718233329  0.0312986651   5.490\nas.factor(hour)13         0.1501299648  0.0313008986   4.796\nas.factor(hour)14         0.1546199385  0.0311814820   4.959\nas.factor(hour)15         0.2738677333  0.0314129060   8.718\nas.factor(hour)16         0.3825663042  0.0310863390  12.307\nas.factor(hour)17         0.5958195674  0.0311303049  19.140\nas.factor(hour)18         0.1871034280  0.0314626011   5.947\nas.factor(hour)19         0.0105635560  0.0320040115   0.330\nas.factor(hour)20        -0.0963042374  0.0327876103  -2.937\nas.factor(hour)21        -0.0718694380  0.0336061149  -2.139\nas.factor(hour)22        -0.0356940800  0.0343008260  -1.041\nas.factor(hour)23        -0.0793554621  0.0360569574  -2.201\ndotw_simple2              0.0015399442  0.0129720488   0.119\ndotw_simple3             -0.0058302798  0.0131356919  -0.444\ndotw_simple4             -0.0584003672  0.0128820191  -4.533\ndotw_simple5             -0.0902616975  0.0132777688  -6.798\ndotw_simple6             -0.0296928615  0.0133240066  -2.229\ndotw_simple7             -0.0404284310  0.0137730955  -2.935\nTemperature               0.0060129110  0.0003255823  18.468\nPrecipitation            -2.6993476761  0.2537854011 -10.636\nlag1Hour                  0.2432506608  0.0023823455 102.106\nlag3Hours                 0.0728128784  0.0024806873  29.352\nlag1day                   0.1565602280  0.0022935086  68.262\nMed_Inc.x                -0.0000001894  0.0000001237  -1.531\nPercent_Taking_Transit.y -0.0038935558  0.0004528281  -8.598\nPercent_White.y           0.0037864819  0.0002282348  16.590\n                                     Pr(&gt;|t|)    \n(Intercept)              &lt; 0.0000000000000002 ***\nas.factor(hour)1                      0.44482    \nas.factor(hour)2                      0.12040    \nas.factor(hour)3                      0.00205 ** \nas.factor(hour)4                      0.00235 ** \nas.factor(hour)5                      0.03170 *  \nas.factor(hour)6              0.0000000925173 ***\nas.factor(hour)7         &lt; 0.0000000000000002 ***\nas.factor(hour)8         &lt; 0.0000000000000002 ***\nas.factor(hour)9                      0.03022 *  \nas.factor(hour)10                     0.15249    \nas.factor(hour)11                     0.00276 ** \nas.factor(hour)12             0.0000000403123 ***\nas.factor(hour)13             0.0000016175893 ***\nas.factor(hour)14             0.0000007104928 ***\nas.factor(hour)15        &lt; 0.0000000000000002 ***\nas.factor(hour)16        &lt; 0.0000000000000002 ***\nas.factor(hour)17        &lt; 0.0000000000000002 ***\nas.factor(hour)18             0.0000000027402 ***\nas.factor(hour)19                     0.74135    \nas.factor(hour)20                     0.00331 ** \nas.factor(hour)21                     0.03247 *  \nas.factor(hour)22                     0.29805    \nas.factor(hour)23                     0.02775 *  \ndotw_simple2                          0.90550    \ndotw_simple3                          0.65715    \ndotw_simple4                  0.0000058070122 ***\ndotw_simple5                  0.0000000000107 ***\ndotw_simple6                          0.02585 *  \ndotw_simple7                          0.00333 ** \nTemperature              &lt; 0.0000000000000002 ***\nPrecipitation            &lt; 0.0000000000000002 ***\nlag1Hour                 &lt; 0.0000000000000002 ***\nlag3Hours                &lt; 0.0000000000000002 ***\nlag1day                  &lt; 0.0000000000000002 ***\nMed_Inc.x                             0.12579    \nPercent_Taking_Transit.y &lt; 0.0000000000000002 ***\nPercent_White.y          &lt; 0.0000000000000002 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.274 on 133822 degrees of freedom\n  (276768 observations deleted due to missingness)\nMultiple R-squared:  0.2192,    Adjusted R-squared:  0.219 \nF-statistic:  1015 on 37 and 133822 DF,  p-value: &lt; 0.00000000000000022\n\n\nModel 4\n\n\nCode\nmodel4 &lt;- lm(\n  Trip_Count ~ as.factor(hour) + dotw_simple + Temperature + Precipitation +\n    lag1Hour + lag3Hours + lag1day +\n    Med_Inc.x + Percent_Taking_Transit.y + Percent_White.y +\n    as.factor(start_station),\n  data = train\n)\n\n# Summary too long with all station dummies, just show key metrics\ncat(\"Model 4 R-squared:\", summary(model4)$r.squared, \"\\n\")\n\n\nModel 4 R-squared: 0.2469249 \n\n\nCode\ncat(\"Model 4 Adj R-squared:\", summary(model4)$adj.r.squared, \"\\n\")\n\n\nModel 4 Adj R-squared: 0.2454536 \n\n\nModel 5\n\n\nCode\nmodel5 &lt;- lm(\n  Trip_Count ~ as.factor(hour) + dotw_simple + Temperature + Precipitation +\n    lag1Hour + lag3Hours + lag1day + rush_hour + as.factor(month) +\n    Med_Inc.x + Percent_Taking_Transit.y + Percent_White.y +\n    as.factor(start_station) +\n    rush_hour * weekend,  # Rush hour effects different on weekends\n  data = train\n)\n\ncat(\"Model 5 R-squared:\", summary(model5)$r.squared, \"\\n\")\n\n\nModel 5 R-squared: 0.2526638 \n\n\nCode\ncat(\"Model 5 Adj R-squared:\", summary(model5)$adj.r.squared, \"\\n\")\n\n\nModel 5 Adj R-squared: 0.251187 \n\n\nMAE Calculation\n\n\nCode\n# Create day of week factor with treatment (dummy) coding\ntest &lt;- test %&gt;%\n  mutate(dotw_simple = factor(dotw, levels = c(\"Mon\", \"Tue\", \"Wed\", \"Thu\", \"Fri\", \"Sat\", \"Sun\")))\n\n# Set contrasts to treatment coding (dummy variables)\ncontrasts(test$dotw_simple) &lt;- contr.treatment(7)\n\ntest &lt;- test %&gt;%\n  mutate(\n    pred1 = predict(model1, newdata = test),\n    pred2 = predict(model2, newdata = test),\n    pred3 = predict(model3, newdata = test),\n    pred4 = predict(model4, newdata = test),\n    pred5 = predict(model5, newdata = test)\n  )\n\n# Calculate MAE for each model\nmae_results &lt;- data.frame(\n  Model = c(\n    \"1. Time + Weather\",\n    \"2. + Temporal Lags\",\n    \"3. + Demographics\",\n    \"4. + Station FE\",\n    \"5. + Rush Hour Interaction\"\n  ),\n  MAE = c(\n    mean(abs(test$Trip_Count - test$pred1), na.rm = TRUE),\n    mean(abs(test$Trip_Count - test$pred2), na.rm = TRUE),\n    mean(abs(test$Trip_Count - test$pred3), na.rm = TRUE),\n    mean(abs(test$Trip_Count - test$pred4), na.rm = TRUE),\n    mean(abs(test$Trip_Count - test$pred5), na.rm = TRUE)\n  )\n)\n\nkable(mae_results, \n      digits = 2,\n      caption = \"Mean Absolute Error by Model (Test Set)\",\n      col.names = c(\"Model\", \"MAE (trips)\")) %&gt;%\n  kable_styling(bootstrap_options = c(\"striped\", \"hover\"))\n\n\n\nMean Absolute Error by Model (Test Set)\n\n\nModel\nMAE (trips)\n\n\n\n\n1. Time + Weather\n0.54\n\n\n2. + Temporal Lags\n0.40\n\n\n3. + Demographics\n0.64\n\n\n4. + Station FE\n0.66\n\n\n5. + Rush Hour Interaction\n0.65\n\n\n\n\n\nCompare results to Q1 2025:\nThe MAE values are lower for Q4 2024 for all of the models compared to Q1 2025, and the difference is between 0.06 and 0.10. Model 2 with Temporal Lags has the lowest MAE for both Q4 2024 and Q1 2025. Some of the reasons that could explain the Q1 2025 has a higher MAE include Q1 typically has more unpredictable ridership due to variable weather—snow days, temperature swings, and rain can make trips more unstable compared to Q4, when weather patterns tend to be more stable in the fall. There could be more snow days that heavily impact trips. Also, during the spring, there are New Year holiday travel and spring break, which make the commute pattern less regular and make the prediction harder. With fewer trips in Q1, relative prediction errors can appear larger. Q4 2024 has more than 299,121 observations, and Q1 2025 only has 201,588 observations. Both Q4 2024 and Q1 2025’s model 3-5 increase MAE after adding more factors. This could suggest that demographics, station fixed effects, and rush hour improve the in-sample fit but don’t generalize well to the test.\nThe daily use of bike share looks almost the same between Q4 2024 and Q1 2025. Both of them have the same morning and evening rush hour peaks on weekdays and then more in the midday on the weekends. What is different is the overall seasonal trend. Q4 starts strong with 5,000 trips per day in October and steadily drops to below 1,000 in late December. Q1 is the opposite, where it starts around 1,500 trips in January and reaches 3,500 trips by the end of March, where the weather is much warmer. This matters in the prediction because Q4’s model is trained on more data and in the fall days. It also helps explain why Q4 has slightly better MAE: the consistent high-ridership days in October give the lag variables a stronger, cleaner signal to learn from.\nBased on the MAE result, it is clear that adding temporal lags is the most important factor, as it reduced the MAE from 0.54 to 0.4, representing a 26% improvement. The baseline Time+Weather is also important because the baseline model performs the second-best among all the models. Hour-of-day and day-of-week dummies establish the fundamental rhythm of commuting. Factors like Demographics, Station FE, and Rush Hour Interaction are not particularly helpful because they all increased the MAE in our test, indicating that they overfit the training data."
  },
  {
    "objectID": "labs/Tim_Ye_Assignment5.html#part-1-replicate-with-quarter-4-2024",
    "href": "labs/Tim_Ye_Assignment5.html#part-1-replicate-with-quarter-4-2024",
    "title": "Space-Time Prediction of Bike Share Demand: Philadelphia Indego",
    "section": "",
    "text": "Code\n# Core tidyverse\nlibrary(tidyverse)\nlibrary(lubridate)\n\n# Spatial data\nlibrary(sf)\nlibrary(tigris)\n\n# Census data\nlibrary(tidycensus)\n\n# Weather data\nlibrary(riem)  # For Philadelphia weather from ASOS stations\n\n# Visualization\nlibrary(viridis)\nlibrary(gridExtra)\nlibrary(knitr)\nlibrary(kableExtra)\n\n# here!\nlibrary(here)\n\n#Knearest distance\nlibrary(FNN)\n# Get rid of scientific notation. We gotta look good!\noptions(scipen = 999)\nSys.setlocale(\"LC_TIME\", \"English_United States\")\n\n\n[1] \"\"\n\n\n\n\nCode\nplotTheme &lt;- theme(\n  plot.title = element_text(size = 14, face = \"bold\"),\n  plot.subtitle = element_text(size = 10),\n  plot.caption = element_text(size = 8),\n  axis.text.x = element_text(size = 10, angle = 45, hjust = 1),\n  axis.text.y = element_text(size = 10),\n  axis.title = element_text(size = 11, face = \"bold\"),\n  panel.background = element_blank(),\n  panel.grid.major = element_line(colour = \"#D0D0D0\", size = 0.2),\n  panel.grid.minor = element_blank(),\n  axis.ticks = element_blank(),\n  legend.position = \"right\"\n)\n\nmapTheme &lt;- theme(\n  plot.title = element_text(size = 14, face = \"bold\"),\n  plot.subtitle = element_text(size = 10),\n  plot.caption = element_text(size = 8),\n  axis.line = element_blank(),\n  axis.text = element_blank(),\n  axis.ticks = element_blank(),\n  axis.title = element_blank(),\n  panel.background = element_blank(),\n  panel.border = element_blank(),\n  panel.grid.major = element_line(colour = 'transparent'),\n  panel.grid.minor = element_blank(),\n  legend.position = \"right\",\n  plot.margin = margin(1, 1, 1, 1, 'cm'),\n  legend.key.height = unit(1, \"cm\"),\n  legend.key.width = unit(0.2, \"cm\")\n)\n\npalette5 &lt;- c(\"#eff3ff\", \"#bdd7e7\", \"#6baed6\", \"#3182bd\", \"#08519c\")\n\n\n\n\nCode\n# Read Q4 2024 data\nindego &lt;- read_csv(here(\"data/indego-trips-2024-q4.csv\"))\n\n# Quick look at the data\nglimpse(indego)\n\n\nRows: 299,121\nColumns: 15\n$ trip_id             &lt;dbl&gt; 1050296434, 1050293063, 1050296229, 1050276817, 10…\n$ duration            &lt;dbl&gt; 22, 8, 12, 1, 5, 9, 5, 6, 16, 16, 16, 39, 6, 40, 1…\n$ start_time          &lt;chr&gt; \"10/1/2024 0:00\", \"10/1/2024 0:06\", \"10/1/2024 0:0…\n$ end_time            &lt;chr&gt; \"10/1/2024 0:22\", \"10/1/2024 0:14\", \"10/1/2024 0:1…\n$ start_station       &lt;dbl&gt; 3322, 3166, 3007, 3166, 3075, 3166, 3030, 3010, 33…\n$ start_lat           &lt;dbl&gt; 39.93638, 39.97195, 39.94517, 39.97195, 39.96718, …\n$ start_lon           &lt;dbl&gt; -75.15526, -75.13445, -75.15993, -75.13445, -75.16…\n$ end_station         &lt;dbl&gt; 3375, 3017, 3244, 3166, 3102, 3008, 3203, 3163, 33…\n$ end_lat             &lt;dbl&gt; 39.96036, 39.98003, 39.93865, 39.97195, 39.96759, …\n$ end_lon             &lt;dbl&gt; -75.14020, -75.14371, -75.16674, -75.13445, -75.17…\n$ bike_id             &lt;chr&gt; \"03580\", \"05386\", \"18082\", \"02729\", \"18519\", \"0272…\n$ plan_duration       &lt;dbl&gt; 365, 365, 365, 1, 30, 1, 365, 365, 30, 30, 30, 30,…\n$ trip_route_category &lt;chr&gt; \"One Way\", \"One Way\", \"One Way\", \"Round Trip\", \"On…\n$ passholder_type     &lt;chr&gt; \"Indego365\", \"Indego365\", \"Indego365\", \"Walk-up\", …\n$ bike_type           &lt;chr&gt; \"standard\", \"standard\", \"electric\", \"standard\", \"e…\n\n\n\n\nCode\nindego &lt;- indego %&gt;%\n  mutate(\n    # Parse datetime\n    start_datetime = mdy_hm(start_time),\n    end_datetime = mdy_hm(end_time),\n    \n    # Create hourly bins\n    interval60 = floor_date(start_datetime, unit = \"hour\"),\n    \n    # Extract time features\n    week = week(interval60),\n    month = month(interval60, label = TRUE),\n    dotw = wday(interval60, label = TRUE),\n    hour = hour(interval60),\n    date = as.Date(interval60),\n    \n    # Create useful indicators\n    weekend = ifelse(dotw %in% c(\"Sat\", \"Sun\"), 1, 0),\n    rush_hour = ifelse(hour %in% c(7, 8, 9, 16, 17, 18), 1, 0)\n  )\n\n# Look at temporal features\nhead(indego %&gt;% select(start_datetime, interval60, week, dotw, hour, weekend))\n\n\n# A tibble: 6 × 6\n  start_datetime      interval60           week dotw   hour weekend\n  &lt;dttm&gt;              &lt;dttm&gt;              &lt;dbl&gt; &lt;ord&gt; &lt;int&gt;   &lt;dbl&gt;\n1 2024-10-01 00:00:00 2024-10-01 00:00:00    40 Tue       0       0\n2 2024-10-01 00:06:00 2024-10-01 00:00:00    40 Tue       0       0\n3 2024-10-01 00:06:00 2024-10-01 00:00:00    40 Tue       0       0\n4 2024-10-01 00:06:00 2024-10-01 00:00:00    40 Tue       0       0\n5 2024-10-01 00:07:00 2024-10-01 00:00:00    40 Tue       0       0\n6 2024-10-01 00:08:00 2024-10-01 00:00:00    40 Tue       0       0\n\n\nGet Weather Data\n\n\nCode\n# Get weather from Philadelphia International Airport (KPHL)\n# This covers Q4 2024: October 1 - December 31\nweather_data &lt;- riem_measures(\n  station = \"PHL\",  # Philadelphia International Airport\n  date_start = \"2024-10-01\",\n  date_end = \"2024-12-31\"\n)\n\n# Process weather data\nweather_processed &lt;- weather_data %&gt;%\n  mutate(\n    interval60 = floor_date(valid, unit = \"hour\"),\n    Temperature = tmpf,  # Temperature in Fahrenheit\n    Precipitation = ifelse(is.na(p01i), 0, p01i),  # Hourly precip in inches\n    Wind_Speed = sknt  # Wind speed in knots\n  ) %&gt;%\n  select(interval60, Temperature, Precipitation, Wind_Speed) %&gt;%\n  distinct()\n\n# Check for missing hours and interpolate if needed\nweather_complete &lt;- weather_processed %&gt;%\n  complete(interval60 = seq(min(interval60), max(interval60), by = \"hour\")) %&gt;%\n  fill(Temperature, Precipitation, Wind_Speed, .direction = \"down\")\n\n# Look at the weather\nsummary(weather_complete %&gt;% select(Temperature, Precipitation, Wind_Speed))\n\n\n  Temperature    Precipitation        Wind_Speed    \n Min.   :12.00   Min.   :0.000000   Min.   : 0.000  \n 1st Qu.:41.00   1st Qu.:0.000000   1st Qu.: 4.000  \n Median :51.00   Median :0.000000   Median : 7.000  \n Mean   :50.80   Mean   :0.004177   Mean   : 7.663  \n 3rd Qu.:60.95   3rd Qu.:0.000000   3rd Qu.:11.000  \n Max.   :83.00   Max.   :0.520000   Max.   :30.000  \n\n\nCensus data download\n\n\nCode\n# Get Philadelphia census tracts\nphilly_census &lt;- get_acs(\n  geography = \"tract\",\n  variables = c(\n    \"B01003_001\",  # Total population\n    \"B19013_001\",  # Median household income\n    \"B08301_001\",  # Total commuters\n    \"B08301_010\",  # Commute by transit\n    \"B02001_002\",  # White alone\n    \"B25077_001\"   # Median home value\n  ),\n  state = \"PA\",\n  county = \"Philadelphia\",\n  year = 2022,\n  geometry = TRUE,\n  output = \"wide\"\n) %&gt;%\n  rename(\n    Total_Pop = B01003_001E,\n    Med_Inc = B19013_001E,\n    Total_Commuters = B08301_001E,\n    Transit_Commuters = B08301_010E,\n    White_Pop = B02001_002E,\n    Med_Home_Value = B25077_001E\n  ) %&gt;%\n  mutate(\n    Percent_Taking_Transit = (Transit_Commuters / Total_Commuters) * 100,\n    Percent_White = (White_Pop / Total_Pop) * 100\n  ) %&gt;%\n  st_transform(crs = 4326)  # WGS84 for lat/lon matching\n\n\n\n  |                                                                            \n  |                                                                      |   0%\n  |                                                                            \n  |=                                                                     |   1%\n  |                                                                            \n  |=                                                                     |   2%\n  |                                                                            \n  |==                                                                    |   2%\n  |                                                                            \n  |==                                                                    |   3%\n  |                                                                            \n  |===                                                                   |   4%\n  |                                                                            \n  |===                                                                   |   5%\n  |                                                                            \n  |====                                                                  |   6%\n  |                                                                            \n  |=====                                                                 |   6%\n  |                                                                            \n  |=====                                                                 |   7%\n  |                                                                            \n  |======                                                                |   8%\n  |                                                                            \n  |======                                                                |   9%\n  |                                                                            \n  |=======                                                               |   9%\n  |                                                                            \n  |=======                                                               |  11%\n  |                                                                            \n  |========                                                              |  12%\n  |                                                                            \n  |=========                                                             |  12%\n  |                                                                            \n  |=========                                                             |  13%\n  |                                                                            \n  |==========                                                            |  14%\n  |                                                                            \n  |===========                                                           |  15%\n  |                                                                            \n  |===========                                                           |  16%\n  |                                                                            \n  |============                                                          |  17%\n  |                                                                            \n  |=============                                                         |  18%\n  |                                                                            \n  |=============                                                         |  19%\n  |                                                                            \n  |==============                                                        |  20%\n  |                                                                            \n  |===============                                                       |  21%\n  |                                                                            \n  |===============                                                       |  22%\n  |                                                                            \n  |================                                                      |  23%\n  |                                                                            \n  |=================                                                     |  24%\n  |                                                                            \n  |==================                                                    |  25%\n  |                                                                            \n  |==================                                                    |  26%\n  |                                                                            \n  |===================                                                   |  27%\n  |                                                                            \n  |====================                                                  |  28%\n  |                                                                            \n  |====================                                                  |  29%\n  |                                                                            \n  |=====================                                                 |  30%\n  |                                                                            \n  |======================                                                |  31%\n  |                                                                            \n  |======================                                                |  32%\n  |                                                                            \n  |=======================                                               |  33%\n  |                                                                            \n  |========================                                              |  34%\n  |                                                                            \n  |========================                                              |  35%\n  |                                                                            \n  |=========================                                             |  36%\n  |                                                                            \n  |==========================                                            |  37%\n  |                                                                            \n  |==========================                                            |  38%\n  |                                                                            \n  |===========================                                           |  39%\n  |                                                                            \n  |============================                                          |  40%\n  |                                                                            \n  |=============================                                         |  41%\n  |                                                                            \n  |=============================                                         |  42%\n  |                                                                            \n  |==============================                                        |  43%\n  |                                                                            \n  |===============================                                       |  44%\n  |                                                                            \n  |===============================                                       |  45%\n  |                                                                            \n  |================================                                      |  46%\n  |                                                                            \n  |=================================                                     |  47%\n  |                                                                            \n  |=================================                                     |  48%\n  |                                                                            \n  |==================================                                    |  49%\n  |                                                                            \n  |===================================                                   |  50%\n  |                                                                            \n  |===================================                                   |  51%\n  |                                                                            \n  |====================================                                  |  52%\n  |                                                                            \n  |=====================================                                 |  53%\n  |                                                                            \n  |=====================================                                 |  54%\n  |                                                                            \n  |======================================                                |  54%\n  |                                                                            \n  |=======================================                               |  55%\n  |                                                                            \n  |========================================                              |  56%\n  |                                                                            \n  |========================================                              |  57%\n  |                                                                            \n  |=========================================                             |  58%\n  |                                                                            \n  |==========================================                            |  59%\n  |                                                                            \n  |==========================================                            |  60%\n  |                                                                            \n  |===========================================                           |  61%\n  |                                                                            \n  |============================================                          |  62%\n  |                                                                            \n  |============================================                          |  63%\n  |                                                                            \n  |=============================================                         |  64%\n  |                                                                            \n  |==============================================                        |  65%\n  |                                                                            \n  |==============================================                        |  66%\n  |                                                                            \n  |===============================================                       |  67%\n  |                                                                            \n  |================================================                      |  68%\n  |                                                                            \n  |================================================                      |  69%\n  |                                                                            \n  |=================================================                     |  70%\n  |                                                                            \n  |==================================================                    |  71%\n  |                                                                            \n  |===================================================                   |  72%\n  |                                                                            \n  |===================================================                   |  73%\n  |                                                                            \n  |====================================================                  |  74%\n  |                                                                            \n  |=====================================================                 |  75%\n  |                                                                            \n  |=====================================================                 |  76%\n  |                                                                            \n  |======================================================                |  77%\n  |                                                                            \n  |=======================================================               |  78%\n  |                                                                            \n  |=======================================================               |  79%\n  |                                                                            \n  |========================================================              |  80%\n  |                                                                            \n  |=========================================================             |  81%\n  |                                                                            \n  |=========================================================             |  82%\n  |                                                                            \n  |==========================================================            |  83%\n  |                                                                            \n  |===========================================================           |  84%\n  |                                                                            \n  |===========================================================           |  85%\n  |                                                                            \n  |============================================================          |  86%\n  |                                                                            \n  |=============================================================         |  87%\n  |                                                                            \n  |=============================================================         |  88%\n  |                                                                            \n  |==============================================================        |  89%\n  |                                                                            \n  |===============================================================       |  90%\n  |                                                                            \n  |================================================================      |  91%\n  |                                                                            \n  |================================================================      |  92%\n  |                                                                            \n  |=================================================================     |  93%\n  |                                                                            \n  |==================================================================    |  94%\n  |                                                                            \n  |==================================================================    |  95%\n  |                                                                            \n  |===================================================================   |  96%\n  |                                                                            \n  |====================================================================  |  97%\n  |                                                                            \n  |====================================================================  |  98%\n  |                                                                            \n  |===================================================================== |  99%\n  |                                                                            \n  |======================================================================| 100%\n\n\nCode\n# Check the data\nglimpse(philly_census)\n\n\nRows: 408\nColumns: 17\n$ GEOID                  &lt;chr&gt; \"42101001500\", \"42101001800\", \"42101002802\", \"4…\n$ NAME                   &lt;chr&gt; \"Census Tract 15; Philadelphia County; Pennsylv…\n$ Total_Pop              &lt;dbl&gt; 3251, 3300, 5720, 4029, 4415, 1815, 3374, 2729,…\n$ B01003_001M            &lt;dbl&gt; 677, 369, 796, 437, 853, 210, 480, 734, 763, 11…\n$ Med_Inc                &lt;dbl&gt; 110859, 114063, 78871, 61583, 32347, 48581, 597…\n$ B19013_001M            &lt;dbl&gt; 24975, 30714, 20396, 22293, 4840, 13812, 6278, …\n$ Total_Commuters        &lt;dbl&gt; 2073, 2255, 3032, 2326, 1980, 969, 2427, 708, 2…\n$ B08301_001M            &lt;dbl&gt; 387, 308, 478, 383, 456, 189, 380, 281, 456, 68…\n$ Transit_Commuters      &lt;dbl&gt; 429, 123, 685, 506, 534, 192, 658, 218, 438, 51…\n$ B08301_010M            &lt;dbl&gt; 188, 66, 219, 144, 285, 71, 278, 184, 176, 235,…\n$ White_Pop              &lt;dbl&gt; 2185, 2494, 3691, 3223, 182, 984, 2111, 231, 35…\n$ B02001_002M            &lt;dbl&gt; 268, 381, 592, 380, 88, 190, 463, 112, 238, 778…\n$ Med_Home_Value         &lt;dbl&gt; 568300, 605000, 350600, 296400, 76600, 289700, …\n$ B25077_001M            &lt;dbl&gt; 58894, 34876, 12572, 22333, 10843, 118720, 1506…\n$ geometry               &lt;MULTIPOLYGON [°]&gt; MULTIPOLYGON (((-75.16558 3..., MU…\n$ Percent_Taking_Transit &lt;dbl&gt; 20.694645, 5.454545, 22.592348, 21.754084, 26.9…\n$ Percent_White          &lt;dbl&gt; 67.2100892, 75.5757576, 64.5279720, 79.9950360,…\n\n\nData Structure Check\n\n\nCode\n# How many trips?\ncat(\"Total trips in Q4 2024:\", nrow(indego), \"\\n\")\n\n\nTotal trips in Q4 2024: 299121 \n\n\nCode\n# Date range\ncat(\"Date range:\", \n    min(mdy_hm(indego$start_time)), \"to\", \n    max(mdy_hm(indego$start_time)), \"\\n\")\n\n\nDate range: 1727740800 to 1735689360 \n\n\nCode\n# How many unique stations?\ncat(\"Unique start stations:\", length(unique(indego$start_station)), \"\\n\")\n\n\nUnique start stations: 256 \n\n\nCode\n# Trip types\ntable(indego$trip_route_category)\n\n\n\n   One Way Round Trip \n    282675      16446 \n\n\nCode\n# Passholder types\ntable(indego$passholder_type)\n\n\n\n  Day Pass   Indego30  Indego365 IndegoFlex       NULL    Walk-up \n     10711     159895     112690          1       1165      14659 \n\n\nCode\n# Bike types\ntable(indego$bike_type)\n\n\n\nelectric standard \n  175503   123618 \n\n\nCreate Space-Time Panel\n\n\nCode\n# Create sf object for stations\nstations_sf &lt;- indego %&gt;%\n  distinct(start_station, start_lat, start_lon) %&gt;%\n  filter(!is.na(start_lat), !is.na(start_lon)) %&gt;%\n  st_as_sf(coords = c(\"start_lon\", \"start_lat\"), crs = 4326)\n\n# Spatial join to get census tract for each station\nstations_census &lt;- st_join(stations_sf, philly_census, left = TRUE) %&gt;%\n  st_drop_geometry()\n\n\nstations_for_map &lt;- indego %&gt;%\n  distinct(start_station, start_lat, start_lon) %&gt;%\n  filter(!is.na(start_lat), !is.na(start_lon)) %&gt;%\n  left_join(\n    stations_census %&gt;% select(start_station, Med_Inc),\n    by = \"start_station\"\n  ) %&gt;%\n  mutate(has_census = !is.na(Med_Inc))\n\n# Add back to trip data\nindego_census &lt;- indego %&gt;%\n  left_join(\n    stations_census %&gt;% \n      select(start_station, Med_Inc, Percent_Taking_Transit, \n             Percent_White, Total_Pop),\n    by = \"start_station\"\n  )\n# Identify which stations to keep\nvalid_stations &lt;- stations_census %&gt;%\n  filter(!is.na(Med_Inc)) %&gt;%\n  pull(start_station)\n\n# Filter trip data to valid stations only\nindego_census &lt;- indego %&gt;%\n  filter(start_station %in% valid_stations) %&gt;%\n  left_join(\n    stations_census %&gt;% \n      select(start_station, Med_Inc, Percent_Taking_Transit, \n             Percent_White, Total_Pop),\n    by = \"start_station\"\n  )\n\n\n\n\nCode\n# Count trips by station-hour\ntrips_panel &lt;- indego_census %&gt;%\n  group_by(interval60, start_station, start_lat, start_lon,\n           Med_Inc, Percent_Taking_Transit, Percent_White, Total_Pop) %&gt;%\n  summarize(Trip_Count = n()) %&gt;%\n  ungroup()\n\n# Calculate expected panel size\nn_stations &lt;- length(unique(trips_panel$start_station))\nn_hours &lt;- length(unique(trips_panel$interval60))\nexpected_rows &lt;- n_stations * n_hours\n\ncat(\"Expected panel rows:\", format(expected_rows, big.mark = \",\"), \"\\n\")\n\n\nExpected panel rows: 523,296 \n\n\nCode\ncat(\"Current rows:\", format(nrow(trips_panel), big.mark = \",\"), \"\\n\")\n\n\nCurrent rows: 150,972 \n\n\nCode\ncat(\"Missing rows:\", format(expected_rows - nrow(trips_panel), big.mark = \",\"), \"\\n\")\n\n\nMissing rows: 372,324 \n\n\nCode\n# Create complete panel\nstudy_panel &lt;- expand.grid(\n  interval60 = unique(trips_panel$interval60),\n  start_station = unique(trips_panel$start_station)\n) %&gt;%\n  # Join trip counts\n  left_join(trips_panel, by = c(\"interval60\", \"start_station\")) %&gt;%\n  # Replace NA trip counts with 0\n  mutate(Trip_Count = replace_na(Trip_Count, 0))\n\n# Fill in station attributes (they're the same for all hours)\nstation_attributes &lt;- trips_panel %&gt;%\n  group_by(start_station) %&gt;%\n  summarize(\n    start_lat = first(start_lat),\n    start_lon = first(start_lon),\n    Med_Inc = first(Med_Inc),\n    Percent_Taking_Transit = first(Percent_Taking_Transit),\n    Percent_White = first(Percent_White),\n    Total_Pop = first(Total_Pop)\n  )\n\nstudy_panel &lt;- study_panel %&gt;%\n  left_join(station_attributes, by = \"start_station\")\n\n\n\n\nCode\nstudy_panel &lt;- study_panel %&gt;%\n  mutate(\n    week = week(interval60),\n    month = month(interval60, label = TRUE),\n    dotw = wday(interval60, label = TRUE),\n    hour = hour(interval60),\n    date = as.Date(interval60),\n    weekend = ifelse(dotw %in% c(\"Sat\", \"Sun\"), 1, 0),\n    rush_hour = ifelse(hour %in% c(7, 8, 9, 16, 17, 18), 1, 0)\n  )\nstudy_panel &lt;- study_panel %&gt;%\n  left_join(weather_complete, by = \"interval60\")\n\n\nCreate Temporal Lag\n\n\nCode\n# Sort by station and time\nstudy_panel &lt;- study_panel %&gt;%\n  arrange(start_station, interval60)\n\n# Create lag variables WITHIN each station\nstudy_panel &lt;- study_panel %&gt;%\n  group_by(start_station) %&gt;%\n  mutate(\n    lag1Hour = lag(Trip_Count, 1),\n    lag2Hours = lag(Trip_Count, 2),\n    lag3Hours = lag(Trip_Count, 3),\n    lag12Hours = lag(Trip_Count, 12),\n    lag1day = lag(Trip_Count, 24)\n  ) %&gt;%\n  ungroup()\n\n# Remove rows with NA lags (first 24 hours for each station)\nstudy_panel_complete &lt;- study_panel %&gt;%\n  filter(!is.na(lag1day))\n\n\nTemporal Train/Test Split\n\n\nCode\n# Split by week\n\n# Which stations have trips in BOTH early and late periods?\nearly_stations &lt;- study_panel_complete %&gt;%\n  filter(week &lt; 50) %&gt;%\n  filter(Trip_Count &gt; 0) %&gt;%\n  distinct(start_station) %&gt;%\n  pull(start_station)\n\nlate_stations &lt;- study_panel_complete %&gt;%\n  filter(week &gt;= 50) %&gt;%\n  filter(Trip_Count &gt; 0) %&gt;%\n  distinct(start_station) %&gt;%\n  pull(start_station)\n\n# Keep only stations that appear in BOTH periods\ncommon_stations &lt;- intersect(early_stations, late_stations)\n\n\n# Filter panel to only common stations\nstudy_panel_complete &lt;- study_panel_complete %&gt;%\n  filter(start_station %in% common_stations)\n\n# NOW create train/test split\ntrain &lt;- study_panel_complete %&gt;%\n  filter(week &lt; 50)\n\ntest &lt;- study_panel_complete %&gt;%\n  filter(week &gt;= 50)\n\ncat(\"Training observations:\", format(nrow(train), big.mark = \",\"), \"\\n\")\n\n\nTraining observations: 410,628 \n\n\nCode\ncat(\"Testing observations:\", format(nrow(test), big.mark = \",\"), \"\\n\")\n\n\nTesting observations: 171,684 \n\n\nCode\ncat(\"Training date range:\", min(train$date), \"to\", max(train$date), \"\\n\")\n\n\nTraining date range: 19997 to 20065 \n\n\nCode\ncat(\"Testing date range:\", min(test$date), \"to\", max(test$date), \"\\n\")\n\n\nTesting date range: 20066 to 20088 \n\n\nHourly Patterns\n\n\nCode\n# Average trips by hour and day type\nhourly_patterns &lt;- indego %&gt;%\n  group_by(hour, weekend) %&gt;%\n  summarize(avg_trips = n() / n_distinct(date)) %&gt;%\n  mutate(day_type = ifelse(weekend == 1, \"Weekend\", \"Weekday\"))\n\nggplot(hourly_patterns, aes(x = hour, y = avg_trips, color = day_type)) +\n  geom_line(linewidth = 1.2) +\n  scale_color_manual(values = c(\"Weekday\" = \"#08519c\", \"Weekend\" = \"#6baed6\")) +\n  labs(\n    title = \"Average Hourly Ridership Patterns - Q4 2024\",\n    subtitle = \"Clear commute patterns on weekdays\",\n    x = \"Hour of Day\",\n    y = \"Average Trips per Hour\",\n    color = \"Day Type\"\n  ) +\n  plotTheme\n\n\n\n\n\n\n\n\n\nTrips over time\n\n\nCode\n# Daily trip counts\ndaily_trips &lt;- indego %&gt;%\n  group_by(date) %&gt;%\n  summarize(trips = n())\n\nggplot(daily_trips, aes(x = date, y = trips)) +\n  geom_line(color = \"#3182bd\", linewidth = 1) +\n  geom_smooth(se = FALSE, color = \"red\", linetype = \"dashed\") +\n  labs(\n    title = \"Indego Daily Ridership - Q4 2024\",\n    subtitle = \"Winter demand patterns in Philadelphia\",\n    x = \"Date\",\n    y = \"Daily Trips\",\n    caption = \"Source: Indego bike share\"\n  ) +\n  plotTheme\n\n\n\n\n\n\n\n\n\nModel 1\n\n\nCode\n# Create day of week factor with treatment (dummy) coding\n\ntrain &lt;- train %&gt;%\n  mutate(dotw_simple = factor(dotw, levels = c(\"Mon\", \"Tue\", \"Wed\", \"Thu\", \"Fri\", \"Sat\", \"Sun\")))\n\n# Set contrasts to treatment coding (dummy variables)\ncontrasts(train$dotw_simple) &lt;- contr.treatment(7)\n\n# Now run the model\nmodel1 &lt;- lm(\n  Trip_Count ~ as.factor(hour) + dotw_simple + Temperature + Precipitation,\n  data = train\n)\nsummary(model1)\n\n\n\nCall:\nlm(formula = Trip_Count ~ as.factor(hour) + dotw_simple + Temperature + \n    Precipitation, data = train)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-1.7089 -0.6783 -0.2159  0.1882 27.2727 \n\nCoefficients:\n                   Estimate Std. Error t value             Pr(&gt;|t|)    \n(Intercept)       -0.489156   0.013731 -35.624 &lt; 0.0000000000000002 ***\nas.factor(hour)1  -0.055267   0.012906  -4.282   0.0000184994686084 ***\nas.factor(hour)2  -0.078435   0.012690  -6.181   0.0000000006378151 ***\nas.factor(hour)3  -0.110234   0.012655  -8.711 &lt; 0.0000000000000002 ***\nas.factor(hour)4  -0.074204   0.012566  -5.905   0.0000000035263476 ***\nas.factor(hour)5   0.038930   0.012623   3.084             0.002042 ** \nas.factor(hour)6   0.296063   0.012639  23.424 &lt; 0.0000000000000002 ***\nas.factor(hour)7   0.562184   0.012853  43.738 &lt; 0.0000000000000002 ***\nas.factor(hour)8   0.940524   0.012603  74.629 &lt; 0.0000000000000002 ***\nas.factor(hour)9   0.681527   0.012691  53.700 &lt; 0.0000000000000002 ***\nas.factor(hour)10  0.562441   0.012526  44.900 &lt; 0.0000000000000002 ***\nas.factor(hour)11  0.602993   0.012538  48.094 &lt; 0.0000000000000002 ***\nas.factor(hour)12  0.698148   0.012435  56.145 &lt; 0.0000000000000002 ***\nas.factor(hour)13  0.667308   0.012338  54.085 &lt; 0.0000000000000002 ***\nas.factor(hour)14  0.685610   0.012386  55.355 &lt; 0.0000000000000002 ***\nas.factor(hour)15  0.791576   0.012780  61.940 &lt; 0.0000000000000002 ***\nas.factor(hour)16  0.906925   0.012549  72.273 &lt; 0.0000000000000002 ***\nas.factor(hour)17  1.110205   0.012639  87.842 &lt; 0.0000000000000002 ***\nas.factor(hour)18  0.831297   0.012728  65.314 &lt; 0.0000000000000002 ***\nas.factor(hour)19  0.541516   0.012768  42.412 &lt; 0.0000000000000002 ***\nas.factor(hour)20  0.341348   0.012858  26.547 &lt; 0.0000000000000002 ***\nas.factor(hour)21  0.233603   0.012881  18.135 &lt; 0.0000000000000002 ***\nas.factor(hour)22  0.175194   0.012820  13.666 &lt; 0.0000000000000002 ***\nas.factor(hour)23  0.076033   0.012906   5.891   0.0000000038337890 ***\ndotw_simple2       0.065588   0.006869   9.549 &lt; 0.0000000000000002 ***\ndotw_simple3       0.062920   0.006857   9.177 &lt; 0.0000000000000002 ***\ndotw_simple4      -0.050061   0.006571  -7.619   0.0000000000000257 ***\ndotw_simple5      -0.012223   0.006833  -1.789             0.073612 .  \ndotw_simple6      -0.024444   0.006806  -3.591             0.000329 ***\ndotw_simple7      -0.050983   0.006924  -7.363   0.0000000000001797 ***\nTemperature        0.012467   0.000161  77.432 &lt; 0.0000000000000002 ***\nPrecipitation     -1.109233   0.081793 -13.562 &lt; 0.0000000000000002 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.144 on 410596 degrees of freedom\nMultiple R-squared:  0.1141,    Adjusted R-squared:  0.114 \nF-statistic:  1705 on 31 and 410596 DF,  p-value: &lt; 0.00000000000000022\n\n\nModel 2\n\n\nCode\nmodel2 &lt;- lm(\n  Trip_Count ~ as.factor(hour) + dotw_simple + Temperature + Precipitation +\n    lag1Hour + lag3Hours + lag1day,\n  data = train\n)\n\nsummary(model2)\n\n\n\nCall:\nlm(formula = Trip_Count ~ as.factor(hour) + dotw_simple + Temperature + \n    Precipitation + lag1Hour + lag3Hours + lag1day, data = train)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-12.8428  -0.4697  -0.1265   0.1225  25.5313 \n\nCoefficients:\n                    Estimate Std. Error t value             Pr(&gt;|t|)    \n(Intercept)       -0.1971724  0.0120171 -16.408 &lt; 0.0000000000000002 ***\nas.factor(hour)1  -0.0175731  0.0112639  -1.560              0.11873    \nas.factor(hour)2  -0.0093400  0.0110785  -0.843              0.39918    \nas.factor(hour)3  -0.0323163  0.0110523  -2.924              0.00346 ** \nas.factor(hour)4  -0.0064036  0.0109793  -0.583              0.55973    \nas.factor(hour)5   0.0625618  0.0110386   5.668  0.00000001449553616 ***\nas.factor(hour)6   0.2581886  0.0110639  23.336 &lt; 0.0000000000000002 ***\nas.factor(hour)7   0.4087297  0.0112659  36.280 &lt; 0.0000000000000002 ***\nas.factor(hour)8   0.6475387  0.0110725  58.482 &lt; 0.0000000000000002 ***\nas.factor(hour)9   0.2687374  0.0111533  24.095 &lt; 0.0000000000000002 ***\nas.factor(hour)10  0.2122365  0.0109778  19.333 &lt; 0.0000000000000002 ***\nas.factor(hour)11  0.2554385  0.0109944  23.234 &lt; 0.0000000000000002 ***\nas.factor(hour)12  0.3583675  0.0108988  32.881 &lt; 0.0000000000000002 ***\nas.factor(hour)13  0.3239304  0.0108163  29.948 &lt; 0.0000000000000002 ***\nas.factor(hour)14  0.3531805  0.0108536  32.540 &lt; 0.0000000000000002 ***\nas.factor(hour)15  0.4322898  0.0112024  38.589 &lt; 0.0000000000000002 ***\nas.factor(hour)16  0.5127136  0.0110123  46.558 &lt; 0.0000000000000002 ***\nas.factor(hour)17  0.6404994  0.0111130  57.635 &lt; 0.0000000000000002 ***\nas.factor(hour)18  0.3266069  0.0112005  29.160 &lt; 0.0000000000000002 ***\nas.factor(hour)19  0.1612724  0.0111995  14.400 &lt; 0.0000000000000002 ***\nas.factor(hour)20  0.0449273  0.0112804   3.983  0.00006812693842296 ***\nas.factor(hour)21  0.0488508  0.0112677   4.335  0.00001454699054146 ***\nas.factor(hour)22  0.0606778  0.0111954   5.420  0.00000005967221309 ***\nas.factor(hour)23  0.0183651  0.0112646   1.630              0.10303    \ndotw_simple2       0.0070021  0.0059975   1.167              0.24301    \ndotw_simple3      -0.0128484  0.0059925  -2.144              0.03203 *  \ndotw_simple4      -0.0448243  0.0057357  -7.815  0.00000000000000551 ***\ndotw_simple5      -0.0422563  0.0059684  -7.080  0.00000000000144379 ***\ndotw_simple6      -0.0367596  0.0059417  -6.187  0.00000000061504630 ***\ndotw_simple7      -0.0593011  0.0060464  -9.808 &lt; 0.0000000000000002 ***\nTemperature        0.0040641  0.0001426  28.494 &lt; 0.0000000000000002 ***\nPrecipitation     -0.9676346  0.0714492 -13.543 &lt; 0.0000000000000002 ***\nlag1Hour           0.3238000  0.0014825 218.412 &lt; 0.0000000000000002 ***\nlag3Hours          0.1180469  0.0014504  81.387 &lt; 0.0000000000000002 ***\nlag1day            0.2027487  0.0013895 145.912 &lt; 0.0000000000000002 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.9988 on 410593 degrees of freedom\nMultiple R-squared:  0.3252,    Adjusted R-squared:  0.3252 \nF-statistic:  5821 on 34 and 410593 DF,  p-value: &lt; 0.00000000000000022\n\n\nModel 3\n\n\nCode\nmodel3 &lt;- lm(\n  Trip_Count ~ as.factor(hour) + dotw_simple + Temperature + Precipitation +\n    lag1Hour + lag3Hours + lag1day +\n    Med_Inc.x + Percent_Taking_Transit.y + Percent_White.y,\n  data = train\n)\n\nsummary(model3)\n\n\n\nCall:\nlm(formula = Trip_Count ~ as.factor(hour) + dotw_simple + Temperature + \n    Precipitation + lag1Hour + lag3Hours + lag1day + Med_Inc.x + \n    Percent_Taking_Transit.y + Percent_White.y, data = train)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-10.2603  -0.7198  -0.2719   0.4456  24.6278 \n\nCoefficients:\n                              Estimate    Std. Error t value\n(Intercept)               0.7009115282  0.0386608126  18.130\nas.factor(hour)1         -0.0339064631  0.0443755122  -0.764\nas.factor(hour)2         -0.0733390073  0.0472207425  -1.553\nas.factor(hour)3         -0.1736198797  0.0563179188  -3.083\nas.factor(hour)4         -0.1602016516  0.0526556216  -3.042\nas.factor(hour)5         -0.0842198175  0.0392042217  -2.148\nas.factor(hour)6          0.1812725910  0.0339389718   5.341\nas.factor(hour)7          0.3659932453  0.0327798040  11.165\nas.factor(hour)8          0.6419351722  0.0315874296  20.322\nas.factor(hour)9          0.0689540443  0.0318174207   2.167\nas.factor(hour)10         0.0455849373  0.0318601406   1.431\nas.factor(hour)11         0.0951277607  0.0317831322   2.993\nas.factor(hour)12         0.1718233329  0.0312986651   5.490\nas.factor(hour)13         0.1501299648  0.0313008986   4.796\nas.factor(hour)14         0.1546199385  0.0311814820   4.959\nas.factor(hour)15         0.2738677333  0.0314129060   8.718\nas.factor(hour)16         0.3825663042  0.0310863390  12.307\nas.factor(hour)17         0.5958195674  0.0311303049  19.140\nas.factor(hour)18         0.1871034280  0.0314626011   5.947\nas.factor(hour)19         0.0105635560  0.0320040115   0.330\nas.factor(hour)20        -0.0963042374  0.0327876103  -2.937\nas.factor(hour)21        -0.0718694380  0.0336061149  -2.139\nas.factor(hour)22        -0.0356940800  0.0343008260  -1.041\nas.factor(hour)23        -0.0793554621  0.0360569574  -2.201\ndotw_simple2              0.0015399442  0.0129720488   0.119\ndotw_simple3             -0.0058302798  0.0131356919  -0.444\ndotw_simple4             -0.0584003672  0.0128820191  -4.533\ndotw_simple5             -0.0902616975  0.0132777688  -6.798\ndotw_simple6             -0.0296928615  0.0133240066  -2.229\ndotw_simple7             -0.0404284310  0.0137730955  -2.935\nTemperature               0.0060129110  0.0003255823  18.468\nPrecipitation            -2.6993476761  0.2537854011 -10.636\nlag1Hour                  0.2432506608  0.0023823455 102.106\nlag3Hours                 0.0728128784  0.0024806873  29.352\nlag1day                   0.1565602280  0.0022935086  68.262\nMed_Inc.x                -0.0000001894  0.0000001237  -1.531\nPercent_Taking_Transit.y -0.0038935558  0.0004528281  -8.598\nPercent_White.y           0.0037864819  0.0002282348  16.590\n                                     Pr(&gt;|t|)    \n(Intercept)              &lt; 0.0000000000000002 ***\nas.factor(hour)1                      0.44482    \nas.factor(hour)2                      0.12040    \nas.factor(hour)3                      0.00205 ** \nas.factor(hour)4                      0.00235 ** \nas.factor(hour)5                      0.03170 *  \nas.factor(hour)6              0.0000000925173 ***\nas.factor(hour)7         &lt; 0.0000000000000002 ***\nas.factor(hour)8         &lt; 0.0000000000000002 ***\nas.factor(hour)9                      0.03022 *  \nas.factor(hour)10                     0.15249    \nas.factor(hour)11                     0.00276 ** \nas.factor(hour)12             0.0000000403123 ***\nas.factor(hour)13             0.0000016175893 ***\nas.factor(hour)14             0.0000007104928 ***\nas.factor(hour)15        &lt; 0.0000000000000002 ***\nas.factor(hour)16        &lt; 0.0000000000000002 ***\nas.factor(hour)17        &lt; 0.0000000000000002 ***\nas.factor(hour)18             0.0000000027402 ***\nas.factor(hour)19                     0.74135    \nas.factor(hour)20                     0.00331 ** \nas.factor(hour)21                     0.03247 *  \nas.factor(hour)22                     0.29805    \nas.factor(hour)23                     0.02775 *  \ndotw_simple2                          0.90550    \ndotw_simple3                          0.65715    \ndotw_simple4                  0.0000058070122 ***\ndotw_simple5                  0.0000000000107 ***\ndotw_simple6                          0.02585 *  \ndotw_simple7                          0.00333 ** \nTemperature              &lt; 0.0000000000000002 ***\nPrecipitation            &lt; 0.0000000000000002 ***\nlag1Hour                 &lt; 0.0000000000000002 ***\nlag3Hours                &lt; 0.0000000000000002 ***\nlag1day                  &lt; 0.0000000000000002 ***\nMed_Inc.x                             0.12579    \nPercent_Taking_Transit.y &lt; 0.0000000000000002 ***\nPercent_White.y          &lt; 0.0000000000000002 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.274 on 133822 degrees of freedom\n  (276768 observations deleted due to missingness)\nMultiple R-squared:  0.2192,    Adjusted R-squared:  0.219 \nF-statistic:  1015 on 37 and 133822 DF,  p-value: &lt; 0.00000000000000022\n\n\nModel 4\n\n\nCode\nmodel4 &lt;- lm(\n  Trip_Count ~ as.factor(hour) + dotw_simple + Temperature + Precipitation +\n    lag1Hour + lag3Hours + lag1day +\n    Med_Inc.x + Percent_Taking_Transit.y + Percent_White.y +\n    as.factor(start_station),\n  data = train\n)\n\n# Summary too long with all station dummies, just show key metrics\ncat(\"Model 4 R-squared:\", summary(model4)$r.squared, \"\\n\")\n\n\nModel 4 R-squared: 0.2469249 \n\n\nCode\ncat(\"Model 4 Adj R-squared:\", summary(model4)$adj.r.squared, \"\\n\")\n\n\nModel 4 Adj R-squared: 0.2454536 \n\n\nModel 5\n\n\nCode\nmodel5 &lt;- lm(\n  Trip_Count ~ as.factor(hour) + dotw_simple + Temperature + Precipitation +\n    lag1Hour + lag3Hours + lag1day + rush_hour + as.factor(month) +\n    Med_Inc.x + Percent_Taking_Transit.y + Percent_White.y +\n    as.factor(start_station) +\n    rush_hour * weekend,  # Rush hour effects different on weekends\n  data = train\n)\n\ncat(\"Model 5 R-squared:\", summary(model5)$r.squared, \"\\n\")\n\n\nModel 5 R-squared: 0.2526638 \n\n\nCode\ncat(\"Model 5 Adj R-squared:\", summary(model5)$adj.r.squared, \"\\n\")\n\n\nModel 5 Adj R-squared: 0.251187 \n\n\nMAE Calculation\n\n\nCode\n# Create day of week factor with treatment (dummy) coding\ntest &lt;- test %&gt;%\n  mutate(dotw_simple = factor(dotw, levels = c(\"Mon\", \"Tue\", \"Wed\", \"Thu\", \"Fri\", \"Sat\", \"Sun\")))\n\n# Set contrasts to treatment coding (dummy variables)\ncontrasts(test$dotw_simple) &lt;- contr.treatment(7)\n\ntest &lt;- test %&gt;%\n  mutate(\n    pred1 = predict(model1, newdata = test),\n    pred2 = predict(model2, newdata = test),\n    pred3 = predict(model3, newdata = test),\n    pred4 = predict(model4, newdata = test),\n    pred5 = predict(model5, newdata = test)\n  )\n\n# Calculate MAE for each model\nmae_results &lt;- data.frame(\n  Model = c(\n    \"1. Time + Weather\",\n    \"2. + Temporal Lags\",\n    \"3. + Demographics\",\n    \"4. + Station FE\",\n    \"5. + Rush Hour Interaction\"\n  ),\n  MAE = c(\n    mean(abs(test$Trip_Count - test$pred1), na.rm = TRUE),\n    mean(abs(test$Trip_Count - test$pred2), na.rm = TRUE),\n    mean(abs(test$Trip_Count - test$pred3), na.rm = TRUE),\n    mean(abs(test$Trip_Count - test$pred4), na.rm = TRUE),\n    mean(abs(test$Trip_Count - test$pred5), na.rm = TRUE)\n  )\n)\n\nkable(mae_results, \n      digits = 2,\n      caption = \"Mean Absolute Error by Model (Test Set)\",\n      col.names = c(\"Model\", \"MAE (trips)\")) %&gt;%\n  kable_styling(bootstrap_options = c(\"striped\", \"hover\"))\n\n\n\nMean Absolute Error by Model (Test Set)\n\n\nModel\nMAE (trips)\n\n\n\n\n1. Time + Weather\n0.54\n\n\n2. + Temporal Lags\n0.40\n\n\n3. + Demographics\n0.64\n\n\n4. + Station FE\n0.66\n\n\n5. + Rush Hour Interaction\n0.65\n\n\n\n\n\nCompare results to Q1 2025:\nThe MAE values are lower for Q4 2024 for all of the models compared to Q1 2025, and the difference is between 0.06 and 0.10. Model 2 with Temporal Lags has the lowest MAE for both Q4 2024 and Q1 2025. Some of the reasons that could explain the Q1 2025 has a higher MAE include Q1 typically has more unpredictable ridership due to variable weather—snow days, temperature swings, and rain can make trips more unstable compared to Q4, when weather patterns tend to be more stable in the fall. There could be more snow days that heavily impact trips. Also, during the spring, there are New Year holiday travel and spring break, which make the commute pattern less regular and make the prediction harder. With fewer trips in Q1, relative prediction errors can appear larger. Q4 2024 has more than 299,121 observations, and Q1 2025 only has 201,588 observations. Both Q4 2024 and Q1 2025’s model 3-5 increase MAE after adding more factors. This could suggest that demographics, station fixed effects, and rush hour improve the in-sample fit but don’t generalize well to the test.\nThe daily use of bike share looks almost the same between Q4 2024 and Q1 2025. Both of them have the same morning and evening rush hour peaks on weekdays and then more in the midday on the weekends. What is different is the overall seasonal trend. Q4 starts strong with 5,000 trips per day in October and steadily drops to below 1,000 in late December. Q1 is the opposite, where it starts around 1,500 trips in January and reaches 3,500 trips by the end of March, where the weather is much warmer. This matters in the prediction because Q4’s model is trained on more data and in the fall days. It also helps explain why Q4 has slightly better MAE: the consistent high-ridership days in October give the lag variables a stronger, cleaner signal to learn from.\nBased on the MAE result, it is clear that adding temporal lags is the most important factor, as it reduced the MAE from 0.54 to 0.4, representing a 26% improvement. The baseline Time+Weather is also important because the baseline model performs the second-best among all the models. Hour-of-day and day-of-week dummies establish the fundamental rhythm of commuting. Factors like Demographics, Station FE, and Rush Hour Interaction are not particularly helpful because they all increased the MAE in our test, indicating that they overfit the training data."
  },
  {
    "objectID": "labs/Tim_Ye_Assignment5.html#part-2-error-analysis",
    "href": "labs/Tim_Ye_Assignment5.html#part-2-error-analysis",
    "title": "Space-Time Prediction of Bike Share Demand: Philadelphia Indego",
    "section": "Part 2: Error Analysis",
    "text": "Part 2: Error Analysis\nSpatial patterns:\nSince model 2 has the lowest error, we are going to analyze model 2’s error\n\n\nCode\ntest &lt;- test %&gt;%\n  mutate(\n    error = Trip_Count - pred2,\n    abs_error = abs(error),\n    time_of_day = case_when(\n      hour &lt; 7 ~ \"Overnight\",\n      hour &gt;= 7 & hour &lt; 10 ~ \"AM Rush\",\n      hour &gt;= 10 & hour &lt; 15 ~ \"Mid-Day\",\n      hour &gt;= 15 & hour &lt;= 18 ~ \"PM Rush\",\n      hour &gt; 18 ~ \"Evening\"\n    )\n  )\n\n# Calculate MAE by station\nstation_errors &lt;- test %&gt;%\n  group_by(start_station, start_lat.x, start_lon.y) %&gt;%\n  summarize(\n    MAE = mean(abs_error, na.rm = TRUE),\n    avg_demand = mean(Trip_Count, na.rm = TRUE),\n    .groups = \"drop\"\n  ) %&gt;%\n  filter(!is.na(start_lat.x), !is.na(start_lon.y))\n\n## Create Two Maps Side-by-Side with Proper Legends (sorry these maps are ugly)\n\n# Calculate station errors\nstation_errors &lt;- test %&gt;%\n  filter(!is.na(pred2)) %&gt;%\n  group_by(start_station, start_lat.x, start_lon.y) %&gt;%\n  summarize(\n    MAE = mean(abs(Trip_Count - pred2), na.rm = TRUE),\n    avg_demand = mean(Trip_Count, na.rm = TRUE),\n    .groups = \"drop\"\n  ) %&gt;%\n  filter(!is.na(start_lat.x), !is.na(start_lon.y))\n\n# Map 1: Prediction Errors\np1 &lt;- ggplot() +\n  geom_sf(data = philly_census, fill = \"grey95\", color = \"white\", size = 0.1) +\n  geom_point(\n    data = station_errors,\n    aes(x = start_lon.y, y = start_lat.x, color = MAE),\n    size = 3.5,\n    alpha = 0.7\n  ) +\n  scale_color_viridis(\n    option = \"plasma\",\n    name = \"MAE (trips)\",\n    direction = -1,\n    breaks = c(0.5, 1.0, 1.5),\n    labels = c(\"0.5\", \"1.0\", \"1.5\")\n  ) +\n  labs(title = \"Prediction Errors\") +\n  mapTheme +\n  theme(\n    legend.position = \"bottom\",\n    legend.title = element_text(size = 10, face = \"bold\"),\n    legend.text = element_text(size = 9),\n    plot.title = element_text(size = 14, face = \"bold\", hjust = 0.5)\n  ) +\n  guides(color = guide_colorbar(\n    barwidth = 12,\n    barheight = 1,\n    title.position = \"top\",\n    title.hjust = 0.5\n  ))\n\n# Map 2: Average Demand  \np2 &lt;- ggplot() +\n  geom_sf(data = philly_census, fill = \"grey95\", color = \"white\", size = 0.1) +\n  geom_point(\n    data = station_errors,\n    aes(x = start_lon.y, y = start_lat.x, color = avg_demand),\n    size = 3.5,\n    alpha = 0.7\n  ) +\n  scale_color_viridis(\n    option = \"viridis\",\n    name = \"Avg Demand (trips/hour)\",\n    direction = -1,\n    breaks = c(0.5, 1.0, 1.5, 2.0, 2.5),\n    labels = c(\"0.5\", \"1.0\", \"1.5\", \"2.0\", \"2.5\")\n  ) +\n  labs(title = \"Average Demand\") +\n  mapTheme +\n  theme(\n    legend.position = \"bottom\",\n    legend.title = element_text(size = 10, face = \"bold\"),\n    legend.text = element_text(size = 9),\n    plot.title = element_text(size = 14, face = \"bold\", hjust = 0.5)\n  ) +\n  guides(color = guide_colorbar(\n    barwidth = 12,\n    barheight = 1,\n    title.position = \"top\",\n    title.hjust = 0.5\n  ))\n\n# Combine\ngrid.arrange(\n  p1, p2,\n  ncol = 2\n  )\n\n\n\n\n\n\n\n\n\n\n\nCode\nerror_sf=st_as_sf(station_errors,coords=c(\"start_lon.y\",\"start_lat.x\"),crs=4326)\nerror_tract=st_join(error_sf,philly_census,join = st_within)\nerror_tract_out=error_tract%&gt;%\n  st_drop_geometry()%&gt;%\n  select(MAE,NAME)\ntop5 &lt;- error_tract_out[order(error_tract_out$MAE, decreasing = TRUE)[1:5], ]\nprint(top5)\n\n\n# A tibble: 5 × 2\n    MAE NAME                                                  \n  &lt;dbl&gt; &lt;chr&gt;                                                 \n1  1.42 Census Tract 125.02; Philadelphia County; Pennsylvania\n2  1.34 Census Tract 12.04; Philadelphia County; Pennsylvania \n3  1.27 Census Tract 19; Philadelphia County; Pennsylvania    \n4  1.21 Census Tract 13.02; Philadelphia County; Pennsylvania \n5  1.19 Census Tract 19; Philadelphia County; Pennsylvania    \n\n\nAbove shows the top 5 census tracts with highest MAE, one of their common characteristics is they all have a distance to University of Penn. Such a distance is not either too close (students can walk to school) or too far that students could not bike at all. As a result, I think putting proximity to Upenn may better resolve such a high MAE.\nTemporal patterns:\n\n\nCode\n# MAE by time of day and day type\ntemporal_errors &lt;- test %&gt;%\n  group_by(time_of_day, weekend) %&gt;%\n  summarize(\n    MAE = mean(abs_error, na.rm = TRUE),\n    .groups = \"drop\"\n  ) %&gt;%\n  mutate(day_type = ifelse(weekend == 1, \"Weekend\", \"Weekday\"))\n\nggplot(temporal_errors, aes(x = time_of_day, y = MAE, fill = day_type)) +\n  geom_col(position = \"dodge\") +\n  scale_fill_manual(values = c(\"Weekday\" = \"#08519c\", \"Weekend\" = \"#6baed6\")) +\n  labs(\n    title = \"Prediction Errors by Time Period\",\n    subtitle = \"When is the model struggling most?\",\n    x = \"Time of Day\",\n    y = \"Mean Absolute Error (trips)\",\n    fill = \"Day Type\"\n  ) +\n  plotTheme +\n  theme(axis.text.x = element_text(angle = 45, hjust = 1))\n\n\n\n\n\n\n\n\n\n\n\nCode\ntrain &lt;- train %&gt;%\n  mutate(\n    pred2_train = predict(model2, newdata = train),\n    error_train = Trip_Count - pred2_train,\n    abs_error_train = abs(error_train),\n    time_of_day = case_when(\n      hour &lt; 7 ~ \"Overnight\",\n      hour &gt;= 7 & hour &lt; 10 ~ \"AM Rush\",\n      hour &gt;= 10 & hour &lt; 15 ~ \"Mid-Day\",\n      hour &gt;= 15 & hour &lt;= 18 ~ \"PM Rush\",\n      hour &gt; 18 ~ \"Evening\"\n    ),\n    season = \"Fall (Training)\"\n  )\n\n\ntest &lt;- test %&gt;%\n  mutate(season = \"Winter (Test)\")\n\n\ntemporal_errors_fall &lt;- train %&gt;%\n  group_by(time_of_day, weekend, season) %&gt;%\n  summarize(\n    MAE = mean(abs_error_train, na.rm = TRUE),\n    .groups = \"drop\"\n  ) %&gt;%\n  mutate(day_type = ifelse(weekend == 1, \"Weekend\", \"Weekday\"))\n\n\ntemporal_errors_winter &lt;- test %&gt;%\n  group_by(time_of_day, weekend, season) %&gt;%\n  summarize(\n    MAE = mean(abs_error, na.rm = TRUE),\n    .groups = \"drop\"\n  ) %&gt;%\n  mutate(day_type = ifelse(weekend == 1, \"Weekend\", \"Weekday\"))\n\n# Combine both seasons\ntemporal_errors_seasonal &lt;- bind_rows(temporal_errors_fall, temporal_errors_winter)\n\n\nggplot(temporal_errors_seasonal, aes(x = time_of_day, y = MAE, fill = day_type)) +\n  geom_col(position = \"dodge\") +\n  facet_wrap(~season) +\n  scale_fill_manual(values = c(\"Weekday\" = \"#08519c\", \"Weekend\" = \"#6baed6\")) +\n  labs(\n    title = \"Prediction Errors by Time Period: Fall vs Winter\",\n    subtitle = \"Seasonal comparison of model performance\",\n    x = \"Time of Day\",\n    y = \"Mean Absolute Error (trips)\",\n    fill = \"Day Type\"\n  ) +\n  plotTheme +\n  theme(axis.text.x = element_text(angle = 45, hjust = 1))\n\n\n\n\n\n\n\n\n\nThe errors are the highest during the AM rush hour and PM rush hour on weekdays. On the weekend, the highest errors are during the AM rush, Mid-Day, and also PM rush. There is a significant decrease in the errors for weekend AM Rush, as it reflects on people’s commuting pattern changes over the weekend.\nThe pattern suggests the model likely underpredicts during rush hours. The model predicts an “average” rush hour, but actual demand swings above and below that average more dramatically than during calmer periods like midday or overnight.\nThe divide for training data and test data is week 50. We assume that prior to week 50 is Fall and after week 50 is considered winter. Looking at both the Fall and Winter for the seasonal pattern. The error overall decreases as it shifts into winter. However, it is likely that the decrease in the errors is because of lower ridership, which makes the prediction easier and results in more stable patterns. Fall errors being higher isn’t because the model performs worse on training data—it’s because Fall has higher absolute ridership. This also means more variability in trip counts and, therefore, more room for prediction error.\nDemographic patterns:\n\n\nCode\n# Join demographic data to station errors\nstation_errors_demo &lt;- station_errors %&gt;%\n  left_join(\n    station_attributes %&gt;% select(start_station, Med_Inc, Percent_Taking_Transit, Percent_White),\n    by = \"start_station\"\n  ) %&gt;%\n  filter(!is.na(Med_Inc))\n\n# Create plots\np1 &lt;- ggplot(station_errors_demo, aes(x = Med_Inc, y = MAE)) +\n  geom_point(alpha = 0.5, color = \"#3182bd\") +\n  geom_smooth(method = \"lm\", se = FALSE, color = \"red\") +\n  scale_x_continuous(labels = scales::dollar) +\n  labs(title = \"Errors vs. Median Income\", x = \"Median Income\", y = \"MAE\") +\n  plotTheme\n\np2 &lt;- ggplot(station_errors_demo, aes(x = Percent_Taking_Transit, y = MAE)) +\n  geom_point(alpha = 0.5, color = \"#3182bd\") +\n  geom_smooth(method = \"lm\", se = FALSE, color = \"red\") +\n  labs(title = \"Errors vs. Transit Usage\", x = \"% Taking Transit\", y = \"MAE\") +\n  plotTheme\n\np3 &lt;- ggplot(station_errors_demo, aes(x = Percent_White, y = MAE)) +\n  geom_point(alpha = 0.5, color = \"#3182bd\") +\n  geom_smooth(method = \"lm\", se = FALSE, color = \"red\") +\n  labs(title = \"Errors vs. Race\", x = \"% White\", y = \"MAE\") +\n  plotTheme\n\ngrid.arrange(p1, p2, p3, ncol = 2)\n\n\n\n\n\n\n\n\n\nThe errors patterns show a relatively weak relationship with the census characteristics but there are still trends. MAE increase slightly as median income increase and MAE also increase in whiter areas. This suggest that the model struggle in affluent and white communities. In contrast, errors decline as transit usage increases, meaning neighborhoods with more transit riders are somewhat easier to predict. These patterns indicate that certain community types—particularly high-income or predominantly White tracts—may be systematically harder for the model to capture. This could reflect that the prediction performance varies in different demographic and socioeconomic groups. The equity implication for this is that model-based decisions could unevenly impact the disadvantage communities unless the biases are identified and corrected."
  },
  {
    "objectID": "labs/Tim_Ye_Assignment5.html#part-3-feature-engineering-model-improvement",
    "href": "labs/Tim_Ye_Assignment5.html#part-3-feature-engineering-model-improvement",
    "title": "Space-Time Prediction of Bike Share Demand: Philadelphia Indego",
    "section": "Part 3: Feature Engineering & model improvement",
    "text": "Part 3: Feature Engineering & model improvement\nPotential features to consider: Philly University and College shapefile\n\n\nCode\n#load data\ncollege=st_read(here(\"data/Universities_Colleges.geojson\"))\n\n\nReading layer `Universities_Colleges' from data source \n  `/Users/ye/Documents/GitHub/portfolio-setup-yezhanggg/data/Universities_Colleges.geojson' \n  using driver `GeoJSON'\nSimple feature collection with 629 features and 10 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: -75.2514 ymin: 39.92048 xmax: -74.97471 ymax: 40.10695\nGeodetic CRS:  WGS 84\n\n\nCode\n#convert them into point\ncollege_point=st_centroid(college)\ncollege=st_transform(college,crs =2272)\nstations_sf_proj=st_transform(stations_sf,crs=2272)\n#calculate the nearest distance from college to each bike station\nuni_coords &lt;- st_coordinates(college)[, 1:2]\nbike_coords &lt;- st_coordinates(stations_sf_proj)[, 1:2]\nnearest &lt;- get.knnx(data = uni_coords,   # search in universities\n                    query = bike_coords,  # for each bike station\n                    k = 1)                # find 1 nearest\n\n# Add results to bike stations\nstations_sf$nearest_uni_distance &lt;- nearest$nn.dist[,1]\nstations_sf$nearest_uni_index &lt;- nearest$nn.index[,1]\n\n# Add university name\nstations_sf$nearest_uni_name &lt;- college$name[nearest$nn.index[,1]]\n\n#join back to panel data\nstations_with_distance &lt;- stations_sf %&gt;%\n  st_drop_geometry() %&gt;%\n  select(start_station, nearest_uni_distance)  # Keep station ID and distance\n\nstudy_panel_complete &lt;- study_panel_complete %&gt;%\n  left_join(stations_with_distance, by = \"start_station\")\n\n\n“Perfect biking weather” indicator (60-75°F, no rain)\n\n\nCode\nstudy_panel_complete=study_panel_complete%&gt;%\n  mutate(perfect_weather=ifelse(Temperature&gt;=60&Temperature&lt;=75&Precipitation==0,1,0))\n\n\nRerun everything Temporal Train/Test Split\n\n\nCode\n# Which stations have trips in BOTH early and late periods?\nearly_stations &lt;- study_panel_complete %&gt;%\n  filter(week &lt; 50) %&gt;%\n  filter(Trip_Count &gt; 0) %&gt;%\n  distinct(start_station) %&gt;%\n  pull(start_station)\n\nlate_stations &lt;- study_panel_complete %&gt;%\n  filter(week &gt;= 50) %&gt;%\n  filter(Trip_Count &gt; 0) %&gt;%\n  distinct(start_station) %&gt;%\n  pull(start_station)\n\n# Keep only stations that appear in BOTH periods\ncommon_stations &lt;- intersect(early_stations, late_stations)\n\n\n# Filter panel to only common stations\nstudy_panel_complete &lt;- study_panel_complete %&gt;%\n  filter(start_station %in% common_stations)\n\n# NOW create train/test split\ntrain &lt;- study_panel_complete %&gt;%\n  filter(week &lt; 50)\n\ntest &lt;- study_panel_complete %&gt;%\n  filter(week &gt;= 50)\n\ncat(\"Training observations:\", format(nrow(train), big.mark = \",\"), \"\\n\")\n\n\nTraining observations: 410,628 \n\n\nCode\ncat(\"Testing observations:\", format(nrow(test), big.mark = \",\"), \"\\n\")\n\n\nTesting observations: 171,684 \n\n\nCode\ncat(\"Training date range:\", min(train$date), \"to\", max(train$date), \"\\n\")\n\n\nTraining date range: 19997 to 20065 \n\n\nCode\ncat(\"Testing date range:\", min(test$date), \"to\", max(test$date), \"\\n\")\n\n\nTesting date range: 20066 to 20088 \n\n\nFinal Model (based on model 2 plus two new features)\n\n\nCode\ntrain &lt;- train %&gt;%\n  mutate(dotw_simple = factor(dotw, levels = c(\"Mon\", \"Tue\", \"Wed\", \"Thu\", \"Fri\", \"Sat\", \"Sun\")))\n\n# Set contrasts to treatment coding (dummy variables)\ncontrasts(train$dotw_simple) &lt;- contr.treatment(7)\nFinal_Model &lt;- lm(\n  Trip_Count ~ as.factor(hour) + dotw_simple + Temperature + Precipitation +\n    lag1Hour + lag3Hours + lag1day + perfect_weather + nearest_uni_distance,\n  data = train\n)\n\nsummary(Final_Model)\n\n\n\nCall:\nlm(formula = Trip_Count ~ as.factor(hour) + dotw_simple + Temperature + \n    Precipitation + lag1Hour + lag3Hours + lag1day + perfect_weather + \n    nearest_uni_distance, data = train)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-12.6491  -0.4718  -0.1260   0.1519  25.5211 \n\nCoefficients:\n                          Estimate    Std. Error t value             Pr(&gt;|t|)\n(Intercept)          -0.1464970415  0.0127434419 -11.496 &lt; 0.0000000000000002\nas.factor(hour)1     -0.0182714609  0.0112486604  -1.624              0.10431\nas.factor(hour)2     -0.0107940128  0.0110643090  -0.976              0.32928\nas.factor(hour)3     -0.0344526072  0.0110459708  -3.119              0.00181\nas.factor(hour)4     -0.0083531795  0.0109717622  -0.761              0.44646\nas.factor(hour)5      0.0611848981  0.0110358703   5.544   0.0000000295508649\nas.factor(hour)6      0.2575670218  0.0110628063  23.282 &lt; 0.0000000000000002\nas.factor(hour)7      0.4098444408  0.0112596421  36.399 &lt; 0.0000000000000002\nas.factor(hour)8      0.6507940325  0.0110663730  58.808 &lt; 0.0000000000000002\nas.factor(hour)9      0.2738975965  0.0111458034  24.574 &lt; 0.0000000000000002\nas.factor(hour)10     0.2175864385  0.0109658961  19.842 &lt; 0.0000000000000002\nas.factor(hour)11     0.2614767519  0.0109827158  23.808 &lt; 0.0000000000000002\nas.factor(hour)12     0.3636194978  0.0108879885  33.396 &lt; 0.0000000000000002\nas.factor(hour)13     0.3290331906  0.0108042597  30.454 &lt; 0.0000000000000002\nas.factor(hour)14     0.3582957691  0.0108404880  33.052 &lt; 0.0000000000000002\nas.factor(hour)15     0.4378781197  0.0111892000  39.134 &lt; 0.0000000000000002\nas.factor(hour)16     0.5179924213  0.0110139789  47.030 &lt; 0.0000000000000002\nas.factor(hour)17     0.6468554690  0.0111187868  58.177 &lt; 0.0000000000000002\nas.factor(hour)18     0.3333275911  0.0112064088  29.744 &lt; 0.0000000000000002\nas.factor(hour)19     0.1663624023  0.0112143240  14.835 &lt; 0.0000000000000002\nas.factor(hour)20     0.0498153289  0.0112824361   4.415   0.0000100896164512\nas.factor(hour)21     0.0518462585  0.0112638217   4.603   0.0000041677035984\nas.factor(hour)22     0.0624837504  0.0111837402   5.587   0.0000000231148447\nas.factor(hour)23     0.0192416386  0.0112496277   1.710              0.08719\ndotw_simple2          0.0071928563  0.0060194123   1.195              0.23211\ndotw_simple3         -0.0119256857  0.0059921760  -1.990              0.04657\ndotw_simple4         -0.0451924239  0.0057326832  -7.883   0.0000000000000032\ndotw_simple5         -0.0418175215  0.0059612574  -7.015   0.0000000000023050\ndotw_simple6         -0.0366658624  0.0059348918  -6.178   0.0000000006497233\ndotw_simple7         -0.0602417427  0.0060890954  -9.893 &lt; 0.0000000000000002\nTemperature           0.0043733480  0.0001797252  24.334 &lt; 0.0000000000000002\nPrecipitation        -0.9771055955  0.0715379325 -13.659 &lt; 0.0000000000000002\nlag1Hour              0.3203241506  0.0014841433 215.831 &lt; 0.0000000000000002\nlag3Hours             0.1141549365  0.0014532176  78.553 &lt; 0.0000000000000002\nlag1day               0.1989228383  0.0013923784 142.865 &lt; 0.0000000000000002\nperfect_weather      -0.0067215626  0.0045380586  -1.481              0.13857\nnearest_uni_distance -0.0000330542  0.0000009882 -33.449 &lt; 0.0000000000000002\n                        \n(Intercept)          ***\nas.factor(hour)1        \nas.factor(hour)2        \nas.factor(hour)3     ** \nas.factor(hour)4        \nas.factor(hour)5     ***\nas.factor(hour)6     ***\nas.factor(hour)7     ***\nas.factor(hour)8     ***\nas.factor(hour)9     ***\nas.factor(hour)10    ***\nas.factor(hour)11    ***\nas.factor(hour)12    ***\nas.factor(hour)13    ***\nas.factor(hour)14    ***\nas.factor(hour)15    ***\nas.factor(hour)16    ***\nas.factor(hour)17    ***\nas.factor(hour)18    ***\nas.factor(hour)19    ***\nas.factor(hour)20    ***\nas.factor(hour)21    ***\nas.factor(hour)22    ***\nas.factor(hour)23    .  \ndotw_simple2            \ndotw_simple3         *  \ndotw_simple4         ***\ndotw_simple5         ***\ndotw_simple6         ***\ndotw_simple7         ***\nTemperature          ***\nPrecipitation        ***\nlag1Hour             ***\nlag3Hours            ***\nlag1day              ***\nperfect_weather         \nnearest_uni_distance ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.9974 on 410591 degrees of freedom\nMultiple R-squared:  0.3271,    Adjusted R-squared:  0.327 \nF-statistic:  5543 on 36 and 410591 DF,  p-value: &lt; 0.00000000000000022\n\n\nCalculate the MAE of the Final Model\n\n\nCode\n# Get predictions on test set\n\n# Create day of week factor with treatment (dummy) coding\ntest &lt;- test %&gt;%\n  mutate(dotw_simple = factor(dotw, levels = c(\"Mon\", \"Tue\", \"Wed\", \"Thu\", \"Fri\", \"Sat\", \"Sun\")))\n\n# Set contrasts to treatment coding (dummy variables)\ncontrasts(test$dotw_simple) &lt;- contr.treatment(7)\n\ntest &lt;- test %&gt;%\n  mutate(\n    pred2 = predict(model2, newdata = test),\n    pred6 = predict(Final_Model, newdata = test)\n  )\n\n# Calculate MAE for each model\nmae_results &lt;- data.frame(\n  Model = c(\n    \"2. + Temporal Lags\",\n    \"6. + Temporal Lags + Spatial/Temporal Feature\"\n  ),\n  MAE = c(\n    mean(abs(test$Trip_Count - test$pred2), na.rm = TRUE),\n    mean(abs(test$Trip_Count - test$pred6), na.rm = TRUE)\n    \n  )\n)\n\nkable(mae_results, \n      digits = 2,\n      caption = \"Mean Absolute Error by Model (Test Set)\",\n      col.names = c(\"Model\", \"MAE (trips)\")) %&gt;%\n  kable_styling(bootstrap_options = c(\"striped\", \"hover\"))\n\n\n\nMean Absolute Error by Model (Test Set)\n\n\nModel\nMAE (trips)\n\n\n\n\n2. + Temporal Lags\n0.40\n\n\n6. + Temporal Lags + Spatial/Temporal Feature\n0.41\n\n\n\n\n\nMap the error out\n\n\nCode\ntest &lt;- test %&gt;%\n  mutate(\n    error = Trip_Count - pred6,\n    abs_error = abs(error),\n    time_of_day = case_when(\n      hour &lt; 7 ~ \"Overnight\",\n      hour &gt;= 7 & hour &lt; 10 ~ \"AM Rush\",\n      hour &gt;= 10 & hour &lt; 15 ~ \"Mid-Day\",\n      hour &gt;= 15 & hour &lt;= 18 ~ \"PM Rush\",\n      hour &gt; 18 ~ \"Evening\"\n    )\n  )\n\n# Calculate MAE by station\nstation_errors &lt;- test %&gt;%\n  group_by(start_station, start_lat.x, start_lon.y) %&gt;%\n  summarize(\n    MAE = mean(abs_error, na.rm = TRUE),\n    avg_demand = mean(Trip_Count, na.rm = TRUE),\n    .groups = \"drop\"\n  ) %&gt;%\n  filter(!is.na(start_lat.x), !is.na(start_lon.y))\n\n## Create Two Maps Side-by-Side with Proper Legends (sorry these maps are ugly)\n\n# Calculate station errors\nstation_errors &lt;- test %&gt;%\n  filter(!is.na(pred6)) %&gt;%\n  group_by(start_station, start_lat.x, start_lon.y) %&gt;%\n  summarize(\n    MAE = mean(abs(Trip_Count - pred6), na.rm = TRUE),\n    avg_demand = mean(Trip_Count, na.rm = TRUE),\n    .groups = \"drop\"\n  ) %&gt;%\n  filter(!is.na(start_lat.x), !is.na(start_lon.y))\n\n# Map 1: Prediction Errors\np1 &lt;- ggplot() +\n  geom_sf(data = philly_census, fill = \"grey95\", color = \"white\", size = 0.1) +\n  geom_point(\n    data = station_errors,\n    aes(x = start_lon.y, y = start_lat.x, color = MAE),\n    size = 3.5,\n    alpha = 0.7\n  ) +\n  scale_color_viridis(\n    option = \"plasma\",\n    name = \"MAE (trips)\",\n    direction = -1,\n    breaks = c(0.5, 1.0, 1.5),\n    labels = c(\"0.5\", \"1.0\", \"1.5\")\n  ) +\n  labs(title = \"Prediction Errors\") +\n  mapTheme +\n  theme(\n    legend.position = \"bottom\",\n    legend.title = element_text(size = 10, face = \"bold\"),\n    legend.text = element_text(size = 9),\n    plot.title = element_text(size = 14, face = \"bold\", hjust = 0.5)\n  ) +\n  guides(color = guide_colorbar(\n    barwidth = 12,\n    barheight = 1,\n    title.position = \"top\",\n    title.hjust = 0.5\n  ))\n\n# Map 2: Average Demand  \np2 &lt;- ggplot() +\n  geom_sf(data = philly_census, fill = \"grey95\", color = \"white\", size = 0.1) +\n  geom_point(\n    data = station_errors,\n    aes(x = start_lon.y, y = start_lat.x, color = avg_demand),\n    size = 3.5,\n    alpha = 0.7\n  ) +\n  scale_color_viridis(\n    option = \"viridis\",\n    name = \"Avg Demand (trips/hour)\",\n    direction = -1,\n    breaks = c(0.5, 1.0, 1.5, 2.0, 2.5),\n    labels = c(\"0.5\", \"1.0\", \"1.5\", \"2.0\", \"2.5\")\n  ) +\n  labs(title = \"Average Demand\") +\n  mapTheme +\n  theme(\n    legend.position = \"bottom\",\n    legend.title = element_text(size = 10, face = \"bold\"),\n    legend.text = element_text(size = 9),\n    plot.title = element_text(size = 14, face = \"bold\", hjust = 0.5)\n  ) +\n  guides(color = guide_colorbar(\n    barwidth = 12,\n    barheight = 1,\n    title.position = \"top\",\n    title.hjust = 0.5\n  ))\n\n# Combine\ngrid.arrange(\n  p1, p2,\n  ncol = 2\n  )\n\n\n\n\n\n\n\n\n\n\n\nCode\nerror_sf=st_as_sf(station_errors,coords=c(\"start_lon.y\",\"start_lat.x\"),crs=4326)\nerror_tract=st_join(error_sf,philly_census,join = st_within)\nerror_tract_out=error_tract%&gt;%\n  st_drop_geometry()%&gt;%\n  select(MAE,NAME)\ntop5 &lt;- error_tract_out[order(error_tract_out$MAE, decreasing = TRUE)[1:5], ]\nprint(top5)\n\n\n# A tibble: 5 × 2\n    MAE NAME                                                  \n  &lt;dbl&gt; &lt;chr&gt;                                                 \n1  1.42 Census Tract 125.02; Philadelphia County; Pennsylvania\n2  1.32 Census Tract 12.04; Philadelphia County; Pennsylvania \n3  1.28 Census Tract 214; Philadelphia County; Pennsylvania   \n4  1.26 Census Tract 19; Philadelphia County; Pennsylvania    \n5  1.20 Census Tract 13.02; Philadelphia County; Pennsylvania \n\n\nThe two new features are perfect_weather and nearest_uni_distance. Perfect weather is defined as days where the temperature is between 60 to 75 degrees with no rain. This predictor was chosen because it allows the model to capture the threshold effect that linear weather terms miss. There are many perfect biking days in October but far fewer in November and December. University areas have unique demand patterns with different ridership based on time of day and time of year. By including distance to the nearest university, the model can adjust predictions for stations and address the spatial clustering of errors around University City.\nLooking at the MAE for both models, the final model has a slightly higher MAE of 0.41 compared to 0.40 for the baseline Model 2. The features helped but didn’t fully solve the problem. The top 5 highest-error tracts are all located in Center City and South Philadelphia—areas that are 1–3 miles from UPenn, which falls in the “bikeable but not walkable” zone where university-related demand is most unpredictable. These tracts have high and variable demand driven by a mix of uses."
  },
  {
    "objectID": "labs/Tim_Ye_Assignment5.html#part-4-critical-reflection",
    "href": "labs/Tim_Ye_Assignment5.html#part-4-critical-reflection",
    "title": "Space-Time Prediction of Bike Share Demand: Philadelphia Indego",
    "section": "Part 4: Critical Reflection",
    "text": "Part 4: Critical Reflection\nIn conclusion, it is hard to decide if the final MAE of the model good enough to use, as the mean MAE is still around 0.4, with maximum MAE of 1.42 (Model 2). Based on the mean of the count data (0.53), such a model would easily overestimate in low-demand area. However, for areas with a high demand, such as center city, the MAE seems not to be an issue. Temporally, the model did not predict well in rush hours while did a better job at night and midnight.\nThe prediction errors were clustered between University City and City Hall (east center city). One of the potential explanation would be that location is too far to commute on foot but too close/expensive to commute by car. The distance is perfect for bike riding for work and school and thus the model underestimate it. However, after adding distance to college, the model even performed worse a little bit. In the future, more comprehensive spatial data such as employment centers/sub centers could be brought into the model.\nBased on the current distribution of Bike station, it is hard to say the data we collected perfectly reflect the demand of biking in Philadelphia. As most of the bike stations are clustered in center city and university city,low-income communities at the north and the south of Philadelphia do not have access to bike station at all. I recommend placing more bike stations in those vulnerable communities to create a more equal access to indego biking."
  }
]